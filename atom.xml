<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ThreeLanes&#39; Site</title>
  
  <subtitle>共享 开放 包容 改进</subtitle>
  <link href="https://deepcity.github.io/atom.xml" rel="self"/>
  
  <link href="https://deepcity.github.io/"/>
  <updated>2024-09-05T09:20:03.132Z</updated>
  <id>https://deepcity.github.io/</id>
  
  <author>
    <name>ThreeLanes</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>英语实践补充记录</title>
    <link href="https://deepcity.github.io/2024/%E8%8B%B1%E8%AF%AD%E5%AE%9E%E8%B7%B5%E8%A1%A5%E5%85%85%E8%AE%B0%E5%BD%95/article.html"/>
    <id>https://deepcity.github.io/2024/%E8%8B%B1%E8%AF%AD%E5%AE%9E%E8%B7%B5%E8%A1%A5%E5%85%85%E8%AE%B0%E5%BD%95/article.html</id>
    <published>2024-09-05T09:19:19.000Z</published>
    <updated>2024-09-05T09:20:03.132Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在许多不同的语言环境中，人们的思考方式与习惯都是不同的。而在我学英语的过程中，我意识到，绝大部分时间里我并没有真正广泛的使用英语或英式思维。于是我希望记录下来我真正使用英语对话的过程。以确保我真的“懂得”了一门语言</p><p>In many different languages environments, People have totally unequalthinking ways and customs. During the period of I was learnging English,I realized that I never use English and English way of thinking in awide range for most of time. Therefore, I wish recording the processingof truly taking advantages of English for dialogs to maked sure I really”understand” a kind of language.</p><p>持续更新</p><p>Keeping upgrade…</p></blockquote><p>Kownledge Environment: Compute Sicence, Post graduate.</p><h1 id="english">English</h1><h2 id="常见引用与缩写">常见引用与缩写</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;在许多不同的语言环境中，人们的思考方式与习惯都是不同的。而在我学英语的过程中，我意识到，绝大部分时间里我并没有真正广泛的使用英语或英式思维。于是我希望记录下来我真正使用英语对话的过程。以确保我真的“懂得”了一门语言&lt;/p&gt;
&lt;p&gt;In many</summary>
      
    
    
    
    <category term="外语" scheme="https://deepcity.github.io/categories/%E5%A4%96%E8%AF%AD/"/>
    
    <category term="English" scheme="https://deepcity.github.io/categories/%E5%A4%96%E8%AF%AD/English/"/>
    
    
    <category term="English" scheme="https://deepcity.github.io/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>CNN卷积神经网络</title>
    <link href="https://deepcity.github.io/2024/CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/article.html"/>
    <id>https://deepcity.github.io/2024/CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/article.html</id>
    <published>2024-09-03T13:52:12.000Z</published>
    <updated>2024-09-03T14:07:19.039Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络CNN"><a href="#卷积神经网络CNN" class="headerlink" title="卷积神经网络CNN"></a>卷积神经网络CNN</h1><blockquote><p>卷积神经网络（Convolutional Neural Networks, CNN）的复杂性和灵活性使其成为深度学习领域的核心研究主题之一。</p></blockquote><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>卷积神经网络的灵感源自<strong>人类视觉系统</strong>，特别是<strong>视觉皮层中的神经元结构</strong>。自Hubel和Wiesel在1962年的开创性工作以来，这一理念已经引发了一系列研究和发展。</p><span id="more"></span><p><strong>早期发展</strong>: 由Yann LeCun等人在上世纪80年代末到90年代初开发的LeNet-5被视为第一个成功的卷积神经网络。LeNet-5在手写数字识别方面取得了令人印象深刻的结果。</p><p><strong>现代崛起</strong>: 随着硬件的快速进展和<span class="exturl" data-url="aHR0cHM6Ly9jbG91ZC50ZW5jZW50LmNvbS9zb2x1dGlvbi9iaWdkYXRhP2Zyb21fY29sdW1uPTIwMDY1JmZyb209MjAwNjU=">大数据<i class="fa fa-external-link-alt"></i></span>的涌现，CNN在21世纪初开始重新崛起，并在各个领域实现了突破性进展。</p><p>CNN的重要性不仅体现在其精度和效率上，而且还体现在其理论洞见上。例如，卷积层通过共享权重减少了参数数量，这有助于更有效地训练模型，还增强了模型对平移不变性的理解。</p><h2 id="结构大观"><a href="#结构大观" class="headerlink" title="结构大观"></a>结构大观</h2><ol><li><p>卷积层</p><p>通过卷积操作检测图像的局部特征。</p></li><li><p>激活函数</p><p>引入非线性函数，增加模型的表达能力。</p></li><li><p>池化层</p><p>减少特征维度，增加模型的鲁棒性。</p></li><li><p>全连接层</p><p>在处理空间特征后，全连接层用于进行分类或回归。</p></li></ol><p>卷积神经网络的这些组件协同工作，使得CNN能够从原始像素中自动学习有意义的特征层次结构。随着深度增加，这些特征从基本形状和纹理逐渐抽象为复杂的对象和场景表现。<img src="https://s2.loli.net/2024/08/29/MKE4wicUudT8CZ1.png" alt="img"></p><h2 id="卷积神经网络层"><a href="#卷积神经网络层" class="headerlink" title="卷积神经网络层"></a>卷积神经网络层</h2><p>卷积神经网络由多个层组成，每个层具有特定的目的和功能。这一部分将探讨<strong>卷积操作、激活函数、池化层、归一化层</strong>基本概念。</p><h3 id="什么是卷积"><a href="#什么是卷积" class="headerlink" title="什么是卷积"></a>什么是卷积</h3><p><img src="https://s2.loli.net/2024/08/29/zXAZaWs4tF7PObV.png" alt="img"></p><p>卷积操作可以理解为一个线性压缩的过程，通过卷积运算（对应元素相乘后求和）使得一个高维举矩阵降维。</p><script type="math/tex; mode=display">(I∗K)(i,j)=∑_m∑_nI(i−m,j−n)×K(m,n)\\I是输入图像\\K是卷积核\\(i,j)是输出特征图的位置\\(m,n)是卷积核中元素的位置\\∗ 表示卷积操作。</script><h4 id="卷积操作的附加与变体"><a href="#卷积操作的附加与变体" class="headerlink" title="卷积操作的附加与变体"></a>卷积操作的附加与变体</h4><ol><li><strong>标准卷积</strong>：最基本的卷积操作，涉及将卷积核覆盖在输入图像的局部区域上，对应元素相乘后求和。</li><li><strong>步长（Stride）</strong>：步长定义了卷积核在输入图像上滑动的间隔。步长为1意味着卷积核每次移动一个像素；步长更大则可以减少输出特征图的空间尺寸。</li><li><strong>填充（Padding）</strong>：在输入图像的边缘添加额外的零（零填充）或通过其他方式扩展输入图像，以控制输出特征图的大小。这有助于保留输入图像的边缘信息。</li><li><strong>扩张卷积（Dilated Convolution）</strong>：在扩张卷积中，卷积核的元素之间插入了间隙，这使得卷积核可以覆盖更大的输入区域，同时仍然保持较小的参数数量。</li><li><strong>分组卷积（Grouped Convolution）</strong>：在深度可分离卷积（Depthwise Separable Convolution）中，输入通道被分成多个组，每组使用独立的卷积核。这种方法可以减少参数数量和计算量。</li><li><strong>转置卷积（Transposed Convolution）</strong>：也称为分数步长卷积（Fractionally-strided Convolution），用于上采样，即增加特征图的空间尺寸。它通过对卷积核应用反向步幅来实现。</li><li><strong>多尺度卷积（Multi-scale Convolution）</strong>：在某些网络结构中，输入图像的不同尺度版本被同时应用于卷积核，以捕获不同尺度的特征。</li><li><strong>空洞卷积（Atrous Convolution）</strong>：在空洞卷积中，卷积核的元素之间可以有更大的间隙，这允许网络以更低的计算成本捕获更广泛的上下文信息。</li><li><strong>卷积变体</strong>：除了标准卷积，还有许多变体，如1x1卷积，用于在深度方向上进行线性变换，以及分离卷积等。</li><li><strong>激活函数</strong>：在卷积操作之后，通常会应用一个非线性激活函数，如ReLU，以增加模型的非线性能力。</li></ol><h3 id="什么是卷积核，特征映射"><a href="#什么是卷积核，特征映射" class="headerlink" title="什么是卷积核，特征映射"></a>什么是卷积核，特征映射</h3><p>卷积核是一个小型的矩阵，通过在输入上滑动来生成特征映射。每个卷积核都能捕获不同的特征，例如边缘、角点等。</p><p><img src="https://developer.qcloudimg.com/http-save/yehe-9008468/025aa52f9c133ebcb4ed9481a54a1f22.png" alt="img"></p><p>输出特征图m与输入图片n维度关系是</p><script type="math/tex; mode=display">m = n - k + 1\ (k为卷积核维度)</script><p>这是一个十分简化的公式，它假定输入图片与卷积核均为方阵。但对于非方阵来说，实际上相当于将n,m以对应行列数代换即可。此处仅适用于步长为1，步长不为1的情况需要考虑是否有填充零。</p><h4 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h4><p><strong>感受野（Receptive Field）</strong>是神经网络中一个重要的概念，特别是在卷积神经网络（CNN）中。它指的是<strong>网络中一个神经元能够“看到”或影响的输入数据的空间范围。</strong>换句话说，它是输入图像中一个给定的像素点到网络中某个特定神经元的映射区域，这个区域包含了该神经元进行特征检测所需的所有像素。</p><h4 id="卷积核的大小如何设置，有何关系"><a href="#卷积核的大小如何设置，有何关系" class="headerlink" title="卷积核的大小如何设置，有何关系"></a>卷积核的大小如何设置，有何关系</h4><p>卷积核的大小影响了它能捕获的特征的尺度。较小的卷积核可以捕获更细致的特征，而较大的卷积核可以捕获更广泛的特征。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 使用3x3的卷积核</span><br><span class="line">conv_layer_small = nn.<span class="title class_">Conv2</span>d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line"># 使用5x5的卷积核</span><br><span class="line">conv_layer_large = nn.<span class="title class_">Conv2</span>d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><h4 id="多通道卷积"><a href="#多通道卷积" class="headerlink" title="多通道卷积"></a>多通道卷积</h4><p>在多通道输入下进行卷积，每个输入通道与一个卷积核进行卷积，然后所有的结果相加。这允许模型从不同的通道捕获不同的特征。</p><h4 id="步长与填充"><a href="#步长与填充" class="headerlink" title="步长与填充"></a>步长与填充</h4><p>设定不同的步长</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用步长为<span class="number">2</span></span><br><span class="line">cov_layer_stride2=nn.<span class="title class_">Conv2</span>d(<span class="number">3</span>,<span class="number">64</span>,<span class="number">3</span>,stride=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>填充通过在输入边缘添加零来控制输出的尺寸。这有助于控制信息在卷积操作中的丢失。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用填充<span class="number">1</span>，使得输出尺寸与输入尺寸相同（假设步长为<span class="number">1</span>）</span><br><span class="line">conv_layer_padding1 = nn.<span class="title class_">Conv2</span>d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h4><p>空洞卷积也称扩展卷积或带空间及,它通过在卷积核的元素之间插入空间采样点(通常是0)来扩大卷积核的感受野。</p><p>这种方法允许网络在保持相同参数量和计算量的情况下捕获更广泛的上下文信息，特别适用于需要大感受野的场景，如语义分割、目标检测等任务。</p><h3 id="分组卷积"><a href="#分组卷积" class="headerlink" title="分组卷积"></a>分组卷积</h3><p>分组卷积通过将输入通道分组并对每组使用不同的卷积核来扩展卷积操作。这增加了模型的容量，并使其能够学习更复杂的表示。</p><p>在分组卷积中，输入特征图（Feature Maps）和卷积核被分成多个组（groups），每组独立进行卷积运算。具体来说，如果输入特征图的尺寸是 $W×H×C_1$，并且我们设定要将其分为 $g$ 个组，那么每组的输入特征通道数为 $\frac{C_1}{g}$。相应地，每个卷积核也被分为 $g$组，每组卷积核的尺寸变为$k\times k \times \frac{C_1}{g}$，$C_2$表示输出通道的通道数，并且每组有 $\frac{C_2}{g}$个卷积核，最终每组生成 $\frac{C_2}{g}$个输出特征图 。</p><ol><li>在进行分组卷积后输出的特征图的通道数并没有改变</li><li>在进行分组卷积后,在同等的卷积核数量下,参数减少到了原来的$\frac{1}{g}$</li></ol><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p><img src="https://s2.loli.net/2024/08/29/8EvioflpSb3jL69.png" alt="img"></p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p><strong>池化层（Pooling Layer）</strong>在卷积神经网络中扮演了重要角色，通常用于降低特征映射的维度，从而减少计算需求，并增加特征检测器的感受野。</p><h4 id="最大池化"><a href="#最大池化" class="headerlink" title="最大池化"></a>最大池化</h4><p><img src="https://developer.qcloudimg.com/http-save/yehe-9008468/86ec7230226837f73b3864aff30aa71c.png" alt="img">最大池化是最常用的池化技术之一。它通过选择窗口中的最大值来降低特征映射的尺寸。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>定义2x2的最大池化层</span><br><span class="line">max_pooling = nn.<span class="title class_">MaxPool2</span>d(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>最大池化的主要优点是它能保留窗口中的最显著特征。然而，它会丢失一些细节信息。</p><h4 id="平均池化"><a href="#平均池化" class="headerlink" title="平均池化"></a>平均池化</h4><p>与最大池化不同，平均池化使用窗口中所有值的平均值。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>定义2x2的平均池化层</span><br><span class="line">average_pooling = nn.<span class="title class_">AvgPool2</span>d(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>平均池化可以减轻最大池化可能导致的过于突出某些特征的问题，但可能会淡化一些重要特征。</p><h4 id="全局平均池化"><a href="#全局平均池化" class="headerlink" title="全局平均池化"></a>全局平均池化</h4><p>全局平均池化是一种更复杂的池化策略，它计算整个特征映射的平均值。这常用于网络的最后一层，直接用于分类。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>定义全局平均池化层</span><br><span class="line">global_average_pooling = nn.<span class="title class_">AdaptiveAvgPool2</span>d(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="池化窗口大小和步长"><a href="#池化窗口大小和步长" class="headerlink" title="池化窗口大小和步长"></a>池化窗口大小和步长</h4><p>池化窗口的大小和步长会直接影响输出的尺寸。较大的窗口和步长会更显著地降低尺寸。</p><h4 id="池化的替代方案"><a href="#池化的替代方案" class="headerlink" title="池化的替代方案"></a>池化的替代方案</h4><p>池化层已经有了一些现代替代方案，例如使用卷积层的步长大于1，或使用空洞卷积。这些方法可能提供更好的特征保存。</p><h4 id="池化层的选择"><a href="#池化层的选择" class="headerlink" title="池化层的选择"></a>池化层的选择</h4><p>选择特定类型的池化层取决于任务需求和特定数据特性。深入理解各种池化技术如何工作，可以帮助深入理解它们是如何影响模型性能的。</p><h3 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a>归一化层</h3><p><img src="https://s2.loli.net/2024/08/29/7Rz5wdh8H3f2isb.png" alt="img"></p><p>归一化层在训练深度神经网络时扮演了关键角色，主要用于改善训练的稳定性和速度。通过将输入数据缩放到合适的范围，归一化层有助于缓解训练过程中的<strong>梯度消失</strong>和<strong>梯度爆炸</strong>问题。</p><h4 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h4><p>通过对每个特征通道的输入进行归一化,将输入所梵高零均值和单位方差。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>定义批量归一化层</span><br><span class="line">batch_norm = nn.<span class="title class_">BatchNorm2</span>d(num_features=<span class="number">64</span>)</span><br></pre></td></tr></table></figure><p><strong>优势与劣势</strong></p><ul><li><strong>优势</strong>：它允许更高的学习率，提供了一些正则化效果，通常导致更快的训练。</li><li><strong>劣势</strong>：在小批量上的统计估计可能会导致训练和推理间的不一致。</li></ul><h4 id="层归一化（LN）"><a href="#层归一化（LN）" class="headerlink" title="层归一化（LN）"></a>层归一化（LN）</h4><p>层归一化是在单个样本上对所有特征进行归一化的变体。它在句子处理和循环神经网络中特别流行。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>定义层归一化</span><br><span class="line">layer_norm = nn.<span class="title class_">LayerNorm</span>(normalized_shape=<span class="number">64</span>)</span><br></pre></td></tr></table></figure><p>LN的计算方法是对一个样本内所有神经元的输出进行归一化处理，使得这一层的输出具有稳定的均值和方差。具体来说，对于一个隐藏层中的所有激活值$a_i^l$，LN首先计算这些激活值的均值 μ和标准差 σ，然后利用以下公式进行归一化：</p><script type="math/tex; mode=display">\mu = \frac{1}{H} \sum_{i=1}^{H} a_i</script><script type="math/tex; mode=display">\sigma^2 = \frac{1}{H} \sum_{i=1}^{H} (a_i - \mu)^2</script><p>H表示该层的隐藏单元数。诡异话后通过可学习参数$\gamma\  \beta$进行缩放和平移，以适应数据分布</p><script type="math/tex; mode=display">\text{output}_i = \gamma \hat{a}_i + \beta</script><h4 id="实例归一化"><a href="#实例归一化" class="headerlink" title="实例归一化"></a>实例归一化</h4><p>实例归一化主要用于样式转换任务，归一化是在每个样本的每个通道上独立进行的。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>定义实例归一化</span><br><span class="line">instance_norm = nn.<span class="title class_">InstanceNorm2</span>d(num_features=<span class="number">64</span>)</span><br></pre></td></tr></table></figure><h4 id="组归一化"><a href="#组归一化" class="headerlink" title="组归一化"></a>组归一化</h4><p>组归一化是批量归一化和层归一化之间的一种折衷方案，将通道分为不同的组，并在每个组内进行归一化。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>定义组归一化</span><br><span class="line">group_norm = nn.<span class="title class_">GroupNorm</span>(num_groups=<span class="number">32</span>, num_channels=<span class="number">64</span>)</span><br></pre></td></tr></table></figure><h4 id="归一化层的选择"><a href="#归一化层的选择" class="headerlink" title="归一化层的选择"></a>归一化层的选择</h4><p>归一化层的选择应基于特定的任务和模型架构。例如，在视觉任务中，批量归一化可能是首选，而在NLP任务中，层归一化可能更有用。</p><h2 id="训练与优化"><a href="#训练与优化" class="headerlink" title="训练与优化"></a>训练与优化</h2><h3 id="训练集的准备与增强"><a href="#训练集的准备与增强" class="headerlink" title="训练集的准备与增强"></a>训练集的准备与增强</h3><p>有效的训练数据是深度学习成功的基础。为了使卷积神经网络有效学习，训练集的选择和增强至关重要。</p><p><strong>数据预处理</strong></p><p>预处理是训练集准备的关键步骤，包括：</p><ul><li><strong>标准化</strong>：将输入缩放到0-1范围。</li><li><strong>中心化</strong>：减去均值，使数据以0为中心。</li><li><strong>数据清洗</strong>：消除不一致和错误的数据。</li></ul><p><strong>数据增强</strong></p><p>数据增强是一种通过应用随机变换增加数据量的技术，从而增加模型的泛化能力。</p><p><strong>常见增强技巧</strong></p><ul><li><strong>图像旋转、缩放和剪裁</strong></li><li><strong>颜色抖动</strong></li><li><strong>随机噪声添加</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用PyTorch进行多种图像增强</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomRotation(<span class="number">10</span>),</span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0.1</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p><strong>训练集分割</strong></p><p>通常将数据分为训练集、验证集和测试集，以确保模型不会过拟合。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数衡量模型预测与真实目标之间的差距。选择适当的损失函数是优化模型性能的关键步骤。</p><h4 id="均方误差-MSE"><a href="#均方误差-MSE" class="headerlink" title="均方误差(MSE)"></a>均方误差(MSE)</h4><p>衡量预测值与真实值之间的平方差</p><p>常用于连续值预测</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>定义<span class="variable constant_">MSE</span>损失</span><br><span class="line">mse_loss = nn.<span class="title class_">MSELoss</span>()</span><br></pre></td></tr></table></figure><h4 id="平滑L1损失"><a href="#平滑L1损失" class="headerlink" title="平滑L1损失"></a>平滑L1损失</h4><ul><li><strong>平滑L1损失</strong>：减少异常值的影响。</li></ul><h4 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h4><ul><li><strong>交叉熵损失</strong>：衡量预测概率分布与真实分布之间的差异。</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>定义交叉熵损失</span><br><span class="line">cross_entropy_loss = nn.<span class="title class_">CrossEntropyLoss</span>()</span><br></pre></td></tr></table></figure><ul><li>二分交叉熵损失：特别用于二分类任务</li><li>多标签损失：用于多标签分类问题</li></ul><h4 id="优化损失函数"><a href="#优化损失函数" class="headerlink" title="优化损失函数"></a>优化损失函数</h4><p>选择适当的损失函数不仅取决于任务类型，还与模型架构、数据分布和特定的业务指标有关。有时，自定义损失函数可能是必要的，以便捕捉特定问题的核心挑战。</p><h3 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h3><p>优化器用于更新神经网络的权重，以便最小化损失函数。每种优化器都有其特定的数学原理和应用场景。</p><h4 id="随机梯度下降-SGD"><a href="#随机梯度下降-SGD" class="headerlink" title="随机梯度下降(SGD)"></a>随机梯度下降(SGD)</h4><p>SGD是最基本的优化算法。</p><ul><li><strong>基本SGD</strong>: 按照负梯度方向更新权重。</li><li><strong>带动量的SGD</strong>: 引入动量项，积累之前的梯度，以便更平稳地收敛。</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>定义带动量的<span class="variable constant_">SGD</span>优化器</span><br><span class="line">optimizer_sgd_momentum = torch.<span class="property">optim</span>.<span class="title function_">SGD</span>(model.<span class="title function_">parameters</span>(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h4 id="自适应优化器"><a href="#自适应优化器" class="headerlink" title="自适应优化器"></a>自适应优化器</h4><p>自适应优化器能自动调整学习率。</p><ul><li><strong>Adam</strong>: 结合了Momentum和RMSProp的优点。</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>定义<span class="title class_">Adam</span>优化器</span><br><span class="line">optimizer_adam = torch.<span class="property">optim</span>.<span class="title class_">Adam</span>(model.<span class="title function_">parameters</span>(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><ul><li><strong>Adagrad、RMSprop等</strong>: 针对不同参数有不同的学习率。</li></ul><p><strong>优化器选择注意事项</strong></p><ul><li><strong>任务相关性</strong>: 不同优化器在不同任务和数据上可能有不同的效果。</li><li><strong>超参数调优</strong>: 如学习率、动量等可能需要调整。</li></ul><h3 id="学习率调整"><a href="#学习率调整" class="headerlink" title="学习率调整"></a>学习率调整</h3><p>学习率是优化器中的关键超参数，其调整对模型训练有深远影响。</p><h4 id="固定学习率"><a href="#固定学习率" class="headerlink" title="固定学习率"></a><strong>固定学习率</strong></h4><p>最简单的方法是使用固定学习率。但可能不够灵活。</p><h4 id="学习率调度"><a href="#学习率调度" class="headerlink" title="学习率调度"></a><strong>学习率调度</strong></h4><p>更复杂的方法是在训练过程中动态调整学习率。</p><h5 id="预定调整"><a href="#预定调整" class="headerlink" title="预定调整"></a><strong>预定调整</strong></h5><ul><li><strong>步骤下降</strong>: 在固定步骤处降低学习率。</li><li><strong>余弦退火</strong>: 周期性调整学习率。</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>定义余弦退火调度器</span><br><span class="line">scheduler = torch.<span class="property">optim</span>.<span class="property">lr_scheduler</span>.<span class="title class_">CosineAnnealingLR</span>(optimizer_adam, T_max=<span class="number">50</span>)</span><br></pre></td></tr></table></figure><h3 id="正则化技巧"><a href="#正则化技巧" class="headerlink" title="正则化技巧"></a>正则化技巧</h3><p>正则化是防止过拟合和提高模型泛化能力的关键技术。</p><p><strong>L1和L2正则化</strong></p><ul><li><strong>L1正则化</strong>：倾向于产生稀疏权重，有助于特征选择。</li><li><strong>L2正则化</strong>：减小权重，使模型更平滑。</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 使用<span class="title class_">PyTorch</span>添加<span class="variable constant_">L1</span>和<span class="variable constant_">L2</span>正则化</span><br><span class="line">l1_lambda = <span class="number">0.0005</span></span><br><span class="line">l2_lambda = <span class="number">0.0001</span></span><br><span class="line">loss = loss + l1_lambda * torch.<span class="title function_">norm</span>(weights, <span class="number">1</span>) + l2_lambda * torch.<span class="title function_">norm</span>(weights, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><strong>Dropout</strong></p><p>随机关闭一部分神经元，使模型更鲁棒。</p><ul><li><strong>普通Dropout</strong>：随机丢弃神经元。</li><li><strong>Spatial Dropout</strong>：在卷积层中随机丢弃整个特征图。</li></ul><p><strong>Batch Normalization</strong></p><p>通过标准化层输入，加速训练并减轻初始化的敏感性。</p><p><strong>数据增强</strong></p><p>如前所述，数据增强是一种重要的正则化手段。</p><h3 id="模型评估和调优"><a href="#模型评估和调优" class="headerlink" title="模型评估和调优"></a>模型评估和调优</h3><p>模型评估是衡量模型性能的过程，调优则是改进性能。</p><p><strong>交叉验证</strong></p><p>使用交叉验证来估计模型的泛化能力。</p><ul><li><strong>k-折交叉验证</strong>：将数据分为k个部分，轮流使用其中一个作为验证集。</li></ul><p><strong>调参技巧</strong></p><ul><li><strong>网格搜索</strong>：尝试不同超参数组合。</li><li><strong>随机搜索</strong>：随机选择超参数，更高效。</li></ul><p><strong>早停技巧</strong></p><p>如果验证损失不再下降，则停止训练，以防止过拟合。</p><p><strong>模型集成</strong></p><p>通过结合多个模型来提高性能。</p><ul><li><strong>Bagging</strong>：训练多个模型并平均预测。</li><li><strong>Boosting</strong>：在先前模型的错误上训练新模型。</li><li><strong>Stacking</strong>：使用新模型组合其他模型的预测。</li></ul><h2 id="实践与CNN模型编写"><a href="#实践与CNN模型编写" class="headerlink" title="实践与CNN模型编写"></a>实践与CNN模型编写</h2><p>下面使用CNN编写一个简单的句子分类器</p><h3 id="字词为何要数据化，怎样数据化的？"><a href="#字词为何要数据化，怎样数据化的？" class="headerlink" title="字词为何要数据化，怎样数据化的？"></a>字词为何要数据化，怎样数据化的？</h3><p>字词需要数据化，因为计算机和机器学习模型只能处理数值型数据。数据化（也称为特征提取或向量化）是将原始文本转换为模型可以理解和处理的数值形式的过程。以下是将字词数据化的常见步骤：</p><ol><li><p><strong>分词（Tokenization）</strong></p><p>将句子分解成单独的单词或标记（tokens），这些单词或标记是数据化的基本单位。</p></li><li><p><strong>构建词汇表（Vocabulary）</strong></p><p>从训练数据中提取所有唯一的单词，并为每个单词分配一个唯一的索引或ID。</p></li><li><p><strong>词嵌入（Word Embedding）</strong></p><p>使用预训练的词向量模型（如word2vec、GloVe或BERT）将每个单词转换为固定大小的向量。这些向量捕捉了单词的语义和语法特征。</p></li><li><p><strong>索引化（Indexing）</strong></p><p>由于句子长度可能不同，通常需要将句子填充到相同的长度，以便能够批量处理。这通常涉及到在句子末尾添加特殊的“填充”标记。</p></li><li><p><strong>序列化（Sequencing）</strong></p><p>将填充后的句子转换为数值序列，每个位置对应一个单词的数值索引或词嵌入向量。</p></li><li><p><strong>特殊标记（Special Tokens）</strong></p><p>在某些模型中，可能会使用特殊的开始（如<code>&lt;BOS&gt;</code>）和结束（如<code>&lt;EOS&gt;</code>）标记来标记句子的开始和结束。</p></li><li><p><strong>转换为张量（Tensorization）</strong></p><p>将数值序列转换为可以被深度学习框架处理的张量（多维数组）</p></li><li><p><strong>上下文化（Contextualization）</strong></p><p>在某些模型中，如Transformer或BERT，单词的表示不是静态的，而是根据它们在句子中的上下文动态生成的。</p></li><li><p><strong>归一化（Normalization）</strong></p><p>有时，为了提高模型性能，会将词嵌入向量进行归一化处理，使它们具有单位长度。</p></li></ol><h3 id="需要准备的第三方包"><a href="#需要准备的第三方包" class="headerlink" title="需要准备的第三方包"></a>需要准备的第三方包</h3><h4 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h4><p>旧版依赖安装已废弃，请阅读CNN补充-pyTorch中有关异常处理的安装问题中的版本控制处理。</p><blockquote><ol><li>Numpy：适用于多维数组方面的数学运算</li><li>pyTorch：深度学习框架库，提供了构建和训练神经网络所需的工具与函数</li></ol><p>使用pip安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install numpy # 安装numpy</span><br><span class="line">pip install torch # 安装pyTorch</span><br></pre></td></tr></table></figure><p>验证是否安装成功</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="built_in">print</span>(numpy.__version__)</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(numpy.__version__)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1.26</span><span class="number">.4</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(torch.__version__)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">2.4</span><span class="number">.0</span>+cpu</span><br></pre></td></tr></table></figure></blockquote><h4 id="下载所需数据集"><a href="#下载所需数据集" class="headerlink" title="下载所需数据集"></a>下载所需数据集</h4><p>分别是情感分析数据集（MR, SST-1, SST-2）或问题分类数据集（TREC）。</p><p>以下内容与GithubCNN-NLP学习系列内容重复，等待整理或异步GithubCNN-NLP学习</p><h4 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h4><h5 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h5><h5 id="神经网络层"><a href="#神经网络层" class="headerlink" title="神经网络层"></a>神经网络层</h5><p>……</p><h4 id="训练与预测函数"><a href="#训练与预测函数" class="headerlink" title="训练与预测函数"></a>训练与预测函数</h4><h5 id="训练函数"><a href="#训练函数" class="headerlink" title="训练函数"></a>训练函数</h5><h5 id="验证函数"><a href="#验证函数" class="headerlink" title="验证函数"></a>验证函数</h5><h5 id="番外"><a href="#番外" class="headerlink" title="番外"></a>番外</h5><ol><li>一般在同文件中写入所有模型所需函数，例如预测函数（单测试数据输出）、参数保存函数</li></ol><p>……</p><h4 id="日志记录与参数保存"><a href="#日志记录与参数保存" class="headerlink" title="日志记录与参数保存"></a>日志记录与参数保存</h4><p>…….</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><span class="exturl" data-url="aHR0cHM6Ly9jbG91ZC50ZW5jZW50LmNvbS9kZXZlbG9wZXIvYXJ0aWNsZS8yMzQ4NDgx">头疼！卷积神经网络是什么？CNN结构、训练与优化一文全解-腾讯云开发者社区-腾讯云 (tencent.com)<i class="fa fa-external-link-alt"></i></span></li><li><a href="Classic\Convolutional Neural Networks for Sentence Classification.pdf">Convolutional Neural Networks for Sentence Classification.pdf</a> </li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;卷积神经网络CNN&quot;&gt;&lt;a href=&quot;#卷积神经网络CNN&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络CNN&quot;&gt;&lt;/a&gt;卷积神经网络CNN&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;卷积神经网络（Convolutional Neural Networks, CNN）的复杂性和灵活性使其成为深度学习领域的核心研究主题之一。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;卷积神经网络的灵感源自&lt;strong&gt;人类视觉系统&lt;/strong&gt;，特别是&lt;strong&gt;视觉皮层中的神经元结构&lt;/strong&gt;。自Hubel和Wiesel在1962年的开创性工作以来，这一理念已经引发了一系列研究和发展。&lt;/p&gt;</summary>
    
    
    
    <category term="数据分析" scheme="https://deepcity.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    <category term="机器学习" scheme="https://deepcity.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="基础模型" scheme="https://deepcity.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="CNN" scheme="https://deepcity.github.io/tags/CNN/"/>
    
    <category term="卷积神经网络" scheme="https://deepcity.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch实战-CV-classification</title>
    <link href="https://deepcity.github.io/2024/PyTorch%E5%AE%9E%E6%88%98-CV-classification/article.html"/>
    <id>https://deepcity.github.io/2024/PyTorch%E5%AE%9E%E6%88%98-CV-classification/article.html</id>
    <published>2024-09-03T13:49:02.000Z</published>
    <updated>2024-09-03T13:55:05.345Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pyTorch"><a href="#pyTorch" class="headerlink" title="pyTorch"></a>pyTorch</h1><p>实战中领悟torch的函数含义，须有基础神经网络结构概念，并有一定线性代数基础</p><h2 id="对图像识别的神经网络模型构建"><a href="#对图像识别的神经网络模型构建" class="headerlink" title="对图像识别的神经网络模型构建"></a>对图像识别的神经网络模型构建</h2><p>目标：实现一个可分类不同衣物图像的神经网络</p><h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2><span id="more"></span><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>shape、dtype、device，分别表示维度，数据类型与存储在什么设备上</p><h3 id="函数使用"><a href="#函数使用" class="headerlink" title="函数使用"></a>函数使用</h3><p><code>torch.tensor</code>将数组转化为张量</p><p><code>torch.from_numpy</code>从numpy转化为张量</p><p><code>.numpy</code> 转换为numpy</p><p><code>torch.ones_like</code>从一个张量复制到另一个张量</p><p><code>torch.rand</code>随机填充（由元组或数组决定维度）填充小于1的小数，扩展<code>torch.ones</code>，<code>torch.zeros</code>填充1，0</p><p><code>tensor.to</code> 将张量转存，常用参数如，‘cpu’,’cuda’</p><p><code>tensor[ ],tensor[:, ] tensor[…, ]</code>行列下标索引</p><p><code>tersor.T</code>即对tensor的转置</p><p><code>torch.cat()</code>即对tensor的相连，即将列表或元组中的的tensor整合为一个tensor</p><p>加减乘除</p><ul><li><code>*</code>指的是对每个对应位置元素分别相乘 <code>.mul</code></li><li><code>@</code> 指的是矩阵乘法<code>.matmul</code> </li></ul><h2 id="数据集-Dataset"><a href="#数据集-Dataset" class="headerlink" title="数据集 Dataset"></a>数据集 Dataset</h2><h3 id="引入数据集"><a href="#引入数据集" class="headerlink" title="引入数据集"></a>引入数据集</h3><p>通过<code>torchvision</code>加载对应数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, Lambda</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>我们可以见到</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这个语法，解析一下，其中<code>root</code>参数是指定数据根目录，<code>train</code>表示当前这个数据集是否是训练集，<code>download</code>则表示该训练集是否不可用时在网上下载，<code>transform</code>则是指定转换数据为何种数据结构<code>target_transform</code>。</p><p>这就是使用torchvision中datasets包引用数据集的基本方法。</p><p>此处的trainning_data以及test_data均为datasets.FashionMNIST类型。</p><h3 id="通过可视化直观感受数据集"><a href="#通过可视化直观感受数据集" class="headerlink" title="通过可视化直观感受数据集"></a>通过可视化直观感受数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">labels_map = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;T-Shirt&quot;</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;Trouser&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;Pullover&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;Dress&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;Coat&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;Sandal&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;Shirt&quot;</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;Sneaker&quot;</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;Bag&quot;</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;Ankle Boot&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line">figure = plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">cols, rows = <span class="number">3</span>, <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, cols * rows + <span class="number">1</span>):</span><br><span class="line">    sample_idx = torch.randint(<span class="built_in">len</span>(training_data), size=(<span class="number">1</span>,)).item()</span><br><span class="line">    img, label = training_data[sample_idx]</span><br><span class="line">    figure.add_subplot(rows, cols, i)</span><br><span class="line">    plt.title(labels_map[label])</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.imshow(img.squeeze(), cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>通过调用<code>print(training_data[sample_idx])</code>可以发现每一个训练数据都是由多维维元组构成的，前面一部分浮点数list描述了每一行每一列单通道的像素明暗度，表示了一幅图案，后一个整型数字这就是ce测试数据的label</p><h3 id="规范化数据"><a href="#规范化数据" class="headerlink" title="规范化数据"></a>规范化数据</h3><p>通过Dataloader导入数据，其中batch_size表示了每一次批次中的数量，shuffle表示随机下标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>通过Dataloader索引图像</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Display image and label.</span></span><br><span class="line">train_features, train_labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Feature batch shape: <span class="subst">&#123;train_features.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Labels batch shape: <span class="subst">&#123;train_labels.size()&#125;</span>&quot;</span>)</span><br><span class="line">img = train_features[<span class="number">0</span>].squeeze()</span><br><span class="line">label = train_labels[<span class="number">0</span>]</span><br><span class="line">plt.imshow(img, cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line">label_name = <span class="built_in">list</span>(labels_map.values())[label]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Label: <span class="subst">&#123;label_name&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="对数据进行变形"><a href="#对数据进行变形" class="headerlink" title="对数据进行变形"></a>对数据进行变形</h3><p>并非所有数据都适合机器学习的final input，因此，需要对一些数据进行变形。</p><p>在此之前，我们需要了解datasets中的数据特征</p><ol><li>他们都是有两部分组成的features与labels的元组</li><li>前一部分描述数据，后一部分描述标签（这里暂时只考虑图像，即单通道或多通道的明暗list与一个表示类别的整型数字）</li></ol><p>可以使用<code>transform</code>来进行数据的整理，<code>target_transform</code>进行标签的整理。</p><p>下面是一个使用Lambda匿名函数的示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">ds = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor(),</span><br><span class="line">    target_transform=Lambda(<span class="keyword">lambda</span> y: torch.zeros(<span class="number">10</span>, dtype=torch.<span class="built_in">float</span>).scatter_(<span class="number">0</span>, torch.tensor(y), value=<span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>其中有个常见的函数，这里提一下，详细可以看源码里的注释</p><p><code>scatter_(dim,index,value)</code>就是将对应index坐标中的值更改为value，dim则是指定的维度</p><p>具体到这个实例中就是弄成，比如T-shirt的标签是</p><p><code>tensor[1,0,0,0,0,0,0,0,0,0]</code></p><ol><li><h2 id="模型生成"><a href="#模型生成" class="headerlink" title="模型生成"></a>模型生成</h2></li></ol><h3 id="模型相关调试"><a href="#模型相关调试" class="headerlink" title="模型相关调试"></a>模型相关调试</h3><p>神经网络的工作原理我就不在这里再重复了，在我的MindSpore概念章中已经提到了有关知识。</p><p>但我仍要针对torch的部分说一下</p><p>首先是引入部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br></pre></td></tr></table></figure><p>这里引用了nn，transform这两个比较难以理解的东西，后面用到了再提</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Using &#123;&#125; device&#x27;</span>.<span class="built_in">format</span>(device))</span><br></pre></td></tr></table></figure><p>这里并<strong>没有设定实际的运行设备</strong>而是输出信息！</p><h3 id="模型引入"><a href="#模型引入" class="headerlink" title="模型引入"></a>模型引入</h3><p>直到这里，可以开始写第一个pyTorch框架下的模型了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">        <span class="variable language_">self</span>.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><p>吸引眼球的函数<code>nn.Flatten</code>,来看一下这个函数是如何描述的，首先，Flatten这个单词表示扁平化，可以联想到我们在运算过程中的降维</p><p>Shape:</p><pre><code>    - Input: :$$(*, S_&#123;\text&#123;start&#125;&#125;,..., S_&#123;i&#125;, ..., S_&#123;\text&#123;end&#125;&#125;, *)$$      where :$S_&#123;i&#125;$ is the size at dimension :math:`i` and :math:`*` means any      number of dimensions including none.            - Output: :math:$(*, \prod_&#123;i=\text&#123;start&#125;&#125;^&#123;\text&#123;end&#125;&#125; S_&#123;i&#125;, *)$.</code></pre><p>注意这里的连乘指的是大小连乘而并非是数值连乘，也就是说如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Examples::</span><br><span class="line">        &gt;&gt;&gt; <span class="built_in">input</span> = torch.randn(<span class="number">32</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">        &gt;&gt;&gt; <span class="comment"># With default parameters</span></span><br><span class="line">        &gt;&gt;&gt; m = nn.Flatten()</span><br><span class="line">        &gt;&gt;&gt; output = m(<span class="built_in">input</span>)</span><br><span class="line">        &gt;&gt;&gt; output.size()</span><br><span class="line">        torch.Size([<span class="number">32</span>, <span class="number">25</span>])</span><br><span class="line">        &gt;&gt;&gt; <span class="comment"># With non-default parameters</span></span><br><span class="line">        &gt;&gt;&gt; m = nn.Flatten(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        &gt;&gt;&gt; output = m(<span class="built_in">input</span>)</span><br><span class="line">        &gt;&gt;&gt; output.size()</span><br><span class="line">        torch.Size([<span class="number">160</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>然后了解一下nn.Linear函数，这个函数的<em>_init\</em>_是这样描述的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">             in_features: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">             out_features: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">             bias: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">             device: <span class="type">Any</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">             dtype: <span class="type">Any</span> = <span class="literal">None</span></span>) -&gt; <span class="literal">None</span></span><br></pre></td></tr></table></figure><p>他有五个参数，输入feature，输出feature，bias（偏差），设备，数据类型</p><p>可见，这里使用nn.Module定义了一个神经网络，由于我们的数据是28*28的图像，这里我们设置：</p><p>第一个in_features28*28的,out_feature则是512的</p><p>而后是一个512到512的中间层（hidden layer），然后是一个512到10的输出层。</p><p>每层都采用ReLU作为激活函数</p><p>然后我们注意力放到<code>forword</code>前向传播函数中，在这里我们设定了这个模型的Flatten函数以及数据如何通过神经网络层</p><h3 id="实例化模型并使用"><a href="#实例化模型并使用" class="headerlink" title="实例化模型并使用"></a>实例化模型并使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><p>我们指定一下示例到哪里运行</p><p>这里device可以有很多取值，但在我们的示例中，仅仅只有‘cpu’,’gpu’两种取值</p><p>接下来就是使用这一个模型</p><p>首先，我们通过torch.rand随机出一个参数X（1，28，28）</p><p>在这之后，我们需要注意，不能直接使用model.forward()函数，而是需要以X为输入并返回一个十维的行预测值</p><p>然后我们来了解一个函数,<code>nn.Softmax</code></p><p>Softmax is defined as:</p><script type="math/tex; mode=display">\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}</script><p>它对指定维度的数字完成归一化的操作，使得他们的和为1而一定程度上保留其数字特征</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device)</span><br><span class="line">logits = model(X) </span><br><span class="line">pred_probab = nn.Softmax(dim=<span class="number">1</span>)(logits)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>通过这段代码，我们实际上使模型第一次<em>流过了</em>数据，但这显然不是神经网络，他没有学到任何东西，于是，我们需要聚焦权值和偏移量。</p><h4 id="Weights-and-Bias"><a href="#Weights-and-Bias" class="headerlink" title="Weights and Bias"></a>Weights and Bias</h4><p>首先，让我们观察一下第一层的两个变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First Linear weights: <span class="subst">&#123;model.linear_relu_stack[<span class="number">0</span>].weight&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First Linear biases: <span class="subst">&#123;model.linear_relu_stack[<span class="number">0</span>].bias&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>看到这些数据，心里的石头终于安心的似了。它也是存在的！</p><blockquote><p>这里再说一下Flatten：</p><p>虽然前面解释了这个函数，但很容易注意到，为什么start_dim这个默认值是1呢？这是因为我们输入图像数据的时候，第一维装进去的是一个批次很多个图像数据。</p><p>是直接第一行放到第一个，第二行放到第二个吗？实践一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor1 = torch.tensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(tensor1.size())</span><br><span class="line">tensor2 = nn.Flatten()(tensor1)</span><br><span class="line"><span class="built_in">print</span>(tensor2)</span><br><span class="line"></span><br><span class="line">tensor1 = torch.tensor([[[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>]],[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]])</span><br><span class="line">tensor2 = nn.Flatten()(tensor1)</span><br><span class="line"><span class="built_in">print</span>(tensor2)</span><br></pre></td></tr></table></figure><p>确实，他就是将不同行按序合并到了同一行，如果指定维度只有1维则不变</p></blockquote><h4 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h4><p>nn.Sequential 是模块的有序容器。数据按照定义的顺序传递到所有模块。您可以使用顺序容器来快速组合出类似 seq_modules 的网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">seq_modules = nn.Sequential(</span><br><span class="line">    flatten,</span><br><span class="line">    layer1,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line">input_image = torch.rand(<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">logits = seq_modules(input_image)</span><br></pre></td></tr></table></figure><h4 id="模型参数查看"><a href="#模型参数查看" class="headerlink" title="模型参数查看"></a>模型参数查看</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model structure: &quot;</span>, model, <span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Layer: <span class="subst">&#123;name&#125;</span> | Size: <span class="subst">&#123;param.size()&#125;</span> | Values : <span class="subst">&#123;param[:<span class="number">2</span>]&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="自动梯度下降"><a href="#自动梯度下降" class="headerlink" title="自动梯度下降"></a>自动梯度下降</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch.autograd 自动梯度下降</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">5</span>)  <span class="comment"># input tensor</span></span><br><span class="line">y = torch.zeros(<span class="number">3</span>)  <span class="comment"># expected output</span></span><br><span class="line">w = torch.randn(<span class="number">5</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)</span><br></pre></td></tr></table></figure><p>以上是一个简单的梯度下降示例，其中使用损失函数是二元交叉熵损失函数。</p><p><code>randn</code>是一个产生正态分布的随机数的函数</p><h4 id="计算图-梯度下降函数"><a href="#计算图-梯度下降函数" class="headerlink" title="计算图,梯度下降函数"></a>计算图,梯度下降函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Gradient function for z =&#x27;</span>,z.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Gradient function for loss =&#x27;</span>, loss.grad_fn)</span><br></pre></td></tr></table></figure><p>这里就比较难以理解了,似乎z是一个数组而已，怎么根据它求导函数呢，事实上，我们要明白并牢记这里的z并非是一个数组，而是一个张量，并且它由w,b两个设定了requires_grad=True的张量计算而来，因此，这里的z就是关于w,b的因变量，也是根据他们两个求导</p><p>更要清楚，我们应用于张量以构建计算图的函数是 Function 类的对象。此对象知道如何在正向计算函数，以及如何在反向传播步骤中计算其导数。对反向传播函数的引用存储在张量的 grad_fn 属性中。</p><h4 id="计算导数"><a href="#计算导数" class="headerlink" title="计算导数"></a>计算导数</h4><p>我们在特定的<code>x</code> 和  <code>y</code>下计算 $\frac{\partial loss}{\partial w}$ and$\frac{\partial loss}{\partial b}$  。</p><p>可以通过调用<code>loss.backward()</code>, 然后获取 <code>w.grad</code> 和 <code>b.grad</code>参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br></pre></td></tr></table></figure><p>这里就有点令人不解了，为什么在获得有关两个自变量的导数时需要先使用一下loss.backward()呢</p><blockquote><p><code>loss.backward()</code> 是一个非常重要的函数，它用于自动计算梯度。</p><p>它实际上是由如下几个步骤组成的</p><ol><li><strong>计算梯度</strong>：<code>loss.backward()</code> 会计算损失函数关于网络参数（如权重和偏置）的梯度。这是通过反向传播算法完成的，该算法从输出层开始，逐层向后计算梯度。</li><li><strong>累积梯度</strong>：在PyTorch中，梯度是累积的，这意味着如果你多次调用 <code>loss.backward()</code> 而不更新参数，梯度会累加。这在某些情况下是有用的，比如在RMSprop或Adam这样的优化器中，它们需要计算梯度的一阶和二阶矩。</li><li><strong>准备参数更新</strong>：计算完梯度后，这些梯度会被用于参数的更新。通常，你会在调用 <code>loss.backward()</code> 之后，使用优化器（如 <code>optimizer.step()</code>）来更新参数。</li><li><strong>清除旧梯度</strong>：在每次迭代开始之前，通常需要清除旧的梯度，以避免梯度累积。这可以通过调用 <code>optimizer.zero_grad()</code> 或 <code>model.zero_grad()</code> 来实现。</li></ol><p><strong>backward中的形参</strong></p><ol><li><p><strong>gradient（可选）</strong>：这是一个用来指示目标张量相对于该张量的梯度的张量。如果指定了 <code>gradient</code>，它的形状必须与目标张量相同。如果不指定，PyTorch 会默认使用 1 作为梯度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = (x + <span class="number">1</span>) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们想要将梯度缩放为 2</span></span><br><span class="line">loss = y</span><br><span class="line">loss.backward(gradient=torch.tensor([<span class="number">2.0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># 输出: tensor([4.])</span></span><br></pre></td></tr></table></figure><p>在这个例子中，<code>y = (x + 1)^2</code> 的导数是 <code>2 * (x + 1)</code>。如果我们不指定 <code>gradient</code> 参数，<code>x.grad</code> 将会是 <code>[4.]</code>（因为 <code>2 * (1 + 1)</code>）。但是，我们通过指定 <code>gradient=torch.tensor([2.0])</code>，实际上是将损失函数对 <code>x</code> 的影响放大了 2 倍，所以最终的梯度是 <code>[8.]</code> 而不是 <code>[4.]</code>。</p></li><li><p><strong>retain_graph（可选）</strong>：这是一个布尔值，用于指定是否保留计算图。默认情况下，<code>retain_graph=False</code>，这意味着计算图会在 <code>backward()</code> 调用后被释放，以节省内存。如果你需要再次对同一个图进行反向传播（例如，在同一个网络中进行多次反向传播），你可以设置 <code>retain_graph=True</code>。</p></li><li><p><strong>retain_variables（可选）</strong>：这是一个布尔值，用于指定是否保留用于计算梯度的变量。默认情况下，<code>retain_variables=False</code>。如果你需要在 <code>backward()</code> 调用后再次使用这些变量，可以设置 <code>retain_variables=True</code>。</p></li></ol></blockquote><h4 id="避免梯度计算"><a href="#避免梯度计算" class="headerlink" title="避免梯度计算"></a>避免梯度计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = torch.matmul(x, w) + b</span><br></pre></td></tr></table></figure><p>通过这样的代码就能<strong>阻断</strong>梯度计算了。</p><p>这种代码对java选手简直就是天书，即使c++选手也会两眼一黑，这里不得不提到python的特性之一，上下文管理。参考 <a href="python补充.md">python补充.md</a> </p><p>值得注意的是：梯度计算是一个<strong>链式过程</strong>，即他是在有向无环图DAGs上进行的反向传播。</p><p><strong>什么情况下会用到阻断梯度计算</strong></p><ol><li>希望冻结参数运行神经网络模型</li><li>希望加速神经网络模型并只进行前向计算</li></ol><h4 id="雅各比行列式"><a href="#雅各比行列式" class="headerlink" title="雅各比行列式"></a>雅各比行列式</h4><p>在多元函数的求导中存在着这么一种求导法则，即雅各比行列式。</p><p>对函数$\vec{y}=f(\vec{x})$，当$\vec{x}=\langle x<em>1,\dots,x_n\rangle$，$\vec{y}=\langle y_1,\dots,y_m\rangle$， $\vec{y}$ 对$\vec{x}$的导数是一个包含$\frac{\partial y</em>{i}}{\partial x<em>{j}}$雅阁比行列式 $J</em>{ij}$ 。</p><p>Pytorch允许以计算 $v^T\cdot J$替代$v=(v_1 \dots v_m)$。这是通过以v作为backward的参数实现的v的大小应该与原始张量的大小相同，要根据原始张量计算乘积。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inp = torch.eye(<span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">out = (inp+<span class="number">1</span>).<span class="built_in">pow</span>(<span class="number">2</span>)</span><br><span class="line">out.backward(torch.ones_like(inp), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First call\n&quot;</span>, inp.grad)</span><br><span class="line">out.backward(torch.ones_like(inp), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nSecond call\n&quot;</span>, inp.grad)</span><br><span class="line">inp.grad.zero_()</span><br><span class="line">out.backward(torch.ones_like(inp), retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nCall after zeroing gradients\n&quot;</span>, inp.grad)</span><br></pre></td></tr></table></figure><p><code>torch.eye</code>返回一个指定维的单位矩阵</p><p><code>retain_graph</code>形参表示累加梯度计算值，若没有此项，重复backward将会报错</p><p><code>inp.grad.zero_</code>原地函数，设置梯度值为零</p><p>值得注意的是gradient形参，该参数是高级用法之一，他最直观的作用就是将求出来的导数乘以一个矩阵(对应位置的乘法)</p><blockquote><ol><li><strong>自定义梯度</strong>：在某些高级用例中，你可能需要为特定的操作或自定的损失函数指定非标准的梯度。例如，在使用强化学习或者某些特殊的优化算法时，你可能需要根据自定义的规则来计算梯度。</li><li><strong>梯度裁剪</strong>：在训练神经网络时，可能会出现梯度爆炸的问题。在这种情况下，你可能需要在执行反向传播之前对梯度进行裁剪，以防止梯度值过大。通过 <code>gradient</code> 参数，你可以在计算梯度时直接应用梯度裁剪，而不是在梯度计算完成后再进行。</li><li><strong>多任务学习</strong>：在多任务学习中，不同的任务可能需要对同一个网络层的输出有不同的梯度贡献。通过为不同的任务指定不同的 <code>gradient</code> 参数，你可以精确控制每个任务对网络参数更新的影响。</li><li><strong>避免梯度覆盖</strong>：在某些复杂的模型或者动态计算图中，你可能需要在不同的时间点对同一个张量计算不同的梯度。使用 <code>gradient</code> 参数可以在不干扰其他计算的情况下，为特定的计算路径指定梯度。</li><li><strong>效率</strong>：在某些情况下，直接在 <code>backward()</code> 中指定 <code>gradient</code> 参数可能比在梯度计算完成后再进行操作更高效。这可以减少中间变量的创建和操作，从而优化内存使用和计算速度。</li></ol></blockquote><h4 id="构建优化参数循环"><a href="#构建优化参数循环" class="headerlink" title="构建优化参数循环"></a>构建优化参数循环</h4><ol><li><p>设定超参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">1e-3</span> <span class="comment"># 学习率</span></span><br><span class="line">batch_size = <span class="number">64</span> <span class="comment"># 批大小</span></span><br><span class="line">epochs = <span class="number">5</span> <span class="comment"># 训练轮数</span></span><br></pre></td></tr></table></figure></li><li><p>设置损失函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure></li><li><p>优化算法</p><p>所有优化的逻辑都封装在<code>optimizer</code>对象中。在这里，我们使用SGD优化器；在PyTorch中，还有许多不同的优化器，如<code>ADAM</code>和<code>RMSProp</code>，它们适用于不同类型的模型和数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br></pre></td></tr></table></figure></li><li><p>完整实现优化循环</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_loop</span>(<span class="params">dataloader, model, loss_fn, optimizer</span>):</span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    <span class="keyword">for</span> batch, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):        </span><br><span class="line">        <span class="comment"># Compute prediction and loss</span></span><br><span class="line">        pred = model(X)</span><br><span class="line">        loss = loss_fn(pred, y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Backpropagation</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            loss, current = loss.item(), batch * <span class="built_in">len</span>(X)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;loss: <span class="subst">&#123;loss:&gt;7f&#125;</span>  [<span class="subst">&#123;current:&gt;5d&#125;</span>/<span class="subst">&#123;size:&gt;5d&#125;</span>]&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_loop</span>(<span class="params">dataloader, model, loss_fn</span>):</span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    test_loss, correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> dataloader:</span><br><span class="line">            pred = model(X)</span><br><span class="line">            test_loss += loss_fn(pred, y).item()</span><br><span class="line">            correct += (pred.argmax(<span class="number">1</span>) == y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>).<span class="built_in">sum</span>().item()</span><br><span class="line">            </span><br><span class="line">    test_loss /= size</span><br><span class="line">    correct /= size</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Test Error: \n Accuracy: <span class="subst">&#123;(<span class="number">100</span>*correct):&gt;<span class="number">0.1</span>f&#125;</span>%, Avg loss: <span class="subst">&#123;test_loss:&gt;8f&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>这里有几个需要注意的函数</p><p><code>optimizer.step()</code>：该函数根据相应点导数值调用优化器优化参数（目前知道这一点即可）</p><p><code>with torch.no_grad():</code>该代码块中利用上下文机制暂时关闭对应的反向传播</p></li><li><p>真正调用模型开始训练循环</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;t+<span class="number">1</span>&#125;</span>\n-------------------------------&quot;</span>)</span><br><span class="line">    train_loop(train_dataloader, model, loss_fn, optimizer)</span><br><span class="line">    test_loop(test_dataloader, model, loss_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Done!&quot;</span>)</span><br></pre></td></tr></table></figure></li></ol><h2 id="保存参数"><a href="#保存参数" class="headerlink" title="保存参数"></a>保存参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&quot;data/model.pth&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Saved PyTorch Model State to model.pth&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="加载参数"><a href="#加载参数" class="headerlink" title="加载参数"></a>加载参数</h2><p>想要加载参数，显然，我们首先要保证神经网络模型是严格相同的。</p><p>使用<code>load_state_dict()</code>方法加载.pth神经网络模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralNetwork()</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;data/model.pth&#x27;</span>))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><p>其中<code>eval</code>函数用以设置模型维评估模式，这样的模式确保了关闭Dropout层以及Batch Normalization批量归一化层，确保了评估模式的准确性。</p><h2 id="开放式神经网络（Open-Neural-Network-Exchange-ONNE）"><a href="#开放式神经网络（Open-Neural-Network-Exchange-ONNE）" class="headerlink" title="开放式神经网络（Open Neural Network Exchange,ONNE）"></a>开放式神经网络（Open Neural Network Exchange,ONNE）</h2><p><strong>开放神经网络交换格式（Open Neural Network Exchange, ONNX）</strong> 运行时为此提供了一种解决方案，它允许你在任何硬件、云端或边缘设备上一次训练模型并加速推理过程。</p><p>ONNX是一种通用格式，许多厂商支持通过该格式来共享神经网络和其他机器学习模型。你可以使用ONNX格式在其他编程语言和框架（如Java、JavaScript、C#和ML.NET）中对模型进行推理。</p><h3 id="在ONNP中导出模型"><a href="#在ONNP中导出模型" class="headerlink" title="在ONNP中导出模型"></a>在ONNP中导出模型</h3><p>导出模型主要涉及一个函数<code>onnx.export</code>，他存在于torch库中</p><ol><li>一个神经网络模型</li><li>一个输入层维数的零向量</li><li>一个文件路径，用以保存.onnx的模型参数</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input_image = torch.zeros((<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">onnx_model = <span class="string">&#x27;data/model.onnx&#x27;</span></span><br><span class="line">onnx.export(model, input_image, onnx_model)</span><br></pre></td></tr></table></figure><h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><p>针对执行中可能会出现一些问题</p><h3 id="发现我是用CPU跑的，我应该如何换用cuda"><a href="#发现我是用CPU跑的，我应该如何换用cuda" class="headerlink" title="发现我是用CPU跑的，我应该如何换用cuda"></a>发现我是用CPU跑的，我应该如何换用cuda</h3><p>应该首先确认是否支持cuda</p><ol><li><p><code>nvidia-smi</code>shell查询是否支持</p><p><img src="./../../../AppData/Roaming/Typora/typora-user-images/image-20240830151101652.png" alt="navidia-smi"></p><p>如上图，我的显卡是3060，因此支持（可以看到QQ正在用！：）QQ用3060跑虚幻吗）</p></li><li><p>安装cuda：<span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9jdWRhLXRvb2xraXQtYXJjaGl2ZQ==">CUDA Toolkit Archive | NVIDIA Developer<i class="fa fa-external-link-alt"></i></span></p><p>cuda很大，你忍一下，因为网络原因，这里推荐用下载器下载（如：IDM）</p><p><strong>注意版本号，到<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9nZXQtc3RhcnRlZC9sb2NhbGx5Lw==">Start Locally | PyTorch<i class="fa fa-external-link-alt"></i></span> 查询最新支持版本</strong></p></li><li><p><code>nvcc -V</code> 查询是否安装成功</p></li><li><p>添加环境变量</p><p><img src="https://s2.loli.net/2024/08/30/SuG7jTYBF2VZmri.png" alt="cuda-path"></p></li><li><p>安装cudnn</p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vUmVueWktRmFuL3AvMTM0NTg1NTkuaHRtbCNfbGFiZWwwXzI=">cuda和cudnn是什么 - 范仁义 - 博客园<i class="fa fa-external-link-alt"></i></span><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vUmVueWktRmFuL3AvMTM0NTg1NTkuaHRtbCNfbGFiZWwwXzI=">cuda和cudnn是什么 - 范仁义 - 博客园<i class="fa fa-external-link-alt"></i></span><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vUmVueWktRmFuL3AvMTM0NTg1NTkuaHRtbCNfbGFiZWwwXzI=">cuda和cudnn是什么 - 范仁义 - 博客园<i class="fa fa-external-link-alt"></i></span>   </p><p>cuDNN是基于CUDA的深度学习GPU加速库，有了它才能在GPU上完成深度学习的计算。</p><p><span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9yZHAvY3Vkbm4tYXJjaGl2ZQ==">cuDNN Archive | NVIDIA Developer<i class="fa fa-external-link-alt"></i></span></p></li><li><p>下载包中对应路径的文件夹的文件粘贴到cuda安装路径下对应的文件夹下</p></li><li><p>主要使用CUDA内置的deviceQuery.exe 和 bandwithTest.exe两个程序：</p><p>首先启动终端，cd到安装目录下D:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\extras\demo_suite（这是我的安装路径，默认是在C盘），然后分别执行bandwidthTest.exe和deviceQuery.exe。</p><p> 如果以上两步都有Result=PASS，那么就表示安装成功。</p></li><li><p>结果检查</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cuda.is_available()</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cuda.device_count() </span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="为什么我明明安装了torchvisio包，但仍然无法调用"><a href="#为什么我明明安装了torchvisio包，但仍然无法调用" class="headerlink" title="为什么我明明安装了torchvisio包，但仍然无法调用"></a>为什么我明明安装了torchvisio包，但仍然无法调用</h3><p>如果你单纯使用了官网的安装脚本就很有可能出现这个问题。</p><p>一个很常见的原因是torch、torchvisio、python、cuda之间的版本并不匹配，如果安装torch时是直接pip install torch torchvisio很大概率会出这个问题（未知原因，可能是网络问题？），按照对应的版本在官网重装torch\torchvision即可</p><p>在该网站安装pyTorch<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9nZXQtc3RhcnRlZC9sb2NhbGx5Lw==">Start Locally | PyTorch<i class="fa fa-external-link-alt"></i></span></p><p>在该网站检查版本依赖并安装torchvision<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3B5dG9yY2gvdmlzaW9u">pytorch/vision: Datasets, Transforms and Models specific to Computer Vision (github.com)<i class="fa fa-external-link-alt"></i></span></p><h3 id="报错-RuntimeError-Expected-all-tensors-to-be-on-the-same-device"><a href="#报错-RuntimeError-Expected-all-tensors-to-be-on-the-same-device" class="headerlink" title="报错[RuntimeError]: Expected all tensors to be on the same device"></a>报错[RuntimeError]: Expected all tensors to be on the same device</h3><p>字面含义，参与的运算有多个变量，有的在GPU，有的在CPU上</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><span class="exturl" data-url="aHR0cHM6Ly9sZWFybi5taWNyb3NvZnQuY29tL3poLWNuL3RyYWluaW5nL21vZHVsZXMvaW50cm8tbWFjaGluZS1sZWFybmluZy1weXRvcmNoLzEtaW50cm9kdWN0aW9u">简介 - Training | Microsoft Learn<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU4ODMyOTExL2FydGljbGUvZGV0YWlscy8xMjA1NjczNDU=">cuda的安装，及pytorch调用GPU步骤_gpu cuda使用-CSDN博客<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNTkwNjM1L2FydGljbGUvZGV0YWlscy8xMTIzODQ3MTg=">RuntimeError: No such operator torchvision::nms问题解决方法_runtimeerror: operator torchvision::nms does not e-CSDN博客<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9sLWZheS5naXRodWIuaW8vMjAyMC8wOS8wOS9weXRvcmNoRXJyb3IwMC8=">RuntimeError: No such operator torchvision::nms | 兰秋廿柒的博客 (l-fay.github.io)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzI4OTEzNS9hcnRpY2xlL2RldGFpbHMvMTIwMDk3NTc5">【python】使用pip安装指定版本的模块，卸载、查看、更新包_pip install version-CSDN博客<i class="fa fa-external-link-alt"></i></span></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;pyTorch&quot;&gt;&lt;a href=&quot;#pyTorch&quot; class=&quot;headerlink&quot; title=&quot;pyTorch&quot;&gt;&lt;/a&gt;pyTorch&lt;/h1&gt;&lt;p&gt;实战中领悟torch的函数含义，须有基础神经网络结构概念，并有一定线性代数基础&lt;/p&gt;
&lt;h2 id=&quot;对图像识别的神经网络模型构建&quot;&gt;&lt;a href=&quot;#对图像识别的神经网络模型构建&quot; class=&quot;headerlink&quot; title=&quot;对图像识别的神经网络模型构建&quot;&gt;&lt;/a&gt;对图像识别的神经网络模型构建&lt;/h2&gt;&lt;p&gt;目标：实现一个可分类不同衣物图像的神经网络&lt;/p&gt;
&lt;h2 id=&quot;张量&quot;&gt;&lt;a href=&quot;#张量&quot; class=&quot;headerlink&quot; title=&quot;张量&quot;&gt;&lt;/a&gt;张量&lt;/h2&gt;</summary>
    
    
    
    <category term="数据分析" scheme="https://deepcity.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    <category term="机器学习" scheme="https://deepcity.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="PyTorch" scheme="https://deepcity.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/PyTorch/"/>
    
    
    <category term="PyTorch" scheme="https://deepcity.github.io/tags/PyTorch/"/>
    
    <category term="CV" scheme="https://deepcity.github.io/tags/CV/"/>
    
    <category term="分类器" scheme="https://deepcity.github.io/tags/%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>Anaconda简明介绍</title>
    <link href="https://deepcity.github.io/2024/Anaconda%E7%AE%80%E6%98%8E%E4%BB%8B%E7%BB%8D/article.html"/>
    <id>https://deepcity.github.io/2024/Anaconda%E7%AE%80%E6%98%8E%E4%BB%8B%E7%BB%8D/article.html</id>
    <published>2024-08-31T08:53:55.000Z</published>
    <updated>2024-08-31T09:03:37.603Z</updated>
    
    <content type="html"><![CDATA[<h1 id="anaconda简明介绍">Anaconda简明介绍</h1><p>已然是不知道从何处了解到的anaconda了，但它默认成为了我的python编译器指定选项，于是今天来了解一下Anaconda是什么，它是干什么的。</p><h2 id="包管理器">包管理器</h2><p>Anaconda（<span class="exturl" data-url="aHR0cHM6Ly93d3cuYW5hY29uZGEuY29tL2Rvd25sb2FkLyNtYWNvcw==">官方网站<i class="fa fa-external-link-alt"></i></span>）就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。</p><span id="more"></span><h3 id="特点">特点</h3><ol type="1"><li>anaconda是一款开源软件</li><li>其安装过程非常简单</li><li>其能够以很好的性能解释R语言与Python语言</li><li>其拥有丰富且免费的社区支持</li></ol><h3 id="内容">内容</h3><ol type="1"><li>conda包</li><li>环境管理器</li><li>1000+开源库</li></ol><h3 id="与其他包管理器的区别">与其他包管理器的区别</h3><h4 id="conda">conda</h4><p>conda是包及其依赖项和环境的管理工具。</p><p>▪ 适用语言：Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++,FORTRAN。</p><p>▪ 适用平台：Windows, macOS, Linux</p><p>▪ 用途：</p><p>① 快速安装、运行和升级包及其依赖项。</p><p>② 在计算机中便捷地创建、保存、加载和切换环境。</p><blockquote><p>如果你需要的包要求不同版本的Python，你无需切换到不同的环境，因为conda同样是一个<strong>环境管理器</strong>。仅需要几条命令，你可以创建一个完全独立的环境来运行不同的Python版本，同时继续在你常规的环境中使用你常用的Python版本。——<span class="exturl" data-url="aHR0cHM6Ly9jb25kYS5pby9kb2NzLw==">Conda官方网站<i class="fa fa-external-link-alt"></i></span></p></blockquote><p>▪ conda为Python项目而创造，但可适用于上述的多种语言。</p><p>▪ conda包和环境管理器包含于Anaconda的所有版本当中。</p><h4 id="pip">pip</h4><p>pip是用于安装和管理软件包的包管理器。</p><p>▪ pip编写语言：Python。</p><p>▪ Python中默认安装的版本：</p><p>① Python 2.7.9及后续版本：默认安装，命令为 *<strong>pip*</strong></p><p>② Python 3.4及后续版本：默认安装，命令为 *<strong>pip3*</strong></p><p>▪pip名称的由来：pip采用的是<strong>递归缩写</strong>进行命名的。其名字被普遍认为来源于2处：</p><p>① “Pip installs Packages”（“pip安装包”）</p><p>② “Pip installs Python”（“pip安装Python”）</p><h4 id="virtualenv"><strong>virtualenv</strong></h4><p>virtualenv是用于创建一个<strong>独立的</strong>Python环境的工具。</p><p>▪ 解决问题：</p><ol type="1"><li>当一个程序需要使用Python 2.7版本，而另一个程序需要使用Python3.6版本，如何同时使用这两个程序？如果将所有程序都安装在系统下的默认路径，如：*<strong>/usr/lib/python2.7/site-packages*</strong>，当不小心升级了本不该升级的程序时，将会对其他的程序造成影响。</li><li>如果想要安装程序并在程序运行时对其库或库的版本进行修改，都会导致程序的中断。</li><li>在共享主机时，无法在全局 *<strong>site-packages*</strong>目录中安装包。</li></ol><p>▪virtualenv将会为它自己的安装目录创建一个环境，这并<strong>不与</strong>其他virtualenv环境共享库；同时也可以<strong>选择性</strong>地不连接已安装的全局库。</p><h4 id="pip与conda的比较">pip与conda的比较</h4><p>▪ pip：</p><p>① <strong>不一定</strong>会展示所需其他依赖包。</p><p>②安装包时<strong>或许</strong>会直接忽略依赖项而安装，仅在结果中提示错误。</p><p>▪ conda：</p><p>① 列出所需其他依赖包。</p><p>② 安装包时自动安装其依赖项。</p><p>③ 可以便捷地在包的不同版本中自由切换。</p><p><strong>→ 环境管理</strong></p><p>▪ pip：维护多个环境难度较大。</p><p>▪ conda：比较方便地在不同环境之间进行切换，环境管理较为简单。</p><p><strong>→ 对系统自带Python的影响</strong></p><p>▪ pip：在系统自带Python中包的更新/回退版本/卸载将影响其他程序。</p><p>▪ conda：不会影响系统自带Python。</p><p><strong>→ 适用语言</strong></p><p>▪ pip：仅适用于Python。</p><p>▪ conda：适用于Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++,FORTRAN。</p><h4 id="conda与pipvirtualenv的关系">conda与pip、virtualenv的关系</h4><p>▪ conda<strong>结合</strong>了pip和virtualenv的功能。</p><h3 id="总结">总结</h3><p>它是一个开源的，多语言的，多平台的，依赖检查，版本独立且云端同步的包管理工具，用于保存切换不同的编译环境与第三方库。</p><h2 id="使用说明">使用说明</h2><ol type="1"><li><p>通过通过anaconda中的python解释器中的pip安装的包同样也会被anaconda所管理</p></li><li><p>可视化界面：在win上可以打开anaconda-nagvitive可视化界面，查看当前环境与包</p></li><li><p>云同步，需要登录，在win可视化界面右上角可以连接到anaconda云端即可实现同步</p></li><li><p>canda channel的配置</p><p>默认canda channel是default，但这个代码包不全，建议使用conda-forgechannel，并严格设置优先使用conda-forge，因为这不同channel的包不完全兼容。</p><p>在安装这个这个渠道的时候无论使用什么样的方式都是可以的，比如图形化又或者命令行。</p><p>有关命令行的配置方法放在参考文献里了．</p></li><li><p>我应该使用pip安装还是使用conda安装第三方包</p><p>随意，在网络上两种建议都有，使用pip安装比较直接，而使用conda安装可以检查依赖，但据说这这个检查环境兼容性可能有问题，因此，我的建议是先尽量使用pip，出现依赖问题用condainstall</p></li><li><p>他是如何切换换第三方库的？应该如何调用不同的环境</p><p>这里有几种方式</p><ol type="1"><li>使用它自己的命令行方式通过命令更换当前所用的环境</li><li>通过图形界面或者命令找到每个环境对应的python.exe或其他语言编译可执行程序，通过调用这个程序获取不同的第三方库。</li></ol></li></ol><h2 id="参考文献">参考文献</h2><ol type="1"><li><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMjkyNTUwMA==">Anaconda介绍、安装及使用教程- 知乎 (zhihu.com)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNDkwODEzNDQ=">Anaconda channel配置笔记 - 知乎 (zhihu.com)<i class="fa fa-external-link-alt"></i></span></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;anaconda简明介绍&quot;&gt;Anaconda简明介绍&lt;/h1&gt;
&lt;p&gt;已然是不知道从何处了解到的anaconda了，但它默认成为了我的python编译器指定选项，于是今天来了解一下Anaconda是什么，它是干什么的。&lt;/p&gt;
&lt;h2 id=&quot;包管理器&quot;&gt;包管理器&lt;/h2&gt;
&lt;p&gt;Anaconda（&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly93d3cuYW5hY29uZGEuY29tL2Rvd25sb2FkLyNtYWNvcw==&quot;&gt;官方网站&lt;i class=&quot;fa fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/span&gt;）就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。&lt;/p&gt;</summary>
    
    
    
    <category term="软件" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/"/>
    
    <category term="环境" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/%E7%8E%AF%E5%A2%83/"/>
    
    <category term="Anaconda" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/%E7%8E%AF%E5%A2%83/Anaconda/"/>
    
    
    <category term="Anaconda" scheme="https://deepcity.github.io/tags/Anaconda/"/>
    
    <category term="Python" scheme="https://deepcity.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Overleaf-toolkit部署</title>
    <link href="https://deepcity.github.io/2024/Overleaf-toolkit%E9%83%A8%E7%BD%B2/article.html"/>
    <id>https://deepcity.github.io/2024/Overleaf-toolkit%E9%83%A8%E7%BD%B2/article.html</id>
    <published>2024-08-25T09:22:58.000Z</published>
    <updated>2024-08-26T03:37:10.378Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Overleaf-toolkit部署"><a href="#Overleaf-toolkit部署" class="headerlink" title="Overleaf-toolkit部署"></a>Overleaf-toolkit部署</h1><blockquote><p>在写latex的过程中，第一个使用的在线latex编辑器就是overleaf，但是在使用过程中受到诸多限制，遂欲购买overleafpro，但发现其学生价也要500yuan/year，实在是财力不足，但overleaf是开源的，要花钱的只是服务器算力以及各种模板，于是就有了部署Overleaf-toolkit的想法</p></blockquote><p>Overleaf是一个在线多用户协作的Latex编辑网站，其采用了如同markdown的“所见即所得”思想，采用及时更新编译的方式辅助编辑latex文件，同时通过项目结构约束文件的排布。是一个是十分学术且普遍应用的latex编辑网站。</p><span id="more"></span><h2 id="安装前置条件"><a href="#安装前置条件" class="headerlink" title="安装前置条件"></a>安装前置条件</h2><ol><li>一台性能至少为2h/2g的服务器（cpu2h空闲且性能较强且剩余内存至少1.5g），个人使用也可以是一台虚拟机。</li><li>并且至少保留10g的内存余量</li><li>最好有个域名。</li></ol><p><img src="https://s2.loli.net/2024/08/25/Gc4RtmuqsK6lQM8.png" alt="Usage"></p><p>上图为容器运行后各个模块对资源的占用量，值得注意的是在本机中，配置好完整的texlatex与中文字集，对硬盘占用量达到了十个g，并且本机cpu为i7-11800h</p><h2 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h2><p>以下部分可能省略部分操作在参考资料中可以找到</p><h3 id="安装overleaf-toolkit"><a href="#安装overleaf-toolkit" class="headerlink" title="安装overleaf-toolkit"></a>安装overleaf-toolkit</h3><p>这一步十分简单，按照overleafgithub官方库中给的教程即可，唯一需要注意的点如下：</p><ol><li>修改overleaf.rc文件需要关闭所有相关服务并bin/up重新生成docker，慎重第一次生成</li><li>默认占用80端口，但是如果服务器配置了nginx并且希望方向代理，则需要更改overleaf.rc文件<ul><li><code>OVERLEAF_PORT</code> 指的是运行 Overleaf 的容器要选择曝露的端口，默认是 80 端口但如果有要使用 Nginx 反向代理的需求的话则需要自选一个端口（不常用的就行）；</li><li><code>SIBLING_CONTAINERS_ENABLED</code> 这个配置真的巨坑，默认的话是 <code>true</code> 但如果没有购买官方的 Server Pro 的话请直接修改成 <code>false</code>，因为这个功能很大程度上依赖于官方提供给 Server Pro 用户的镜像，如果不调到 <code>false</code> 的话极有可能出现编译失败的情况；</li></ul></li><li>站点标题在variables.env中</li></ol><h2 id="一些基础配置"><a href="#一些基础配置" class="headerlink" title="一些基础配置"></a>一些基础配置</h2><h3 id="安装完整版texLive"><a href="#安装完整版texLive" class="headerlink" title="安装完整版texLive"></a>安装完整版texLive</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL292ZXJsZWFmL3Rvb2xraXQvYmxvYi9tYXN0ZXIvZG9jL2NlLXVwZ3JhZGluZy10ZXhsaXZlLm1kP3NwbT1hMmM2aC4xMjg3MzYzOS5hcnRpY2xlLWRldGFpbC4xNC41YTMzNjk0NVVtU01peCZmaWxlPWNlLXVwZ3JhZGluZy10ZXhsaXZlLm1k">官方升级TexLive文档<i class="fa fa-external-link-alt"></i></span></p><p>根据官方文档升级即可，注意version很有可能是不同的，overleaf官方每隔半年就会更新一次，因此，需要修改部分命令，如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker commit sharelatex sharelatex/sharelatex:[version]-with-texlive-full</span><br><span class="line">echo [version]-with-texlive-full &gt; config/version</span><br></pre></td></tr></table></figure><h3 id="新增中文字体"><a href="#新增中文字体" class="headerlink" title="新增中文字体"></a>新增中文字体</h3><p>我们可以通过导入我们自己电脑中的中文字体做到这一步</p><ol><li><p>如何将自己电脑中的winfonts.tar.gz生成出来</p><ol><li><p><img src="https://s2.loli.net/2024/08/24/iWBOjIt7LwRvDSe.png" alt="字体文件夹"></p><p>找到上图的这个文件夹，复制走想要的字体（绝大部分中文字体），然后压缩为winfonts.tar.gz(任意格式)</p></li><li><p>ftp传到linux主机中就好</p></li></ol></li><li><p>继续参照参考资料2进入docker操作</p><ol><li><p>安装必要的包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it share</span><br><span class="line">apt update</span><br><span class="line">apt install -y latex-cjk-all texlive-lang-chinese texlive-lang-english</span><br><span class="line">apt install -y xfonts-wqy</span><br></pre></td></tr></table></figure></li><li><p>将所有字体文件移动到/usr/share/fonts/winfonts中并执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkfontscale</span><br><span class="line">mkfontdir</span><br><span class="line">fc-cache -fv</span><br></pre></td></tr></table></figure></li></ol></li></ol><h2 id="修复XeLatex"><a href="#修复XeLatex" class="headerlink" title="修复XeLatex"></a>修复XeLatex</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it sharelatex /bin/bash/</span><br><span class="line">apt update</span><br><span class="line">apt install -y texlive-xetex texlive-latex-extra texlive-science</span><br></pre></td></tr></table></figure><h3 id="刻录镜像"><a href="#刻录镜像" class="headerlink" title="刻录镜像"></a>刻录镜像</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat config/version # 该命令会得到[version]</span><br><span class="line">docker commit sharelatex sharelatex/sharelatex:[version]-with-texlive-full</span><br><span class="line">echo [version]-with-texlive-full &gt; config/version</span><br></pre></td></tr></table></figure><p>注意在编写中文文档时需要调整至XeLaTex，并且调用Ctex包</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL292ZXJsZWFmL3Rvb2xraXQ=">overleaf/toolkit (github.com)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIuYWxpeXVuLmNvbS9hcnRpY2xlLzE1NzI2MDY=">Linux 快速搭建 Overleaf 5.0 附中文字体及完整 TexLive 安装教程（2024最新版）-阿里云开发者社区 (aliyun.com)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly93d3cudG5uaWRtLmNvbS9idWlsZC1hbmQtdXNlLW92ZXJsZWFmLXNlcnZlci8=">搭建和使用overleaf服务器 | Tnnidm-Blog<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL292ZXJsZWFmL3Rvb2xraXQvYmxvYi9tYXN0ZXIvZG9jL2NlLXVwZ3JhZGluZy10ZXhsaXZlLm1kP3NwbT1hMmM2aC4xMjg3MzYzOS5hcnRpY2xlLWRldGFpbC4xNC41YTMzNjk0NVVtU01peCZmaWxlPWNlLXVwZ3JhZGluZy10ZXhsaXZlLm1k">toolkit/doc/ce-upgrading-texlive.md at master · overleaf/toolkit (github.com)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly95eG5jaGVuLmdpdGh1Yi5pby90ZWNobmlxdWUvRG9ja2Vy6YOo572yU2hhcmVMYVRlWOW5tueugOWNlemFjee9ruS4reaWh+eOr+Wigy8j5YeG5aSH5bel5L2c">Docker部署ShareLaTeX并简单配置中文环境 | YXN’s Blog (yxnchen.github.io)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9taXJyb3JzLmliaWJsaW8ub3JnL0NUQU4vbGFuZ3VhZ2UvY2hpbmVzZS9jdGV4L2N0ZXgucGRm">CTeX 宏集手册 (ibiblio.org)<i class="fa fa-external-link-alt"></i></span></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Overleaf-toolkit部署&quot;&gt;&lt;a href=&quot;#Overleaf-toolkit部署&quot; class=&quot;headerlink&quot; title=&quot;Overleaf-toolkit部署&quot;&gt;&lt;/a&gt;Overleaf-toolkit部署&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;在写latex的过程中，第一个使用的在线latex编辑器就是overleaf，但是在使用过程中受到诸多限制，遂欲购买overleafpro，但发现其学生价也要500yuan/year，实在是财力不足，但overleaf是开源的，要花钱的只是服务器算力以及各种模板，于是就有了部署Overleaf-toolkit的想法&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Overleaf是一个在线多用户协作的Latex编辑网站，其采用了如同markdown的“所见即所得”思想，采用及时更新编译的方式辅助编辑latex文件，同时通过项目结构约束文件的排布。是一个是十分学术且普遍应用的latex编辑网站。&lt;/p&gt;</summary>
    
    
    
    <category term="软件" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/"/>
    
    <category term="latex" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/latex/"/>
    
    <category term="overleaf" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/latex/overleaf/"/>
    
    
    <category term="latex" scheme="https://deepcity.github.io/tags/latex/"/>
    
    <category term="overleaf" scheme="https://deepcity.github.io/tags/overleaf/"/>
    
  </entry>
  
  <entry>
    <title>Docker镜像与容器使用</title>
    <link href="https://deepcity.github.io/2024/Docker%E9%95%9C%E5%83%8F%E4%B8%8E%E5%AE%B9%E5%99%A8%E4%BD%BF%E7%94%A8/article.html"/>
    <id>https://deepcity.github.io/2024/Docker%E9%95%9C%E5%83%8F%E4%B8%8E%E5%AE%B9%E5%99%A8%E4%BD%BF%E7%94%A8/article.html</id>
    <published>2024-08-25T09:16:26.000Z</published>
    <updated>2024-08-25T11:17:18.833Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker-Image-amp-amp-container"><a href="#Docker-Image-amp-amp-container" class="headerlink" title="Docker Image &amp;&amp; container"></a>Docker Image &amp;&amp; container</h1><h2 id="基础命令"><a href="#基础命令" class="headerlink" title="基础命令"></a>基础命令</h2><ol><li><code>docker stats</code> 该命令可以查看docker contianer对各种资源的占用量</li><li><code>docker image ls -a</code> 查看image状况 -a表示所有的</li><li><code>docker container ls -a</code> 查看container状况 -a表示查看所有的</li><li><code>docker container start/stop [container_id]</code>启停对应的container</li><li><code>docker container/image rm [container_id]/[image_id]</code>移除镜像或容器。</li><li><code>docker commit sharelatex sharelatex:version_tag</code> 提交容器为镜像</li></ol><span id="more"></span><h2 id="Image与Container的关系"><a href="#Image与Container的关系" class="headerlink" title="Image与Container的关系"></a>Image与Container的关系</h2><p>Container可以commit为Image</p><p>Image也可以run为Container</p><h3 id="什么是docker-image"><a href="#什么是docker-image" class="headerlink" title="什么是docker image"></a>什么是docker image</h3><p>Docker Image（Docker镜像）是用于创建Docker容器的只读模板。它包含了运行应用程序所需的所有内容，包括代码、依赖项、库、环境变量和配置文件。Docker镜像是容器的“蓝图”，容器是镜像的运行实例。</p><ol><li><p>Docker Image 是只读的</p></li><li><p>Docker Image的存储是分层的</p><p><img src="https://s2.loli.net/2024/08/25/eZ5XUC1zKHLRfJh.png" alt="DockerImages"></p><p>对如图的1,2,4镜像，通过tag显然可以发现他们的顺序是2-3-4，即总共占用8g而非16g</p></li></ol><h3 id="什么是Docker-Container"><a href="#什么是Docker-Container" class="headerlink" title="什么是Docker Container"></a>什么是Docker Container</h3><p>Docker Container（Docker容器）是一个独立运行的应用程序实例，它基于Docker镜像启动，并包含了运行应用所需的所有依赖、配置和环境。Docker容器是轻量级的、可移植的，它们在隔离的环境中运行，确保应用程序的行为在任何系统中都是一致的。</p><ol><li>Container是共享主机的操作系统内核的（但是同样可以使用不同的操作系统）</li><li>Container之间是隔离的</li><li>Container是独立的，并不基于除主机以外的东西</li><li>Container是可以短时（瞬时）启动和关闭的</li></ol><h3 id="如何启动Container"><a href="#如何启动Container" class="headerlink" title="如何启动Container"></a>如何启动Container</h3><h4 id="通过Docker源，网络下载启动容器"><a href="#通过Docker源，网络下载启动容器" class="headerlink" title="通过Docker源，网络下载启动容器"></a>通过Docker源，网络下载启动容器</h4><p>一般来讲，我们可以通过</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p [HostIP]:[HostPort]:[DockerPort] --name [ContainerName] [SoftwareName]:[version]</span><br></pre></td></tr></table></figure><p>启动一个特定的容器并将docker的某一个开放端口映射到主机上，其中-d使得容器的输出并不同步到主机上</p><h4 id="通过本地Image仓库启动容器"><a href="#通过本地Image仓库启动容器" class="headerlink" title="通过本地Image仓库启动容器"></a>通过本地Image仓库启动容器</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p [HostIP]:[HostPort]:[DockerPort] &lt;image_name&gt;:&lt;tag&gt;</span><br></pre></td></tr></table></figure><h3 id="如何将Container提交为Image"><a href="#如何将Container提交为Image" class="headerlink" title="如何将Container提交为Image"></a>如何将Container提交为Image</h3><p>相当简单</p><p>直接调用一条命令即可<code>docker commit sharelatex sharelatex:version_tag</code>即可</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Docker-Image-amp-amp-container&quot;&gt;&lt;a href=&quot;#Docker-Image-amp-amp-container&quot; class=&quot;headerlink&quot; title=&quot;Docker Image &amp;amp;&amp;amp; container&quot;&gt;&lt;/a&gt;Docker Image &amp;amp;&amp;amp; container&lt;/h1&gt;&lt;h2 id=&quot;基础命令&quot;&gt;&lt;a href=&quot;#基础命令&quot; class=&quot;headerlink&quot; title=&quot;基础命令&quot;&gt;&lt;/a&gt;基础命令&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;code&gt;docker stats&lt;/code&gt; 该命令可以查看docker contianer对各种资源的占用量&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker image ls -a&lt;/code&gt; 查看image状况 -a表示所有的&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker container ls -a&lt;/code&gt; 查看container状况 -a表示查看所有的&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker container start/stop [container_id]&lt;/code&gt;启停对应的container&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker container/image rm [container_id]/[image_id]&lt;/code&gt;移除镜像或容器。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker commit sharelatex sharelatex:version_tag&lt;/code&gt; 提交容器为镜像&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="软件" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/"/>
    
    <category term="Docker" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/Docker/"/>
    
    
    <category term="Docker" scheme="https://deepcity.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker安装与网络配置</title>
    <link href="https://deepcity.github.io/2024/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/article.html"/>
    <id>https://deepcity.github.io/2024/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/article.html</id>
    <published>2024-08-25T07:23:56.000Z</published>
    <updated>2024-08-27T08:03:17.215Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><p>在部署服务器时，经常在各式各样的仓库中见到Docker部署这一方式，开始不以为然，后来发现它极高的普及率与及其方便的部署方式吸引了我的注意。</p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuZG9ja2VyLmNvbS8=">docker<i class="fa fa-external-link-alt"></i></span>是一个开源的平台，主要用于开发、运输和运行应用程序。它通过使用容器（Containers）技术，使得应用程序能够在任何环境下以一致的方式运行，无论是在开发、测试，还是在生产环境中。</p><span id="more"></span><p>可以说，只要你配置好了docker，所有知名的软件就都向你敞开了大门。下面，从我是用docker的角度出发，讲一下实用的docker概念与使用技巧。</p><ol><li><p>docker与dockercompose</p><p>Docker 和 Docker Compose 是两个密切相关的工具，但它们的功能和用途有所不同，通常一起使用来管理容器化的应用程序。</p><p>Docker是一个基础平台，安装docker 通常讲指单纯安装这一组件，实际上，很多下载方式也都是这么做的，<strong>但我并不推荐这种方式</strong>，实际上就截至目前的体验，dockercompose的体验对新手是要好于docker的。</p><p>dockercompose是docker的具体独立工具依赖 Docker，但它是一个独立的工具，需要单独安装。</p></li><li><p>网络问题</p><p>由于发展中的问题，对docker的下载时常是困难的，且下载完整且最新的docker很可能是“<strong>有误导性</strong>”的，因为，如果你挂了镜像而且即使更新了镜像，也无从得知下载的版本是否正确，能否运行所需的软件。</p><p>这里还是推荐直接上机场或VPS直连通过官网方式不通过镜像来下载，虽然经济上可能有损耗，但总体来看，减少时间成本是多于增加的经济成本的。</p><p>然而，<strong>挂载了加速可能你也会发现</strong>，docker的网络并不通过我们的代理，事实上，docker并不走“常见”的网络代理，在linux中，<strong>在执行docker pull时，是由守护进程dockerd来执行。因此，代理需要配在dockerd的环境中。而这个环境，则是受systemd所管控，因此实际是systemd的配置。</strong> </p></li></ol><h2 id="安装最新Docker"><a href="#安装最新Docker" class="headerlink" title="安装最新Docker"></a>安装最新Docker</h2><p>若只想安装一个可用的源，截至2024/7/24，有一份教程在参考资料第6个。</p><h3 id="科学上网"><a href="#科学上网" class="headerlink" title="科学上网"></a>科学上网</h3><p>对于安装最新Docker，科学上网几乎是必备的，无论你使用的任何镜像（ps：除非是确保可信且不会过期的，包括国内部分知名互联网公司及大部分学校镜像都是无效或过期的）</p><p>比较知名的源有：Aliyun，清华，ustc中科大</p><p>参考文章<a href="Linux网络设置/article.html">Linux网络设置</a></p><h3 id="通过yum安装"><a href="#通过yum安装" class="headerlink" title="通过yum安装"></a>通过yum安装</h3><p>该命令可移除全部docker</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo yum remove docker \</span><br><span class="line">                docker-client \</span><br><span class="line">                docker-client-latest \</span><br><span class="line">                docker-common \</span><br><span class="line">                docker-latest \</span><br><span class="line">                docker-latest-logrotate \</span><br><span class="line">                docker-logrotate \</span><br><span class="line">                docker-engine</span><br></pre></td></tr></table></figure><ol><li><p>更新系统</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum update -y</span><br></pre></td></tr></table></figure></li><li><p>安装依赖包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y yum-utils \</span><br><span class="line">    device-mapper-persistent-data \</span><br><span class="line">    lvm2 </span><br></pre></td></tr></table></figure></li><li><p>安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure></li><li><p>额外设置与验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start docker</span><br><span class="line">sudo systemctl enable docker</span><br><span class="line">docker version</span><br></pre></td></tr></table></figure></li></ol><p>你可能已经发现了，即使通过以上方式下载了docker，也不一定是最新版，甚至相当可能是1.1x等超老版本，这是因为使用的yum镜像源的问题，国内很多镜像源都在许多年前“停止了维护”。但如果你的yum是国外源的话，很有可能没这个问题，也就不用看下面的内容了，查找最新版本请看docker官网（在参考资料中）</p><h3 id="通过dnf进行安装"><a href="#通过dnf进行安装" class="headerlink" title="通过dnf进行安装"></a>通过dnf进行安装</h3><p>以下操作需要保证你的yum仓库状态正常，并且联通外网，如果原本<code>yum makecache</code>就无法成功，那么dnf也是救不了的</p><p><img src="https://s2.loli.net/2024/08/23/riq6ygcF9jKfEvA.png" alt="[Supported platforms](https://docs.docker.com/desktop/install/linux-install/#supported-platforms)"></p><p>在docker官网里，我们可以查询到他所支持的Linux列表，而在Centos中，我们可以通过<code>cat /proc/version</code> <code>uname -a</code>,<code>cat /etc/os-release</code>分别查询到内核以及系统版本，而Centos7属于RHEL的克隆版本。因此尝试使用RHEL的安装方式，即官网描述的dnf新一代RPM库安装</p><ol><li><p>安装依赖</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install epel-release -y</span><br></pre></td></tr></table></figure></li><li><p>安装dnf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install dnf -y</span><br></pre></td></tr></table></figure></li><li><p>配置dnf，与添加常用的第三方库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dnf install &#x27;dnf-command(config-manager)&#x27;</span><br><span class="line">dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm</span><br><span class="line">dnf install pass</span><br><span class="line">dnf install gnome-shell-extension-appindicator # GNOME</span><br><span class="line">gnome-extensions enable appindicatorsupport@rgcjonas.gmail.com # GNOME</span><br><span class="line">sudo dnf install gnome-terminal # GNOME</span><br><span class="line">dnf install docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure></li><li><p>额外设置与验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start docker</span><br><span class="line">sudo systemctl enable docker</span><br><span class="line">docker version</span><br></pre></td></tr></table></figure></li></ol><h3 id="AliyunECS"><a href="#AliyunECS" class="headerlink" title="AliyunECS"></a>AliyunECS</h3><p>当服务器满足下图条件时，可通过扩展程序安装Docker</p><p><img src="https://s2.loli.net/2024/08/24/Eyqh8e6HoAnFVxr.png" alt="Aliyun"></p><p>详见对参考资料中阿里云的官方文档</p><h2 id="Docker的网络代理"><a href="#Docker的网络代理" class="headerlink" title="Docker的网络代理"></a>Docker的网络代理</h2><p><img src="https://s2.loli.net/2024/08/27/nPUjY3yCtleLosD.png" alt="docker与daemon在代理中的关系"></p><p><img src="https://s2.loli.net/2024/08/24/vYtQrRgiSsyd14E.png" alt="daemon与网络的关系"></p><p>事实上，daemon与client可以运行在两个主机上，在同一主机上运行时，真正与互联网进行交互的实际上是Daemon守护程序，也就是说，当我们普通的配置env进行代理的时候，我们配置的是Client与Daemon交互的代理，而配置Daemon后才能达到“端到端”的代理。</p><p>下面只阐述如何进行端到端的代理配置</p><p>在<code>/etc/systemd/system</code>该路径下寻找<code>docker.service.d/http-proxy.conf</code>,没有则新建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">Environment=&quot;HTTP_PROXY=http://IP:Port/&quot;</span><br><span class="line">Environment=&quot;HTTPS_PROXY=http://IP:Port/&quot;</span><br><span class="line">Environment=&quot;NO_PROXY=localhost,127.0.0.1,.example.com&quot;</span><br></pre></td></tr></table></figure><p>在这里，我是用的clash-for-linux的IP为127.0.0.1，Port为7890，可以通过 <a href="../Linux网络设置.md">Linux网络设置.md</a>设置。 </p><p>最后重启他俩即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h3 id="Dockers-Container的网络代理"><a href="#Dockers-Container的网络代理" class="headerlink" title="Dockers Container的网络代理"></a>Dockers Container的网络代理</h3><h3 id="直接在容器内添加代理"><a href="#直接在容器内添加代理" class="headerlink" title="直接在容器内添加代理"></a>直接在容器内添加代理</h3><p>这种方式如同在主机添加代理，步骤一致，但并不推荐，如果不是测试使用，最好还是保证容器的完整性。</p><blockquote><p>但如果需要该容器处处可用，修改环境变量也是可行的</p></blockquote><p>略</p><h3 id="在Docker17版本以上"><a href="#在Docker17版本以上" class="headerlink" title="在Docker17版本以上"></a>在Docker17版本以上</h3><p><code>Docker</code> 提供了一个<strong>全局的配置</strong>，<strong>可以通过配置 <code>Docker</code> 客户端以自动将代理信息传递给容器</strong>，从而让所有的容器内部都支持代理访问。</p><p>在<strong>启动容器的用户的主目录中</strong>创建或编辑文件 <code>~/.docker/config.json</code></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"> <span class="attr">&quot;proxies&quot;</span><span class="punctuation">:</span></span><br><span class="line"> <span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;default&quot;</span><span class="punctuation">:</span></span><br><span class="line">   <span class="punctuation">&#123;</span></span><br><span class="line">     <span class="attr">&quot;httpProxy&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://127.0.0.1:7890&quot;</span><span class="punctuation">,</span></span><br><span class="line">     <span class="attr">&quot;httpsProxy&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://127.0.0.1:7890&quot;</span><span class="punctuation">,</span></span><br><span class="line">     <span class="attr">&quot;noProxy&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*.test.example.com,.example2.com,127.0.0.0/8&quot;</span></span><br><span class="line">   <span class="punctuation">&#125;</span></span><br><span class="line"> <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>然后重启docker两兄弟即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0VsZWd5Y2xvdWQvY2xhc2gtZm9yLWxpbnV4LWJhY2t1cA==">Elegycloud/clash-for-linux-backup: 基于Clash Core 制作的Clash For Linux备份仓库 A Clash For Linux Backup Warehouse Based on Clash Core (github.com)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmRvY2tlci5jb20vZGVza3RvcC9pbnN0YWxsL2xpbnV4LWluc3RhbGwv">Install Docker Desktop on Linux | Docker Docs<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZG9ja2VyLmNvbS8=">Docker: Accelerated Container Application Development<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9jbi5saW51eC10ZXJtaW5hbC5jb20vP3A9NTE4MA==">如何在CentOS 7上安装DNF (linux-terminal.com)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9oZWxwLmFsaXl1bi5jb20vemgvZWNzL3VzZS1jYXNlcy9pbnN0YWxsLWFuZC11c2UtZG9ja2VyLW9uLWEtbGludXgtZWNzLWluc3RhbmNlIzI5OGE4YzZiZGMxOTM=">安装Docker并使用_云服务器 ECS(ECS)-阿里云帮助中心 (aliyun.com)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vY29kZW5vb2IvcC8xODI4MTk5Mg==">使用国内源安装新版docker（2024.7.3） - navist2020 - 博客园 (cnblogs.com)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BlbmcyaHVpMTMxNC9hcnRpY2xlL2RldGFpbHMvMTI0MjY3MzMz">快速设置 Docker 的三种网络代理配置_docker 代理-CSDN博客<i class="fa fa-external-link-alt"></i></span></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Docker&quot;&gt;&lt;a href=&quot;#Docker&quot; class=&quot;headerlink&quot; title=&quot;Docker&quot;&gt;&lt;/a&gt;Docker&lt;/h1&gt;&lt;p&gt;在部署服务器时，经常在各式各样的仓库中见到Docker部署这一方式，开始不以为然，后来发现它极高的普及率与及其方便的部署方式吸引了我的注意。&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly93d3cuZG9ja2VyLmNvbS8=&quot;&gt;docker&lt;i class=&quot;fa fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/span&gt;是一个开源的平台，主要用于开发、运输和运行应用程序。它通过使用容器（Containers）技术，使得应用程序能够在任何环境下以一致的方式运行，无论是在开发、测试，还是在生产环境中。&lt;/p&gt;</summary>
    
    
    
    <category term="软件" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/"/>
    
    <category term="Docker" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/Docker/"/>
    
    
    <category term="Docker" scheme="https://deepcity.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>CentOS运维记录</title>
    <link href="https://deepcity.github.io/2024/CentOS%E8%BF%90%E7%BB%B4%E8%AE%B0%E5%BD%95/article.html"/>
    <id>https://deepcity.github.io/2024/CentOS%E8%BF%90%E7%BB%B4%E8%AE%B0%E5%BD%95/article.html</id>
    <published>2024-08-25T07:23:35.000Z</published>
    <updated>2024-08-25T09:34:23.702Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CentOS运维"><a href="#CentOS运维" class="headerlink" title="CentOS运维"></a>CentOS运维</h1><p>记录我在CentOS中折腾的经验。</p><h2 id="存储空间不足"><a href="#存储空间不足" class="headerlink" title="存储空间不足"></a>存储空间不足</h2><p>在不断的使用CentOS虚拟机的过程中，docker对磁盘空间的庞大需求最终还是占满了我为学习准备的20g空间，因此，我对CentOS的磁盘开始了第一次折腾。</p><span id="more"></span><h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><p>写在前面为一些缩写给出解释</p><ol><li><p>LVM的基本组成</p><ol><li>物理卷 (PV，Physical Volume)<br>一个可供存储LVM的块设备. 如硬盘分区（MBR或GPT分区）、SAN 的硬盘、RAID 或 LUN，一个回环文件, 一个被内核映射的设备 (例如 dm-crypt)，它包含一个特殊的LVM头，它是 LVM 构建的实际硬件或存储系统。</li><li>卷组 (VG，Volume Group)<br>卷组是对一个或多个物理卷的集合，并在设备文件系统中显示为 /dev/VG_NAME。</li><li>逻辑卷 (LV，Logical Volume)<br>逻辑卷是可供系统使用的最终元设备，它们在卷组中创建和管理，由物理块组成，实际上就是一个虚拟分区，并显示为 /dev/VG_NAME/LV_NAME，通常在其上可以创建文件系统。</li><li>物理块 (PE，Physical Extends)<br>一个卷组中最小的连续区域(默认为4 MiB)，多个物理块将被分配给一个逻辑卷。你可以把它看成物理卷的一部分，这部分可以被分配给一个逻辑卷。</li></ol></li><li><p>xfs文件系统：一种自Centos7起使用的文件系统</p></li></ol><h3 id="常用命令阐述"><a href="#常用命令阐述" class="headerlink" title="常用命令阐述"></a>常用命令阐述</h3><ol><li><code>lsblk</code>该命令可以查看当前硬盘的分区状况</li><li><code>fdisk</code>该命令可以启动分盘程序，常用<code>fdisk -l</code>查看磁盘情况</li><li><code>pvdisplay</code>显示物理卷</li><li><code>vgdisplay</code>显示虚拟卷组</li><li><code>lvdisplay</code>显示逻辑卷</li><li><code>df</code>显示文件系统的容量以及挂载点位，常用参数<code>-h</code>显示适当的大小</li><li><code>pvcreate</code>创建物理分区</li><li><code>vgextend</code>扩容vg空间</li><li><code>lvextend</code> 扩容lv空间 <code>-L</code>指按照指定空间，<code>-l</code>指按照指定百分比</li><li><code>xfs_growfs</code>增加 XFS 文件系统的大小，必须挂载 XFS 文件系统，并且底层设备上必须有可用空间。</li></ol><h3 id="虚拟机扩容"><a href="#虚拟机扩容" class="headerlink" title="虚拟机扩容"></a>虚拟机扩容</h3><p>在Vmware虚拟机中，如果VM对硬盘预留了空间，那么可以直接二通过VMware进行扩容</p><p><img src="https://s2.loli.net/2024/08/25/JwhxWA1cV8niUu2.png" alt="VMware"></p><p>但很多情况下，这个方法都是不起作用的，尤其是分配了多块硬盘的情况。</p><h3 id="LVM扩容"><a href="#LVM扩容" class="headerlink" title="LVM扩容"></a>LVM扩容</h3><p>LVM是扩容时所使用的空间的格式，是Linux所特有的空间的处理方法。</p><ol><li><p>通过虚拟机或服务器提供商增加硬盘大小</p></li><li><p>为当前硬盘新建分区，并格式化为lvm</p><ol><li><code>lsblk</code>显示硬盘状态</li><li><code>fdisk [sdx]</code> 指定硬盘名调用fdisk系统</li><li>新建分区</li><li><code>t</code>指定<code>8e</code>lvm文件系统</li></ol></li><li><p>将新建硬盘加入vg</p><ol><li><p>创建pv，<code>pvcreate /dev/[sdx]</code> </p></li><li><p>合并到已有vg组</p><ol><li><code>vgdisplay</code> 查看当前vg</li><li>对欲增加容量的vg使用<code>vgextend [VG Name] /dev/[sdx]</code></li></ol></li><li><p>扩展lv空间</p><ol><li><p><code>lvdisplay</code>查看已有lv空间</p></li><li><p><code>df -h</code>查看对应挂载点空间</p><blockquote><p>如LV name是home，他的LV Path是/dev/centos/home。</p><p>假如我们想添加空间到/home中，可以在df -h的结果中看到其对应着/dev/mapper/centos-home</p></blockquote></li><li><p><code>lvextend -l/L +xx%/+xxG /dev/centos/xxxxx</code>扩展空间</p></li></ol></li><li><p><code>xfs_growfs</code>使新的空间可用</p></li></ol></li></ol><h2 id="掉网络问题"><a href="#掉网络问题" class="headerlink" title="掉网络问题"></a>掉网络问题</h2><p>见 <a href="../Linux网络设置/article.html">Linux网络设置.md</a> 中异常处理一章</p><h2 id="经验"><a href="#经验" class="headerlink" title="经验"></a>经验</h2><ol><li>多快照，尤其是当你修改/etc/目录下的一些配置文件，尤其是<code>/etc/fstab</code>该文件。</li><li>maintain模式下，小数字键盘是不起作用的，而且这时不会有提示告诉你有问题</li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><span class="exturl" data-url="aHR0cHM6Ly9jbG91ZC50ZW5jZW50LmNvbS9kZXZlbG9wZXIvYXJ0aWNsZS8yNDI0MTM2">Linux Centos系统 磁盘分区和文件系统管理 （深入理解）-腾讯云开发者社区-腾讯云 (tencent.com)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbGFiLXpqL3AvMTM0NzQ1NDkuaHRtbA==">VMware虚拟机（centos7）容量不足调整（LVM） - 小小小光子 - 博客园 (cnblogs.com)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vb3JhbmdlLUNDL3AvMTI3MTEwNzguaHRtbA==">存储系列之 XFS文件系统简介 - orange-C - 博客园 (cnblogs.com)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkxNTQzMS9hcnRpY2xlL2RldGFpbHMvMTIxODgxMDU0">Linux下的磁盘管理之LVM详解及lvm的常用磁盘操作命令_lvm命令-CSDN博客<i class="fa fa-external-link-alt"></i></span></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;CentOS运维&quot;&gt;&lt;a href=&quot;#CentOS运维&quot; class=&quot;headerlink&quot; title=&quot;CentOS运维&quot;&gt;&lt;/a&gt;CentOS运维&lt;/h1&gt;&lt;p&gt;记录我在CentOS中折腾的经验。&lt;/p&gt;
&lt;h2 id=&quot;存储空间不足&quot;&gt;&lt;a href=&quot;#存储空间不足&quot; class=&quot;headerlink&quot; title=&quot;存储空间不足&quot;&gt;&lt;/a&gt;存储空间不足&lt;/h2&gt;&lt;p&gt;在不断的使用CentOS虚拟机的过程中，docker对磁盘空间的庞大需求最终还是占满了我为学习准备的20g空间，因此，我对CentOS的磁盘开始了第一次折腾。&lt;/p&gt;</summary>
    
    
    
    <category term="软件" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/"/>
    
    <category term="Linux" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/Linux/"/>
    
    
    <category term="Linux" scheme="https://deepcity.github.io/tags/Linux/"/>
    
    <category term="CentOS" scheme="https://deepcity.github.io/tags/CentOS/"/>
    
  </entry>
  
  <entry>
    <title>Linux网络设置</title>
    <link href="https://deepcity.github.io/2024/Linux%E7%BD%91%E7%BB%9C%E8%AE%BE%E7%BD%AE/article.html"/>
    <id>https://deepcity.github.io/2024/Linux%E7%BD%91%E7%BB%9C%E8%AE%BE%E7%BD%AE/article.html</id>
    <published>2024-08-25T07:23:20.000Z</published>
    <updated>2024-08-27T08:07:33.479Z</updated>
    
    <content type="html"><![CDATA[<h1 id="linux的网络配置">Linux的网络配置</h1><p>在我反复鼓捣Linux的过程中，时常遇到Linux在网络配置上的问题，有个互联网笑话是“老钟是最擅长计算机网络的”。既然如此，我也有了一个想法——记录下我在Linux网络配置中所遭遇的坑。硬件主要集中于LinuxECS服务器与VMware虚拟机，软件版本则是以CentOS7，Ubuntu18.04为主。</p><h2 id="基础网络命令">基础网络命令</h2><ol type="1"><li><code>ifconfig</code>最基本最常用的命令，用于查看本地网卡的信息，一般查看值为IP或者本地物理地址</li><li><code>ping</code>简单的测试网络丢包率，有时<code>curl</code>命令对网站的检测更为准确，因为两者对于代理的检测并不完全一致</li><li><code>netstat</code>用于查看本地端口的状态信息，一般用于查看当前那些端口是开放的，那些端口是被某个特定应用监控的，一般搭配<code>grep</code>使用</li><li><code>traceroute</code>针对特定目标地址的报文转发追踪，基于IP报文中的生存时间TTL实现，一般用于查看对特定网络服务是否联通，也可用于查看本机某个端口是否对外界开放，最经典的例子就是mail端口25，这个端口国内很少开放</li><li><code>whois</code>查看域名建立时间，有效期等，一般用于查看网站的归属信息以及ssl证书的有效期</li><li><code>host</code>查看有关目的地址的信息，既可以通过IP查看域名，也可以通过域名查看IP</li><li><code>ifdown</code><code>ifup</code>,这两个命令用于对特定网卡的关启</li><li><code>nmtui</code> 通过调用一个内置的网卡配置图形化界面配置网卡</li><li><code>systemctl start/stop/enable/restart NetworkManager</code>这是一个特殊的通过systemctl控制网络服务以实现对网络进行开关的命令，常用于对<code>/etc/NetworkManager/NetworkManager.conf</code>该文件进行修改后的重新配置网络</li><li><code>env | grep -E 'http_proxy|https_proxy'</code>这条命令比较特殊，一般用于查看当前的代理</li></ol><span id="more"></span><h2 id="如何科学上网">如何科学上网</h2><h3 id="windows平台">Windows平台</h3><p>对于Windows平台，该平台有大量用户基础，并且生态良好，很容易找到代理上网的软件以及节点。（初接触者千万区分这两者，代理是代理也称机场，软件是软件）。两者一个收费最好，一个完全免费。</p><h4 id="推荐软件">推荐软件</h4><p>V2ray、ClashForWindows等</p><h3 id="linux平台">Linux平台</h3><p>推荐clash-for-linux，一个个人自用的备份库</p><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0VsZWd5Y2xvdWQvY2xhc2gtZm9yLWxpbnV4LWJhY2t1cA==">Elegycloud/clash-for-linux-backup:基于Clash Core 制作的Clash For Linux备份仓库 A Clash For Linux BackupWarehouse Based on Clash Core (github.com)<i class="fa fa-external-link-alt"></i></span></p><p>根据md文档操作即可</p><p>注：</p><ol type="1"><li>这样的操作一般用于加速对github的下载</li><li>配置其他的服务代理一般需要特别配置，因为很多服务实际上是通过守护进程来进行网络通信的</li></ol><h4 id="常用的软件配置">常用的软件配置</h4><h5 id="yum">yum</h5><p>注意对yum文件的禁用修改方式，不要使用rm，而是加文件名后缀禁用文件</p><ol type="1"><li><p>全局修改</p><p>修改/etc/yum.conf文件即可，在结尾添加你的本地代理网址</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">proxy=http://IP:Port</span><br><span class="line">// 在本文中为</span><br><span class="line">proxy=http://127.0.0.1：7890</span><br></pre></td></tr></table></figure></li><li><p>对特定的仓库启用代理</p><p>比如<code>CentOS-Base.repo</code>中有三个仓库：base、updates和extras。我们只想给base仓库设置代理，则只需要在对应的仓库后面添加一行，如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[base]</span><br><span class="line">name=CentOS-$releasever - Base</span><br><span class="line">#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=os&amp;infra=$infra</span><br><span class="line">baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</span><br><span class="line"># 下一行为新增,效果为base仓库走代理</span><br><span class="line">proxy=http://ip:port</span><br><span class="line"># 下注释一行为base不走代理</span><br><span class="line"># proxy=_none_</span><br><span class="line">#released updates </span><br><span class="line">[updates]</span><br><span class="line">name=CentOS-$releasever - Updates</span><br><span class="line">#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updates&amp;infra=$infra</span><br><span class="line">baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</span><br><span class="line"></span><br><span class="line">#additional packages that may be useful</span><br><span class="line">[extras]</span><br><span class="line">name=CentOS-$releasever - Extras</span><br><span class="line">#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extras&amp;infra=$infra</span><br><span class="line">baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=file:///etc/pki/rpm-</span><br><span class="line">gpg/RPM-GPG-KEY-CentOS-7</span><br></pre></td></tr></table></figure></li></ol><h5 id="docker">docker</h5><p>按照需求修改文件即可，详见</p><p><a href="Docker安装与网络配置/article.html">Docker.md</a></p><h3 id="无广告个人自用机场">无广告，个人自用机场</h3><p><span class="exturl" data-url="aHR0cHM6Ly9qdXN0bXlzb2Nrcy5hcHAv">Just My Socks官网<i class="fa fa-external-link-alt"></i></span></p><h2 id="异常修复">异常修复</h2><p><strong>重装系统或恢复快照</strong></p><p>在花式乱搞后，虚拟机（服务器）的网络很容易会出现问题，比如，完全看不到虚拟机的网络选项，它有网卡但是却根本不获取ip！</p><figure><img src="https://s2.loli.net/2024/08/24/uRIr2VMaYvtkFhH.png"alt="Error" /><figcaption aria-hidden="true">Error</figcaption></figure><p>这种情况一般都是 network这个网卡服务出现了问题</p><ol type="1"><li><p>找到错误</p><p>查找虚拟机设置或者通过其他命令与文件查找到错误之后，通过<code>systemctl restart network</code>验证该服务是否出错</p></li><li><p>排除关联</p><p>通过</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line">systemctl start network.service</span><br></pre></td></tr></table></figure><p>成功启动服务，确定network服务启动失败与NetworkManager之间的关联</p><p>但可以观察到，此时问题仍未解决，虚拟机仍无法上网，但是通过<code>ifconfig ens33</code>（ens33为网卡名）可以发现dhcp正常工作，更进一步的，我们从虚拟机ping主机也是可以ping通的</p></li><li><p>尝试解决上网问题</p><p>每种vm切换连接方式都解决不了问题，虚拟机仍无法上网，但可以发现，每次启动NetworkManager服务，就会导致虚拟机失去IP从而无法通信</p></li></ol><p>可见NetworkManager与network之间发生了一些冲突，导致两个服务相互影响使得网络处于要么能分配ip但上不了网，要么根本没ip的局面。</p><figure><img src="https://s2.loli.net/2024/08/24/1K8cfE5atGrgeAY.png"alt="xe" /><figcaption aria-hidden="true">xe</figcaption></figure><p>尝试查看错误信息可以发现其中奥秘。可以发现这一切都与一个名叫lo的本地环回接口有关，它被设置为不受管理。</p><p>在我询问chatgpt后，我得到了一种解决方式</p><p><strong>这种方式使得NetworkManager与network服务同时进行，可以访问网络，但无网络连接图标</strong>（虚拟机环境）</p><p>具体方式为</p><ol type="1"><li><p>确保 <code>lo</code> 接口没有被 NetworkManager管理。<code>/etc/NetworkManager/NetworkManager.conf</code>文件中修改配置</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="meta">main</span>]</span><br><span class="line">plugins=keyfile</span><br><span class="line"></span><br><span class="line">[<span class="meta">keyfile</span>]</span><br><span class="line"><span class="keyword">unmanaged</span>-devices=<span class="keyword">interface</span>-<span class="title">name</span>:<span class="title">lo</span></span><br></pre></td></tr></table></figure></li><li><p>清除可能的网络配置缓存</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr flush dev ens33</span><br><span class="line">sudo ip route flush table main</span><br></pre></td></tr></table></figure></li><li><p>重启服务</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart NetworkManager</span><br><span class="line">sudo systemctl restart network.service</span><br></pre></td></tr></table></figure></li></ol><p><del>对于虚拟机的网络显示问题，目前还未找到好的解决方式，网络上博主表示直接重装系统即可。。。</del>找到方法了，直接挂起然后启动就好，很神奇，重启解决不了，挂起能解决。。。</p><blockquote><p>网络管理器(NetworManager)是检测网络、自动连接网络的程序。无论是无线还是有线连接，它都可以令您轻松管理。对于无线网络,网络管理器优先连接已知的网络并可以自动切换到最可靠的无线网络。利用网络管理器的程序可以自由切换在线和离线模式。网络管理器会相对无线网络优先选择有线网络，支持VPN。网络管理器最初由 Redhat 公司开发，现在由 GNOME 管理。</p><p>NetworkManager由一个管理系统网络连接、并且将其状态通过D-BUS（是一个提供简单的应用程序互相通讯的途径的自由软件项目，它是作为freedesktoporg项目的一部分来开发的。）进行报告的后台服务，以及一个允许用户管理网络连接的客户端程序。</p></blockquote><h2 id="参考文献">参考文献</h2><ol type="1"><li><span class="exturl" data-url="aHR0cHM6Ly9wc2hpemhzeXN1LmdpdGJvb2suaW8vbGludXgveXVtL3dlaS15dW0teXVhbi1wZWktemhpLWRhaS1saQ==">为yum源配置代理| linux (gitbook.io)<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vd2FuZ2Z4L3AvMTcyOTM0NzIuaHRtbA==">解决centos7网卡启动失败解决（亲测有效！！）- 王飞侠 - 博客园 (cnblogs.com)<i class="fa fa-external-link-alt"></i></span></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;linux的网络配置&quot;&gt;Linux的网络配置&lt;/h1&gt;
&lt;p&gt;在我反复鼓捣Linux的过程中，时常遇到Linux在网络配置上的问题，有个互联网笑话是“老钟是最擅长计算机网络的”。既然如此，我也有了一个想法——记录下我在Linux网络配置中所遭遇的坑。硬件主要集中于LinuxECS服务器与VMware虚拟机，软件版本则是以CentOS7，Ubuntu18.04为主。&lt;/p&gt;
&lt;h2 id=&quot;基础网络命令&quot;&gt;基础网络命令&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;code&gt;ifconfig&lt;/code&gt;最基本最常用的命令，用于查看本地网卡的信息，一般查看值为IP或者本地物理地址&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ping&lt;/code&gt;
简单的测试网络丢包率，有时&lt;code&gt;curl&lt;/code&gt;命令对网站的检测更为准确，因为两者对于代理的检测并不完全一致&lt;/li&gt;
&lt;li&gt;&lt;code&gt;netstat&lt;/code&gt;用于查看本地端口的状态信息，一般用于查看当前那些端口是开放的，那些端口是被某个特定应用监控的，一般搭配&lt;code&gt;grep&lt;/code&gt;使用&lt;/li&gt;
&lt;li&gt;&lt;code&gt;traceroute&lt;/code&gt;针对特定目标地址的报文转发追踪，基于IP报文中的生存时间TTL实现，一般用于查看对特定网络服务是否联通，也可用于查看本机某个端口是否对外界开放，最经典的例子就是mail端口25，这个端口国内很少开放&lt;/li&gt;
&lt;li&gt;&lt;code&gt;whois&lt;/code&gt;
查看域名建立时间，有效期等，一般用于查看网站的归属信息以及ssl证书的有效期&lt;/li&gt;
&lt;li&gt;&lt;code&gt;host&lt;/code&gt;查看有关目的地址的信息，既可以通过IP查看域名，也可以通过域名查看IP&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ifdown&lt;/code&gt;
&lt;code&gt;ifup&lt;/code&gt;,这两个命令用于对特定网卡的关启&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nmtui&lt;/code&gt; 通过调用一个内置的网卡配置图形化界面配置网卡&lt;/li&gt;
&lt;li&gt;&lt;code&gt;systemctl start/stop/enable/restart NetworkManager&lt;/code&gt;这是一个特殊的通过systemctl控制网络服务以实现对网络进行开关的命令，常用于对&lt;code&gt;/etc/NetworkManager/NetworkManager.conf&lt;/code&gt;该文件进行修改后的重新配置网络&lt;/li&gt;
&lt;li&gt;&lt;code&gt;env | grep -E &#39;http_proxy|https_proxy&#39;&lt;/code&gt;这条命令比较特殊，一般用于查看当前的代理&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="软件" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/"/>
    
    <category term="Linux" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/Linux/"/>
    
    
    <category term="Linux" scheme="https://deepcity.github.io/tags/Linux/"/>
    
    <category term="网络" scheme="https://deepcity.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>算法数论基础</title>
    <link href="https://deepcity.github.io/2024/%E7%AE%97%E6%B3%95%E6%95%B0%E8%AE%BA%E5%9F%BA%E7%A1%80/article.html"/>
    <id>https://deepcity.github.io/2024/%E7%AE%97%E6%B3%95%E6%95%B0%E8%AE%BA%E5%9F%BA%E7%A1%80/article.html</id>
    <published>2024-08-24T08:38:26.000Z</published>
    <updated>2024-08-27T08:33:08.752Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言需要用到的偏僻语法知识">前言：需要用到的偏僻语法知识</h1><h2 id="c随机数函数">c++随机数函数</h2><p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80NDIwMDg1ODk=">如何优雅的用C++生成随机数 - 知乎 (zhihu.com)<i class="fa fa-external-link-alt"></i></span></p><p>头文件：c: cstdlib c++: random</p><p><code>cstdlib</code> 中的 rand（）和 srand（）函数是 C语言使用的随机数生成方法，通过 <strong><em>线性同余法</em></strong>计算。</p><span id="more"></span><blockquote><p>srand 常用时间作为种子</p></blockquote><p>C++标准建议使用 <code>random</code> 代替它们。</p><h3 id="随机数生成引擎-random-number-engines">随机数生成引擎<strong>Random number engines</strong></h3><p><code>random</code> 提供了三种引擎，使用哪种需要权衡：</p><ul><li>linear_congruential_engine（线性同余法）：速度比较快，储存很少的中间变量。</li><li>mersenne_twister_engine：比较慢，占用存储空间较大，但是在参数设置合理的情况下，可生成最长的不重复序列，且具有良好的频谱特征。</li><li>subtract_with_carry_engine：速度最快，占用存储空间较大，频谱特性有时不佳。</li></ul><h3 id="预定义算法">预定义算法</h3><p>算法包括minstd_rand0、minstd_rand、mt19937、mt19937_64、ranlux24_base、ranlux48_base等。</p><p>以下是费马小定理素性检验的随机数实际应用</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;random&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function">mt19937 <span class="title">eng</span><span class="params">(time(<span class="literal">nullptr</span>))</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">randint</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="function">uniform_int_distribution&lt;<span class="type">int</span>&gt; <span class="title">dis</span><span class="params">(a, b)</span></span>;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">dis</span>(eng);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">quickPow</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> n, <span class="type">int</span> p)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="type">int</span> res = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (n)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (n &amp; <span class="number">1</span>)res = (ll)res * x%p;</span><br><span class="line">x = (ll)x * x%p;</span><br><span class="line">n &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">isPrime</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (x &lt; <span class="number">3</span>)<span class="keyword">return</span> x == <span class="number">2</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">12</span>; i++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a = <span class="built_in">randint</span>(<span class="number">2</span>, x - <span class="number">1</span>);</span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">quickPow</span>(a, x - <span class="number">1</span>, x) != <span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">isPrime</span>(<span class="number">9997579</span>))<span class="built_in">puts</span>(<span class="string">&quot;YES&quot;</span>);</span><br><span class="line"><span class="keyword">else</span> <span class="built_in">puts</span>(<span class="string">&quot;NO&quot;</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr /><p><strong>以下代码中将不会给出例如：randint, quickpow的代码（图个方便）</strong></p><h1 id="gcd-与-lcd-以及其衍生的素数判定问题">GCD 与 LCD以及其衍生的素数判定问题</h1><h2 id="gcd-的数学推导">GCD 的数学推导</h2><h3 id="取模运算">取模运算</h3><p>amodp 表示 a 除以 p 的余数</p><p><strong>模 ｐ 加法</strong> <span class="math inline">\((a+b)mod\ p =(amodp+bmodp)\ mod\ p\)</span></p><p><strong>模 p 减法</strong> <span class="math inline">\((a-b)mod\ p =(amodp-bmodp + p)\ mod\ p\)</span></p><p>注意在这里有个很容易犯得错误，在数学中，我们称mod是不会结果为负数得，但在计算机中，对负数进行去摸结果仍是负数。</p><blockquote><p>例如：</p><p>对-1进行取模，结果为n-1，而在计算机中，结果仍为-1</p></blockquote><p><strong>模 p 乘法</strong> <span class="math inline">\((a*b)mod\ p =(amodp*bmodp)mod\ p\)</span></p><p><strong>幂模 p</strong> <spanclass="math inline">\((a^b)modp=((amodp)^b)modp\)</span></p><p>  模运算满足结合律、交换律和分配律。<spanclass="math inline">\(a=b(mod\ n)\)</span> 表示 $ a$ 和 <spanclass="math inline">\(b\)</span> 模 <spanclass="math inline">\(n\)</span> 同余，即 <spanclass="math inline">\(a\)</span> 和 <spanclass="math inline">\(b\)</span> 除以 <spanclass="math inline">\(n\)</span> 的余数相等。</p><h3 id="最大公约数">最大公约数</h3><p>gcd 即最大公约数 lcm 即最小公倍数</p><p>## gcd 的具体实现与算法优化</p><figure><img src="https://s2.loli.net/2024/08/24/hjFAPLWQ98seZr5.png"alt="image-20220508102231840" /><figcaption aria-hidden="true">image-20220508102231840</figcaption></figure><p><strong>gcd 的基础实现</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">gcd</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> b ? <span class="built_in">gcd</span>(b, a % b) : a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>gcd 的运算符优化</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">int gcd(int a, int b) &#123;</span><br><span class="line">    // make sure a &gt;= b.</span><br><span class="line">    if (a &lt; b) &#123;</span><br><span class="line">        std::swap(a, b);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (b == 0) &#123;</span><br><span class="line">        return a;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    bool a_isodd = a &amp; 1;</span><br><span class="line">    bool b_isodd = b &amp; 1;</span><br><span class="line"></span><br><span class="line">    if (a_isodd &amp;&amp; b_isodd) &#123;</span><br><span class="line">        return gcd((a - b) &gt;&gt; 1, b);</span><br><span class="line">    &#125; else if (a_isodd &amp;&amp; !b_isodd) &#123;</span><br><span class="line">        return gcd(a, b &gt;&gt; 1);</span><br><span class="line">    &#125; else if (!a_isodd &amp;&amp; b_isodd) &#123;</span><br><span class="line">        return gcd(a &gt;&gt; 1, b);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // both a and b are even numbers.</span><br><span class="line">    return gcd(a &gt;&gt; 1, b &gt;&gt; 1) &lt;&lt; 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="素数检验算法">素数检验算法</h2><h3 id="纯暴力-o-n">纯暴力 O( n )</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">isprime</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">2</span>;i&lt;=x/i;i++)</span><br><span class="line"><span class="keyword">if</span>(x%i==<span class="number">0</span>)<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="费马素性检验">费马素性检验</h3><p>那么反过来呢？如果存在某个 <span class="math inline">\(a^{p-1}\equiv1(mod\ p)\)</span>，是否就能判定 <span class="math inline">\(p\)</span>是素数呢？<strong>并不行</strong>，例如 <spanclass="math inline">\(2^{341-1}\equiv 1 (mod341)\)</span>，但 <spanclass="math inline">\(341\)</span> 是合数，满足该同余等式的合数被称为<strong>费马伪素数</strong>。</p><p>幸好，一个合数是费马伪素数的概率并不是很高。所以我们可以多测试几个<span class="math inline">\(a\)</span>。只要存在某个 <spanclass="math inline">\(a^{p-1}\not \equiv 1 (mod\ p)\)</span>，即可说明<span class="math inline">\(p\)</span> 不是素数。而如果多组测试下来<span class="math inline">\(a^{p-1}\equiv 1\)</span> 都成立，那它就<strong>很可能</strong> 是素数了。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">isPrime</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (x &lt; <span class="number">3</span>)<span class="keyword">return</span> x == <span class="number">2</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">12</span>; i++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a = <span class="built_in">randint</span>(<span class="number">2</span>, x - <span class="number">1</span>);</span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">quickPow</span>(a, x - <span class="number">1</span>, x) != <span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="米勒-拉宾素性检验">米勒-拉宾素性检验</h3><p>对于待检验的数为偶数，可直接判断其为非素数，而奇数则可写成 <spanclass="math inline">\(a^{x-1}\)</span> 表示为 <spanclass="math inline">\(a^{2^rd}\)</span> 当考虑 x 为奇数时，<spanclass="math inline">\(a^d,a^{2d},...,a^{2^rd}\)</span>这样一串数字的性质。</p><p>我们已经知道对奇素数 <span class="math inline">\(x\)</span>，<spanclass="math inline">\(a^{2^rd} \equiv 1 (mod\ x)\)</span> （a 是 x的倍数的情况下特判)，也就是说这串数字以 1 结尾，由于 x 是奇素数，且<span class="math inline">\(1^{\frac{x-1}{2}} \equiv 1 (mod\x)\)</span></p><h1 id="素数筛">素数筛</h1><p>##　暴力判断</p><h2 id="埃氏筛法">埃氏筛法</h2><h2 id="欧拉筛法">欧拉筛法</h2><p>上面得三种筛法都是十分简单而且基础得。这里就不多阐述了。</p><h2 id="不能秒杀的题">不能秒杀的题</h2><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9wcm9ibGVtL2NvbnRlbnQvMTI5NS8=">1293夏洛特和他的女朋友<i class="fa fa-external-link-alt"></i></span></p><p>看到这道题，最先开始的想法，对每个数与其质因子连一条线，对建出来的图，做染色，保证每一条边的两个端点是不同颜色</p><p>但这并非正解，对于每一条边，其两端的链接必定是一个合数和一个素数，因此该图为一个二分图</p><figure><img src="https://s2.loli.net/2024/08/24/zKgVIluDya9mUxJ.png"alt="image-20220519164522508" /><figcaption aria-hidden="true">image-20220519164522508</figcaption></figure><p>剩下的很简单，n &lt;3时ans=1，n&gt; = 3 时 ans = 2；</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1000006</span>;</span><br><span class="line"><span class="type">int</span> prime[N], countNum;</span><br><span class="line"><span class="type">bool</span> st[N];</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">oula</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">2</span>; i &lt;= N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (!st[i])prime[countNum++] = i;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; prime[j] * i &lt;= n; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            st[i * prime[j]] = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span> (i % prime[j] == <span class="number">0</span>)<span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">oula</span>(N);</span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="keyword">if</span>(n&lt;<span class="number">3</span>)&#123;</span><br><span class="line">        <span class="built_in">puts</span>(<span class="string">&quot;1&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)<span class="built_in">printf</span>(<span class="string">&quot;1 &quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">puts</span>(<span class="string">&quot;2&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">2</span>;i &lt;= n + <span class="number">1</span>;i++)</span><br><span class="line">            <span class="keyword">if</span>(st[i]) <span class="built_in">printf</span>(<span class="string">&quot;2 &quot;</span>);</span><br><span class="line">            <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">&quot;1 &quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="素数筛思想的实际应用">素数筛思想的实际应用</h2><p>对于每个区间内的所有数，合数必有一个sqrt（a）的因子，故可通过预处理出 5*10^4内的所有素数，再通过筛查质因子处理出每一个合数</p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9wcm9ibGVtL2NvbnRlbnQvMTk4Lw==">196质数距离<i class="fa fa-external-link-alt"></i></span></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1000010</span>;</span><br><span class="line"><span class="type">int</span> l, r;</span><br><span class="line"><span class="type">int</span> prime[N], countNum;</span><br><span class="line"><span class="type">bool</span> st[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">memset</span>(st, <span class="number">0</span>, <span class="keyword">sizeof</span> st);</span><br><span class="line">    countNum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">2</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (!st[i])prime[countNum++] = i;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; prime[j] * i &lt;= n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            st[prime[j] * i] = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span> (i % prime[j] == <span class="number">0</span>)<span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (cin &gt;&gt; l &gt;&gt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">init</span>(<span class="number">50000</span>);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">memset</span>(st, <span class="number">0</span>, <span class="keyword">sizeof</span> st);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; countNum; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            ll p = prime[i];</span><br><span class="line">            <span class="keyword">for</span> (ll j = <span class="built_in">max</span>(p * <span class="number">2</span>, (l + p - <span class="number">1</span>) / p * p); j &lt;= r; j += p)</span><br><span class="line">                st[j - l] = <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        countNum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;= r - l; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (!st[i] &amp;&amp; i + l &gt;= <span class="number">2</span>)</span><br><span class="line">                prime[countNum++] = i + l;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (countNum &lt; <span class="number">2</span>)<span class="built_in">puts</span>(<span class="string">&quot;There are no adjacent primes.&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">int</span> minp = <span class="number">0</span>, maxp = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; countNum - <span class="number">1</span>; i++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="type">int</span> d = prime[i + <span class="number">1</span>] - prime[i];</span><br><span class="line">                <span class="keyword">if</span> (d &lt; prime[minp + <span class="number">1</span>] - prime[minp])minp = i;</span><br><span class="line">                <span class="keyword">if</span> (d &gt; prime[maxp + <span class="number">1</span>] - prime[maxp])maxp = i;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d,%d are closest, %d,%d are most distant.\n&quot;</span>,</span><br><span class="line">                prime[minp], prime[minp + <span class="number">1</span>], prime[maxp], prime[maxp + <span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="分解质因数">分解质因数</h1><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9wcm9ibGVtL2NvbnRlbnQvMTk5Lw==">197阶乘分解<i class="fa fa-external-link-alt"></i></span></p><p>最先开始想的是对于每一个 n以内的数都枚举一下，记录其对各个质数的约数，但这样就会有 1e6*1e5（1e6以内的素数个数）的时间复杂度，仍然超限，因此，转换一下思路，枚举素数，对每个该素数的倍数加入s 值，即： <span class="math display">\[s = n/p + n/p^2 + n/p^3 + n/p^4...\]</span></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1000006</span>;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">int</span> prime[N],countNum;</span><br><span class="line"><span class="type">bool</span> st[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">2</span>;i&lt;=n;i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(!st[i])prime[countNum++]=i;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;prime[j]*i&lt;=n;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            st[i*prime[j]]=<span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span>(i%prime[j]==<span class="number">0</span>)<span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">init</span>(<span class="number">1000000</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;countNum;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(n&lt;prime[i])<span class="keyword">break</span>;</span><br><span class="line">        ll p=prime[i],s=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(n&gt;=p)s+=n/p,p*=prime[i];</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d %d\n&quot;</span>,prime[i],s);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="约数之和">约数之和</h1><p>约数定理 n 的 <spanclass="math inline">\((a_1+1)(a_2+1)...(a_k+1)\)</span> 个正约数之和为<spanclass="math inline">\((p_1^0+p_1^1+...p_1^{a1})(p_2^0+p_2^1+...+p_2^{a2})...(p_k^0+p_k^1+..+p_k^{ak})\)</span></p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9hY3Rpdml0eS9jb250ZW50L3Byb2JsZW0vY29udGVudC84MDQ2Lw==">AcWing97. 约数之和（算法提高课） - AcWing<i class="fa fa-external-link-alt"></i></span></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;cstdio&gt;</span><br><span class="line"></span><br><span class="line">const int mod = 9901;</span><br><span class="line"></span><br><span class="line">int qmi(int a, int k)</span><br><span class="line">&#123;</span><br><span class="line">    int res = 1;</span><br><span class="line">    a %= mod;</span><br><span class="line">    while (k)</span><br><span class="line">    &#123;</span><br><span class="line">        if (k &amp; 1) res = res * a % mod;</span><br><span class="line">        a = a * a % mod;</span><br><span class="line">        k &gt;&gt;= 1;</span><br><span class="line">    &#125;</span><br><span class="line">    return res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int sum(int p, int k)</span><br><span class="line">&#123;</span><br><span class="line">    if (k == 1) return 1;</span><br><span class="line">    if (k % 2 == 0) return (1 + qmi(p, k / 2)) * sum(p, k / 2) % mod;</span><br><span class="line">    return (sum(p, k - 1) + qmi(p, k - 1)) % mod;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int a, b;</span><br><span class="line">    scanf(&quot;%d%d&quot;, &amp;a, &amp;b);</span><br><span class="line"></span><br><span class="line">    int ans = 1;</span><br><span class="line">    for (int i = 2; i * i &lt;= a; i ++ )</span><br><span class="line">        if (a % i == 0)</span><br><span class="line">        &#123;</span><br><span class="line">            int s = 0;</span><br><span class="line">            while (a % i == 0)</span><br><span class="line">            &#123;</span><br><span class="line">                a /= i, s ++ ;</span><br><span class="line">            &#125;</span><br><span class="line">            ans = ans * sum(i, b * s + 1) % mod;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    if (a &gt; 1) ans = ans * sum(a, b + 1) % mod;</span><br><span class="line">    if (a == 0) ans = 0;</span><br><span class="line"></span><br><span class="line">    printf(&quot;%d\n&quot;, ans);</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="约数个数">约数个数</h1><p>暴力求约数个数</p><h2id="求给定元素集中每一个数字的约数个数">求给定元素集中每一个数字的约数个数</h2><p>利用筛法思想，对于每一个数字枚举他的倍数</p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9wcm9ibGVtL2NvbnRlbnQvMTI5My8=">1291轻拍牛头<i class="fa fa-external-link-alt"></i></span></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1000010</span>;</span><br><span class="line"><span class="type">int</span> a[N],cnt[N],s[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)&#123;</span><br><span class="line">        cin&gt;&gt;a[i];</span><br><span class="line">        cnt[a[i]]++;</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;N;i++)</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j=i;j&lt;N;j+=i)</span><br><span class="line">    &#123;</span><br><span class="line">        s[j]+=cnt[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++) cout&lt;&lt;s[a[i]]<span class="number">-1</span>&lt;&lt;endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="阶乘平方的质数个数">阶乘、平方的质数个数</h2><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9wcm9ibGVtL2NvbnRlbnQvMTI5Ni8=">1294樱花<i class="fa fa-external-link-alt"></i></span></p><p><span class="math inline">\(1/x+1/y=1/n!\)</span> =&gt; <spanclass="math inline">\(y=n!+n!^2/(x-n!)\)</span></p><p>因为 x, y 为正整数，因此要求的个数等价于 <spanclass="math inline">\(n!^2\)</span> 的约数个数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1000010</span>;</span><br><span class="line"><span class="type">int</span> prime[N],countNum;</span><br><span class="line"><span class="type">bool</span> st[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">2</span>;i&lt;=n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(!st[i])prime[countNum++]=i;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;prime[j]*i&lt;=n;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            st[i*prime[j]]=<span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span>(i%prime[j]==<span class="number">0</span>)<span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> mod=<span class="number">1e9</span><span class="number">+7</span>;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">init</span>(n);</span><br><span class="line">    </span><br><span class="line">    ll ans=<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;countNum;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// cout&lt;&lt;prime[i]&lt;&lt;endl;</span></span><br><span class="line">        <span class="type">int</span> j=n;</span><br><span class="line">        <span class="type">int</span> s=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(j)s+=j/prime[i],j/=prime[i];</span><br><span class="line">        ans=(ans*(<span class="number">2</span>*s<span class="number">+1</span>))%mod;</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2id="暴力枚举求最多约数个数的数一定范围内最大">暴力枚举求最多约数个数的数（一定范围内最大）</h2><p>dfs 搜索</p><ul><li>确定需要的质数数（2...23 共九个质数限定 1e9 内的约数最多的数）</li><li>确定需要的最大 α（30，确定 2e9 次方的数）</li></ul><p>注意可行性剪枝</p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9wcm9ibGVtL2NvbnRlbnQvMjAwLw==">198反素数<i class="fa fa-external-link-alt"></i></span></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> primes[<span class="number">9</span>]=&#123;<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">11</span>,<span class="number">13</span>,<span class="number">17</span>,<span class="number">19</span>,<span class="number">23</span>&#125;;</span><br><span class="line"><span class="type">int</span> maxs,number,n;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> u,<span class="type">int</span> last,<span class="type">int</span> p,<span class="type">int</span> s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(s &gt; maxs||s == maxs&amp;&amp;p&lt;number)</span><br><span class="line">    &#123;</span><br><span class="line">        maxs=s;</span><br><span class="line">        number=p;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(u==<span class="number">9</span>)<span class="keyword">return</span> ;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=last;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="number">1ll</span>*p*primes[u]&gt;n)<span class="keyword">break</span>;</span><br><span class="line">        p*=primes[u];</span><br><span class="line">        <span class="built_in">dfs</span>(u<span class="number">+1</span>,i,p,s*(i<span class="number">+1</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">dfs</span>(<span class="number">0</span>,<span class="number">30</span>,<span class="number">1</span>,<span class="number">1</span>);<span class="comment">//初始最大的α为30</span></span><br><span class="line">    </span><br><span class="line">    cout&lt;&lt;number&lt;&lt;endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="分解质因数优化暴力枚举约数">分解质因数优化暴力枚举约数</h2><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9wcm9ibGVtL2NvbnRlbnQvMjAwLw==">198反素数<i class="fa fa-external-link-alt"></i></span></p><p><strong>2e9 次方以内的数的最多约数的数共有 1600左右的约数数量</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> x first</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> y second</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; PII;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">50010</span>;</span><br><span class="line"><span class="type">int</span> prime[N],countNum;</span><br><span class="line"><span class="type">bool</span> st[N];</span><br><span class="line">PII factor[N];</span><br><span class="line"><span class="type">int</span> dividor[N];</span><br><span class="line"><span class="type">int</span> fcnt,dcnt;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">gcd</span><span class="params">(<span class="type">int</span> a,<span class="type">int</span> b)</span></span>&#123;<span class="keyword">return</span> b?<span class="built_in">gcd</span>(b,a%b):a;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">2</span>;i&lt;=n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(!st[i])prime[countNum++]=i;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;prime[j]*i&lt;=n;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            st[i*prime[j]]=<span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span>(i%prime[j]==<span class="number">0</span>)<span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> u,<span class="type">int</span> p)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(u==fcnt)</span><br><span class="line">    &#123;</span><br><span class="line">        dividor[dcnt++]=p;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;=factor[u].y;i++)&#123;</span><br><span class="line">        <span class="built_in">dfs</span>(u<span class="number">+1</span>,p);</span><br><span class="line">        p*=factor[u].x;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">init</span>(N - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">while</span> (n -- )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> a, b, c, d;</span><br><span class="line">        cin &gt;&gt; a &gt;&gt; b &gt;&gt; c &gt;&gt; d;</span><br><span class="line"></span><br><span class="line">        fcnt = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> t = d;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; prime[i] &lt;= t / prime[i]; i ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> p = prime[i];</span><br><span class="line">            <span class="keyword">if</span> (t % p == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="type">int</span> s = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">while</span> (t % p == <span class="number">0</span>) t /= p, s ++ ;</span><br><span class="line">                factor[fcnt ++ ] = &#123;p, s&#125;;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (t &gt; <span class="number">1</span>) factor[fcnt ++ ] = &#123;t, <span class="number">1</span>&#125;;</span><br><span class="line"></span><br><span class="line">        dcnt = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">dfs</span>(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; dcnt; i ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> x = dividor[i];</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">gcd</span>(a, x) == b &amp;&amp; (ll)c * x / <span class="built_in">gcd</span>(c, x) == d) res ++ ;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        cout &lt;&lt; res &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="欧拉函数">欧拉函数</h1><h2 id="欧拉函数概念">欧拉函数概念</h2><p>1-N 中与 N 互质的数的个数被称为欧拉函数记为 <spanclass="math inline">\(φ(n)\)</span></p><p>由容斥原理推出的公式：<spanclass="math inline">\(φ(n)=N(1-\frac{1}{p_1})(1-\frac{1}{p_2})...(1-\frac{1}{p_k})\)</span></p><p>容斥原理证明公式</p><p>对于 1-N 当中的每一个数（假设 1-N 中有三个 N 的质因子分别设为 <spanclass="math inline">\(p_1p_2p_3\)</span>）</p><p><span class="math display">\[φ(N)=N-N/p_1-N/p_2-N/p_3+N/（p_1p_2)+N/(p_2p_3)+N/(p_3p_1)-N/(p_1p_2p_3)\]</span> 化简即可推出上面的公式</p><p>递推式：<spanclass="math inline">\(φ(ab)=\frac{φ(a)φ(b)gcd(a,b)}{φ(gcd(a,b))}\)</span></p><h2 id="欧拉函数的题目">欧拉函数的题目</h2><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9wcm9ibGVtL2NvbnRlbnQvMjAzLw==">203可见的点<i class="fa fa-external-link-alt"></i></span></p><p><strong>思维+欧拉函数</strong></p><p>对于每一个被光照到的点，其 x, y 必然满足以下条件</p><ol type="1"><li>x, y 为整数</li><li>(x, y)是直线 y = kx 在第一象限以原点为端点的射线上的第一个整点</li><li>即 x, y 为互质的数</li></ol><p><strong>证明</strong></p><p>若 x, y 为非互质的数则存在这么一个整数点(x/d, y/d)使得其与(x,y)处于同一直线且位于(x, y)的左下方</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1010</span>;</span><br><span class="line"><span class="type">int</span> primes[N],countNum;</span><br><span class="line"><span class="type">bool</span> st[N];</span><br><span class="line"><span class="type">int</span> eular[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">initEular</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    eular[<span class="number">1</span>]=<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">2</span>;i&lt;=n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(!st[i])&#123;</span><br><span class="line">            primes[countNum++]=i;</span><br><span class="line">            eular[i]=i<span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;primes[j]*i&lt;=n;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            st[i*primes[j]]=<span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span>(i%primes[j]==<span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                eular[primes[j]*i]=eular[i]*primes[j];</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            eular[primes[j]*i]=eular[i]*(primes[j]<span class="number">-1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">initEular</span>(N<span class="number">-1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> _;</span><br><span class="line">    cin&gt;&gt;_;</span><br><span class="line">    <span class="type">int</span> T=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(_--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n;</span><br><span class="line">        cin&gt;&gt;n;</span><br><span class="line">        ll sum=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++) sum+=eular[i]*<span class="number">2</span>;</span><br><span class="line">        cout&lt;&lt;++T&lt;&lt;<span class="string">&#x27; &#x27;</span>&lt;&lt;n&lt;&lt;<span class="string">&#x27; &#x27;</span>&lt;&lt;sum&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9wcm9ibGVtL2NvbnRlbnQvMjIyLw==">220最大公约数<i class="fa fa-external-link-alt"></i></span></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1e7</span><span class="number">+10</span>;</span><br><span class="line"><span class="type">int</span> primes[N],countNum;</span><br><span class="line"><span class="type">bool</span> st[N];</span><br><span class="line">ll phi[N],s[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">eular</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">2</span>;i&lt;=n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(!st[i])&#123;</span><br><span class="line">            primes[countNum++]=i;</span><br><span class="line">            phi[i]=i<span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;primes[j]*i&lt;=n;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            st[i*primes[j]]=<span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span>(i%primes[j]==<span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                phi[i*primes[j]]=phi[i]*primes[j];</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;phi[i*primes[j]]=phi[i]*(primes[j]<span class="number">-1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)s[i]=s[i<span class="number">-1</span>]+phi[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">eular</span>(n);</span><br><span class="line">    </span><br><span class="line">    ll ans=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;countNum;i++ [数论，算法]g)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> p=primes[i];</span><br><span class="line">        ans+=s[n/p]*<span class="number">2</span><span class="number">+1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="同余">同余</h1><h2 id="扩展欧几里得算法裴蜀定理">扩展欧几里得算法（裴蜀定理）</h2><h3 id="概念与推导">概念与推导</h3><p><span class="math inline">\((a,b)=d =&gt; a * x +b * y = d\)</span><span class="math inline">\((b,a mod b)\)</span></p><p><strong>假定</strong> <span class="math inline">\(y * b + x * (a modb) =d\)</span></p><p><spanclass="math inline">\(y*b+x(a-\lfloor\frac{a}{b}\rfloor*b)=d\)</span></p><p><spanclass="math inline">\(y*b+a*x-\lfloor\frac{a}{b}\rfloor*b*x=d\)</span></p><p><span class="math inline">\(a*x +b*(y-\lfloor\frac{a}{b}\rfloor*x)=d\)</span></p><p><span class="math inline">\(=&gt; x&#39;=x\quady&#39;=y-\lfloor\frac{a}{b}\rfloor*x\)</span></p><h3 id="拓展">拓展</h3><p>对于 <span class="math inline">\((a,b)=d\)</span></p><p>对于该方程的一组解</p><p><span class="math inline">\(a*x_0+b*y_0=d\)</span></p><p>有以下结论: <span class="math display">\[x=x_0+k*(\frac{a}{d})\quad y=y_0-k*(\frac{b}{d})\]</span> 为该同余方程的通解</p><p>这种变形实际上也是一种十分常见得数学变形方式，在数学上叫做零和变形。</p><h2 id="同余方程">同余方程</h2><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9wcm9ibGVtL2NvbnRlbnQvMjA1Lw==">203同余方程<i class="fa fa-external-link-alt"></i></span> <span class="math inline">\(ax≡1(modb)\)</span></p><p>=&gt; <span class="math inline">\(ax-by=1\)</span></p><p>且 x 一定为正值（数学中的取模不会取到正值）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">exgcd</span><span class="params">(<span class="type">int</span> a,<span class="type">int</span> b,<span class="type">int</span>&amp; x,<span class="type">int</span>&amp; y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(!b)&#123;</span><br><span class="line">        x=<span class="number">1</span>,y=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">return</span> a;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> d=<span class="built_in">exgcd</span>(b,a%b,y,x);</span><br><span class="line">    y-=(a/b)*x;</span><br><span class="line">    <span class="keyword">return</span> d;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> a,b,x,y;</span><br><span class="line">    cin&gt;&gt;a&gt;&gt;b;</span><br><span class="line">    <span class="built_in">exgcd</span>(a,b,x,y);</span><br><span class="line">    cout&lt;&lt;(x%b+b)%b&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="青蛙的约会">青蛙的约会</h2><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9wcm9ibGVtL2NvbnRlbnQvMjI0Lw==">222青蛙的约会<i class="fa fa-external-link-alt"></i></span></p><p>A 追 B (b-a) , 每跳一次 A 追 B(m-n)米</p><p><span class="math inline">\((m-n)x=b-a+yL\)</span></p><p>=&gt; <span class="math inline">\((m-n)x=b-a+yL\)</span>[数论，算法]g =&gt; <spanclass="math inline">\((m-n)x-yL=b-a\)</span></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"></span><br><span class="line"><span class="function">ll <span class="title">exgcd</span><span class="params">(ll a,ll b,ll&amp; x,ll&amp; y)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(!b)</span><br><span class="line">    &#123;</span><br><span class="line">        x=<span class="number">1</span>,y=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">return</span> a;</span><br><span class="line">    &#125;</span><br><span class="line">    ll d=<span class="built_in">exgcd</span>(b,a%b,y,x);</span><br><span class="line">    y-=a/b*x;</span><br><span class="line">    <span class="keyword">return</span> d;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll a,b,m,n,L;</span><br><span class="line">    cin&gt;&gt;a&gt;&gt;b&gt;&gt;m&gt;&gt;n&gt;&gt;L;</span><br><span class="line">    ll x,y;</span><br><span class="line">    ll d = <span class="built_in">exgcd</span>(m-n,L,x,y);</span><br><span class="line">    <span class="keyword">if</span>((b-a)%d!=<span class="number">0</span>)<span class="built_in">puts</span>(<span class="string">&quot;Impossible&quot;</span>);</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        x*=(b-a)/d;</span><br><span class="line">        ll t=<span class="built_in">abs</span>(L/d);</span><br><span class="line">        cout&lt;&lt;(x%t+t)%t&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="最幸运的数字">最幸运的数字</h2><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWN3aW5nLmNvbS9wcm9ibGVtL2NvbnRlbnQvMjA0Lw==">202最幸运的数字<i class="fa fa-external-link-alt"></i></span></p><p>对于一串 8，用一个公式表示这个数字</p><p>8888..8(x 个 8) =&gt; 8 * 1111..1 =&gt; 8 * 9999..9/9 =&gt; 8 *(10^x-1)/9</p><p>也可以用公比为 10，初项为 8 的等比数列求和思量</p><p>考量题目</p><p>对于一个 888..8 的约数 L|8*(10^x-1)/9 &lt;=&gt; (9L/d)|(10 ^ x-1)</p><p>&lt;=&gt; 10<sup>x</sup>= 1(mod C) C = 9L/d</p><p>eular 定理：对于 α<sup>phi(n)</sup>= 1(mod n) (α, n)= 1</p><p>根据 eular 与 10<sup>x</sup> = 1(mod C)</p><p>枚举 phi(c)的约数，最小的 10^i%c == 1 即为答案</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"></span><br><span class="line"><span class="function">ll <span class="title">gcd</span><span class="params">(ll a,ll b)</span></span>&#123;<span class="keyword">return</span> b?<span class="built_in">gcd</span>(b,a%b):a;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">ll <span class="title">slow_mul</span><span class="params">(ll a,ll b,ll p)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll res=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(b)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(b&amp;<span class="number">1</span>)res=(res+a)%p;</span><br><span class="line">        a=(a+a)%p;</span><br><span class="line">        b&gt;&gt;=<span class="number">1</span>;</span><br><span class="line">     [数论，算法]g&#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">ll <span class="title">qmi</span><span class="params">(ll a,ll b,ll c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll res=<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>(b)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(b&amp;<span class="number">1</span>)res=<span class="built_in">slow_mul</span>(res,a,c);</span><br><span class="line">        a=<span class="built_in">slow_mul</span>(a,a,c);</span><br><span class="line">        b&gt;&gt;=<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res%c;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">ll <span class="title">get_eular</span><span class="params">(ll a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ll res=a;</span><br><span class="line">    <span class="keyword">for</span>(ll i=<span class="number">2</span>;i&lt;=a/i;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(a%i==<span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">while</span>(a%i==<span class="number">0</span>)a/=i;</span><br><span class="line">            res=res/i*(i<span class="number">-1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(a&gt;<span class="number">1</span>) res=res/a*(a<span class="number">-1</span>);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T=<span class="number">1</span>;</span><br><span class="line">    ll L;</span><br><span class="line">    <span class="keyword">while</span>(cin&gt;&gt;L,L)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> d=<span class="number">1</span>;</span><br><span class="line">        d=<span class="built_in">gcd</span>(L,<span class="number">8</span>) [数论，算法]g;</span><br><span class="line"></span><br><span class="line">        ll c= <span class="number">9</span>*L/d;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// cout&lt;&lt;c&lt;&lt;endl;</span></span><br><span class="line"></span><br><span class="line">        ll phi = <span class="built_in">get_eular</span>(c);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// cout&lt;&lt;phi&lt;&lt;endl;</span></span><br><span class="line"></span><br><span class="line">        ll ans=<span class="number">1e18</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">gcd</span>(c,<span class="number">10</span>)!=<span class="number">1</span>)ans=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(ll i=<span class="number">1</span>;i&lt;=phi/i;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(phi%i==<span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>((<span class="built_in">qmi</span>(<span class="number">10</span>,i,c))==<span class="number">1</span>)ans=<span class="built_in">min</span>(ans,i);</span><br><span class="line">                <span class="keyword">if</span>((<span class="built_in">qmi</span>(<span class="number">10</span>,phi/i,c))==<span class="number">1</span>)ans=<span class="built_in">min</span>(ans,phi/i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case %d: %lld\n&quot;</span>,T++,ans);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="曹冲养猪">曹冲养猪</h2><p><strong>中国剩余定理</strong></p><p><span class="math display">\[\begin{cases}x=a_1(mod\quadm_1)\\x=a_2(mod\quad m_2)\\x=a_3(mod\quad m_3)\\...\\x=a_n(mod\quadm_n)\end{cases}\]</span></p><p>设 <span class="math inline">\(M=m_1m_2m_3...m_n\)</span>[数论，算法]g 令 <span class="math inline">\(M_i=M/m_i\)</span> <spanclass="math inline">\(t_i\)</span> 是 <spanclass="math inline">\(M_i\)</span> 关于 M 的逆元</p><p><span class="math inline">\(M_it_i=1(mod\quad m_i)\)</span></p><p><span class="math inline">\(x=\sum a_iM_it_i\)</span></p><p>==构造解，硬记，比较难==</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">20</span>;</span><br><span class="line">ll a[N],b[N];</span><br><span class="line"></span><br><span class="line"><span class="function">ll <span class="title">exgcd</span><span class="params">(ll a,ll b,ll&amp; x,ll&amp; y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(!b)</span><br><span class="line">    &#123;</span><br><span class="line">        x=<span class="number">1</span>,y=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">return</span> a;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> d=<span class="built_in">exgcd</span>(b,a%b,y,x);</span><br><span class="line">    y-=a/b*x;</span><br><span class="line">    <span class="keyword">return</span> d;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    ll M=<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)&#123;</span><br><span class="line">        cin&gt;&gt;a[i]&gt;&gt;b[i] [数论，算法]g;</span><br><span class="line">        M*=a[i];</span><br><span class="line">    &#125;</span><br><span class="line">    ll ans=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ll mi=M/a[i];</span><br><span class="line">        ll ti,x;</span><br><span class="line">        <span class="built_in">exgcd</span>(mi,a[i],ti,x);</span><br><span class="line">        ans+=b[i]*mi*ti;</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;(ans%M+M)%M&lt;&lt;endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="矩阵乘法">矩阵乘法</h1><h2 id="求斐波那契数列的和">求斐波那契数列的和</h2><p>fn+1 = fn+fn-1;</p><p>sn = f1+f2+f3+f4...+fn</p><p>sn+1 = f1+f2+f3+f4+...+fn+1</p><p><span class="math inline">\(s_{n+1}-s_n=f_{n+1}\)</span></p><p>构造一个矩阵 Fn = fn, fn+1, sn</p><p>Fn ={fn, fn+1, sn}*</p><p>{ {0,1,0}</p><p>{1,1,1}</p><p>{0,0,1}}= Fn+1 ={fn+1，fn+2，sn+1};</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;vector [数论，算法]g&gt;</span><br><span class="line">#include&lt;cstring&gt;</span><br><span class="line">#include&lt;cstdio&gt;</span><br><span class="line">#include&lt;algorithm&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">const int N = 3;</span><br><span class="line">int n, m;</span><br><span class="line"></span><br><span class="line">void mul(int c[], int a[], int b[][N])</span><br><span class="line">&#123;</span><br><span class="line">    int t[N]=&#123;0&#125;;</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = 0; j &lt; N; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            t[i] = (t[i] + 1ll * a[j] * b[j][i]) % m;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    memcpy(c, t, sizeof t);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void mul(int c[][N], int a[][N], int b[][N])</span><br><span class="line">&#123;</span><br><span class="line">    int t[N][N]=&#123;0&#125;;</span><br><span class="line">    for (int i = 0; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j = 0; j &lt; N; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            for (int k = 0; k &lt; N; k++)</span><br><span class="line">            &#123;</span><br><span class="line">                t[i][j] = (t[i][j] + 1ll * a[i][k] * b[k][j]) % m;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    memcpy(c, t, sizeof t);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line"> [数论，算法]g&#123;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    int F1[N] = &#123; 1,1,1 &#125;;</span><br><span class="line">    int A[N][N] = &#123;</span><br><span class="line">        &#123;0,1,0&#125;,</span><br><span class="line">        &#123;1,1,1&#125;,</span><br><span class="line">        &#123;0,0,1&#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">n--;</span><br><span class="line">    while (n)</span><br><span class="line">    &#123;</span><br><span class="line">        /*for(int i=0;i&lt;N;i++)&#123;</span><br><span class="line">        for(int j=0;j&lt;N;j++)</span><br><span class="line">        cout&lt;&lt;A[i][j]&lt;&lt;&#x27; &#x27;;</span><br><span class="line">        puts(&quot;&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        for(int i=0;i&lt;N;i++)cout&lt;&lt;F1[i]&lt;&lt;&#x27; &#x27;;</span><br><span class="line">        puts(&quot;&quot;);*/</span><br><span class="line">        if (n &amp; 1)mul(F1, F1, A);</span><br><span class="line">        mul(A, A, A);</span><br><span class="line">        n &gt;&gt;= 1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; F1[2] &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1id="求一个关于斐波那契数列的特殊数列和">求一个关于斐波那契数列的特殊数列和</h1><p>T(n)=(f1+2f2+3f3+…+nfn)modm</p><p>nsn-tn =(n-1)f1+(n-2)f2+...+fn-1</p><p>(n+1)sn+1-tn+1 = nf1+(n-1)f2+...+fn</p><p>(n+1)sn+1-tn+1-(nsn-tn)= sn</p><p>那么我们设 pn = sn-tn</p><p>Fn ={fn, fn+1, sn, pn} [数论，算法]g*</p><p>{</p><p>{0,1,0,0},</p><p>{1,1,1,0},</p><p>{0,0,1,1},</p><p>{0,0,0,1}</p><p>};= Fn+1 ={fn+1.fn+2, sn+1, sn+2}</p><p>ps: 为了简化代码，我们将初始的 F1 扩展为二维矩阵</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">4</span>;</span><br><span class="line"><span class="type">int</span> n,m;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">void</span> <span class="title">mul</span><span class="params">(<span class="type">int</span> c[][N],<span class="type">int</span> a[][N],<span class="type">int</span> b[][N])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> t[N][N]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;N;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> k=<span class="number">0</span>;k&lt;N;k++)</span><br><span class="line">            &#123;</span><br><span class="line">                t[i][j]=(t[i][j]<span class="number">+1ll</span>*a[i][k]*b[k][j])%m;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">memcpy</span>(c,t,<span class="keyword">sizeof</span> t);</span><br><span class="line"> [数论，算法]g&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin&gt;&gt;n&gt;&gt;m;</span><br><span class="line">    <span class="type">int</span> F1[N][N]=&#123;<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">int</span> A[N][N]=&#123;</span><br><span class="line">        &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">        &#123;<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>&#125;,</span><br><span class="line">        &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>&#125;,</span><br><span class="line">        &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>&#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> k=n<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">while</span>(k)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(k&amp;<span class="number">1</span>)<span class="built_in">mul</span>(F1,F1,A);</span><br><span class="line">        <span class="built_in">mul</span>(A,A,A);</span><br><span class="line">        k&gt;&gt;=<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    cout&lt;&lt;(<span class="number">1ll</span>*F1[<span class="number">0</span>][<span class="number">2</span>]*n-F1[<span class="number">0</span>][<span class="number">3</span>]+m)%m&lt;&lt;endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="组合数学">组合数学</h1><p>这里给出一下一个非常常用的全排列函数：</p><p>next_permutation()</p><p>这个函数也十分便于记忆，permutation即<code>排列</code>的意思。</p><h2 id="总结">总结</h2><p><strong>I</strong></p><p>利用 Cab = Ca-1b+Ca-1b-1 的组合数学规律 dp 出二位数组保存结果</p><p><strong>II</strong></p><p>利用除以一个数等于乘以一个数的逆元的形式预处理出阶乘，o1的时间内得到特定结果</p><h1 id="多重集合的全排列">多重集合的全排列</h1><p>多重集合的定义：<strong>多重集合不要求元素不能重复</strong></p><h2 id="多重集合表示">多重集合表示：</h2><p>M ={k1⋅a1, k2⋅a2, ⋯, kn⋅an}M ={k1⋅a1, k2⋅a2, ⋯, kn⋅an}(其中每个 ai代表是不同的元素，每个元素 ai 有 ki 个，ki 可以是有限数，也可以是∞。)(其中每个 ai 代表是不同的元素，每个元素 ai 有 ki 个，ki可以是有限数，也可以是 ∞。)</p><h2 id="多重集的排列">多重集的排列:</h2><ul><li>多重集合 M ={k1⋅a1, k2⋅a2, ⋯, kn⋅an}的 r 排列数为 kr 多重集合 M={k1⋅a1, k2⋅a2, ⋯, kn⋅an}的 r 排列数为 <spanclass="math inline">\(k^r\)</span></li><li>多重集合 M ={k1⋅a1, k2⋅a2, ⋯, kn⋅an}的全排列数为：<spanclass="math inline">\(\frac{(k1+k2+⋯+kn)!}{k1!k2!⋯kn!}\)</span></li></ul><h1 id="数学知识">数学知识</h1><h1 id="排列">排列</h1><h2 id="错排公式">错排公式</h2><h3 id="错排问题">错排问题</h3><p><strong>错排问题</strong> 考虑一个有 n个元素的排列，若一个排列中所有的元素都不在自己原来的位置上，那么这样的排列就称为原排列的一个错排。n 个元素的错排数记为D(n)。研究一个排列错排个数的问题，叫做错排问题或称为更列问题。</p><h3 id="错排公式的递推">错排公式的递推</h3><p>对于 $D(n) $，考虑第 <span class="math inline">\(n\)</span>个位置，它可以与 <span class="math inline">\(n-1\)</span>前的任意位置交换 <spanclass="math inline">\(((n-1)D(n-1))\)</span>，在考虑编号为 k的位置，这是有两种情况</p><p>（1）将它放到 <spanclass="math inline">\(n\)</span>，那么，对于剩下的 n-1 个元素，由于第 k个元素放到了位置 n，剩下 n-2 个元素就有 <spanclass="math inline">\(D(n-2)\)</span> 种方法，此时放置方法有 <spanclass="math inline">\(D(n-2)\)</span> 种。</p><p>（2）将它不放到 n，那么，剩下 n-2 个元素就有D(n-2)种方法，此时放置方法有 <spanclass="math inline">\(((n-2)*D(n-2))\)</span> 种。</p><p><strong>递推关系式：D(n) = (n-1) [D(n-2) + D(n-1)] (n &gt;=3)</strong></p><p>特别的 <span class="math inline">\(D(1)=0,D(2)=1\)</span>;</p><p>其实到这里就结束了，通过递推关系式可以计算机直接算出 <spanclass="math inline">\(D(n)\)</span></p><p>但是下面还是给出错排公式的推导</p><p>以上是必须要会的内容，最好是尝试自己在没有辅助材料的情况下过一遍。（这种方法又称费曼学习法）</p><h3 id="错排公式的推导">错排公式的推导</h3><p>[数论，算法]g 假设 <span class="math inline">\(D(k) = k! N(k), k = 1,2, …, n,\)</span> 且有 <span class="math inline">\(N(1) = 0, N(2) =1/2.\)</span> 当 <span class="math inline">\(n ≥ 3\)</span> 时，<spanclass="math inline">\(n!\cdot N(n) = (n-1) (n-1)! N(n-1) + (n-1)!N(n-2)\)</span></p><p>即有公式</p><p><span class="math display">\[N(n) = (n-1) N(n-1) +N(n-2)\]</span></p><p>于是有 <span class="math display">\[N(n) - N(n-1) = - [N(n-1) -N(n-2)] / n = (-1/n) [-1/(n-1)] [-1/(n-2)]…(-1/3) [N(2) - N(1)] = (-1)^n/ n!\]</span></p><p>因此</p><p><span class="math inline">\(N(n-1) - N(n-2) = (-1)^{(n-1)} /(n-1)!\)</span>,</p><p><span class="math display">\[N(2) - N(1) = (-1)^2 / 2!\]</span></p><p>相加，可得</p><p><span class="math display">\[N(n) = (-1)^2/2! + … + (-1)^(n-1) /(n-1)! + (-1)^n/n!\]</span></p><p>因此</p><p><span class="math display">\[D(n) = n! [(-1)^2/2! + … +(-1)^(n-1)/(n-1)! + (-1)^n/n!]\]</span></p><p><span class="math display">\[D(n) = ∑_{k=2}^{n} (-1)^k * n! /k!\]</span></p><p>此即错排公式。</p><p>### 另一种方式的推导——容斥原理</p><p>正整数 1, 2, 3, ……, n 的全排列有 n! 种，其中第 k 位是 k 的排列有(n-1)! 种; 当 k 分别取 1, 2, 3, ……, n 时，共有 n*(n-1)!种排列是至少放对了一个的，由于所求的是错排的种数，所以应当减去这些排列;但是此时把同时有两个数错排的排列多排除了一次，应补上;在补上时，把同时有三个数不错排的排列多补上了一次，应排除;……;继续这一过程，得到错排的排列种数为</p><p>D(n) = n! - n!/1! + n!/2! - n!/3! + … + (-1)^n <em>n!/n! = ∑(k = 2~n)(-1)^k </em> n! / k!,</p><p>即 D(n) = n! [1/0! - 1/1! + 1/2! - 1/3! + 1/4! + ... +(-1)^n/n!].</p><h2 id="线性代数">线性代数</h2><h3 id="线性基">线性基</h3><h4 id="有关线性基的一些概念">有关线性基的一些概念</h4><p>[数论，算法]g ##### 张成的概念</p><p>设 ，所有这样的子集 的异或和组成的集合称为集合 的<strong>张成</strong>，记作 。即，在中选出任意多个数，其异或和的所有可能的结果组成的集合。</p><h5 id="线性相关">线性相关</h5><p>对于一个集合 ，如果存在一个元素 ，使得， 在去除这个元素后得到的集合的张成 中包含 ，则称集合 <strong>线性相关</strong>。</p><p>更形象地，可以表示为，存在一个元素，可以用其它若干个元素异或起来得到。</p><p>相对的，如果不存在这样的元素 ，则称集合<strong>线性无关</strong>。</p><p>一个显然的结论是，对于这个线性相关的集合，去除这个元素后，集合的张成不变。</p><h4 id="概念与性质">概念与性质</h4><p>线性基是向量空间的一组基，通常用来解决有关异或的题目，通俗的讲法就是由一个集合构造出来的另一个集合，它有以下几个性质</p><ol type="1"><li>线性基的元素能相互异或得到原集合的所有相互异或得到的值</li><li>线性基是满足性质 1 的最小的集合</li><li>线性基没有异或和为 0 的子集</li><li>线性基种的每个元素的异或方案唯一，也就是说，线性基中的异或组合异或出的数都是不一样的</li><li>线性基中的每个元素的二级制最高位互不相同</li></ol><h4 id="线性基的构造方法">线性基的构造方法</h4><p>对原集合的每一个数 p 转化为二进制，从高位向低位扫，对于第 x 位为 1的，如果 <span class="math inline">\(a_x\)</span> 不存在，那么令 <spanclass="math inline">\(a_x = p\)</span> 并结束扫描，如果存在，令 <spanclass="math inline">\(p_i=p_ixor a_x\)</span></p><p>code:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">void</span> <span class="title">insert</span><span class="params">(<span class="type">long</span> <span class="type">long</span> x)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">55</span>; i + <span class="number">1</span>; i--) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!(x &gt;&gt; i))  <span class="comment">// x的第i位是0</span></span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    <span class="keyword">if</span> (!p[i]) &#123;</span><br><span class="line">      p[i] = x;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    x ^= p[i];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="查询原集合内任意几个元素-xor-的最大值">查询原集合内任意几个元素xor 的最大值</h4><p>将线性基从高位向低位扫，若 xor 上当前扫到的 <spanclass="math inline">\(a_x\)</span> 答案变大，就把答案异或上 <spanclass="math inline">\(a_x\)</span></p><h4 id="第-k-大的子集合异或和">第 k 大的子集合异或和</h4><p><span class="exturl" data-url="aHR0cHM6Ly92anVkZ2UuY3NncmFuZGV1ci5jbi9wcm9ibGVtL0hEVS0zOTQ5">HDU3949<i class="fa fa-external-link-alt"></i></span></p><p>要求我们查询一个数组能异或出来的第 k 大的值</p><p>构造一个特殊的线性基，使得每一个线性基中的值都只有一位是 1</p><p>如 a1:0001000 a2:0000010 a3:00000001</p><p>从小到大存入一个容器中，再枚举查询的第 k 大的 k 值某一位上是否为1，如果是 1，则将 ans 异或上对应下标的线性基数组中的值</p><p>注意如果线性基的大小与原数组的大小不一样，说明原数组是线性相关的，此时则需要将k-1，在进行查询（0 是最小的）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;set&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bitset&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;array&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;atomic&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;sstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iomanip&gt;</span></span></span><br><span class="line"><span class="comment">//#include&lt;bits/stdc++.h&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> int ll</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pb push_back</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> endl <span class="string">&#x27;\n&#x27;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> x first</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> y second</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> Endl endl</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pre(i,a,b) for(int i=a;i&lt;=b;i++)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> rep(i,b,a) for(int i=b;i&gt;=a;i--)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> si(x) scanf(<span class="string">&quot;%d&quot;</span>, &amp;x);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> sl(x) scanf(<span class="string">&quot;%lld&quot;</span>, &amp;x);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ss(x) scanf(<span class="string">&quot;%s&quot;</span>, x);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> YES &#123;puts(<span class="string">&quot;YES&quot;</span>);return;&#125;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NO &#123;puts(<span class="string">&quot;NO&quot;</span>); return;&#125;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> all(x) x.begin(),x.end()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> ull;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; PII;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, PII&gt; PIII;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">char</span>, <span class="type">int</span>&gt; PCI;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, <span class="type">char</span>&gt; PIC;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">double</span>, <span class="type">double</span>&gt; PDD;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;ll, ll&gt; PLL;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">200010</span>, M = <span class="number">2</span> * N, B = N, MOD = <span class="number">1000000007</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> INF = <span class="number">0x3f3f3f3f</span>;</span><br><span class="line"><span class="type">const</span> ll LLINF = <span class="number">0x3f3f3f3f3f3f3f3f</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> dx[<span class="number">4</span>] = &#123; <span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span> &#125;, dy[<span class="number">4</span>] = &#123; <span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span> &#125;;</span><br><span class="line"><span class="type">int</span> n, m, k;</span><br><span class="line">ll a[N], p[N], q[N];</span><br><span class="line"></span><br><span class="line"><span class="function">ll <span class="title">gcd</span><span class="params">(ll a, ll b)</span> </span>&#123; <span class="keyword">return</span> b ? <span class="built_in">gcd</span>(b, a % b) : a; &#125;</span><br><span class="line"><span class="function">ll <span class="title">lowbit</span><span class="params">(ll x)</span> </span>&#123; <span class="keyword">return</span> x &amp; -x; &#125;</span><br><span class="line"><span class="function">ll <span class="title">qmi</span><span class="params">(ll a, ll b, ll mod)</span> </span>&#123;</span><br><span class="line">    ll res = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (b) &#123;</span><br><span class="line">        <span class="keyword">if</span> (b &amp; <span class="number">1</span>) res = res * a % mod;</span><br><span class="line">        a = a * a % mod;</span><br><span class="line">        b &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">insert</span><span class="params">(ll x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">rep</span>(i, <span class="number">63</span>, <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (!(x &gt;&gt; i))<span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span> (!p[i]) &#123;</span><br><span class="line">            p[i] = x;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        x ^= p[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">slove</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">memset</span>(p, <span class="number">0</span>, <span class="keyword">sizeof</span> p);</span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> T = <span class="number">0</span>;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="built_in">pre</span>(i, <span class="number">1</span>, n) &#123;</span><br><span class="line">        cin &gt;&gt; a[i];</span><br><span class="line">        <span class="built_in">insert</span>(a[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">rep</span>(i, <span class="number">63</span> - <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">pre</span>(j, i + <span class="number">1</span>, <span class="number">63</span> - <span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (p[j] &gt;&gt; i &amp; <span class="number">1</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                p[j] ^= p[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    vector&lt;ll&gt; ves;</span><br><span class="line">    <span class="built_in">pre</span>(i, <span class="number">0</span>, <span class="number">63</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (p[i])ves.<span class="built_in">push_back</span>(p[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cin &gt;&gt; m;</span><br><span class="line">    <span class="built_in">pre</span>(i, <span class="number">1</span>, m) cin &gt;&gt; q[i];</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Case #%d:\n&quot;</span>, ++T);</span><br><span class="line">    <span class="built_in">pre</span>(i, <span class="number">1</span>, m)</span><br><span class="line">    &#123;</span><br><span class="line">        ll res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (n != ves.<span class="built_in">size</span>())q[i]--;</span><br><span class="line">        <span class="built_in">pre</span>(j, <span class="number">0</span>, <span class="number">63</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (q[i] &gt;&gt; j &amp; <span class="number">1</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (j &gt;= ves.<span class="built_in">size</span>()) &#123;</span><br><span class="line">                    res = <span class="number">-1</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                res ^= ves[j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; res &lt;&lt; Endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">signed</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> _;</span><br><span class="line">    <span class="built_in">si</span>(_);</span><br><span class="line">    <span class="comment">//_ = 1;</span></span><br><span class="line">    <span class="built_in">init</span>();</span><br><span class="line">    <span class="keyword">while</span> (_--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">slove</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="最大路径异或和">最大路径异或和</h4><p>[P4151 <span class="exturl" data-url="aHR0cHM6Ly93d3cubHVvZ3UuY29tLmNuL3Byb2JsZW0vUDQxNTE=">WC2011]最大XOR 和路径 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn)<i class="fa fa-external-link-alt"></i></span></p><p>求从 1 到 n的最大路径异或和，首先在纸上作图，发现，来回的走一条路径是等价于没有走过的，因此，我们可以从图上的任一点到达另一点后走回来是等价于停留在原地的，因此我们可以将1-n的路径拓展到所有的点上而保持值不变，通过观察，我们可以发现，环可以为我们的路径权值提供贡献值，因为它们是可以在（扩展到全图后的路径）路径中走奇数遍的。</p><p>因此，我们 dfs 对环建立线性基，刚开始随机选取一条 1-n的路径，与线性基异或取最大值（注意，因为 ans 刚开始并不是0，因此每次选取需要 max）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;set&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bitset&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;array&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;atomic&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;sstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iomanip&gt;</span></span></span><br><span class="line"><span class="comment">//#include&lt;bits/stdc++.h&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> int ll</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pb push_back</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> endl <span class="string">&#x27;\n&#x27;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> x first</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> y second</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> Endl endl</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pre(i,a,b) for(int i=a;i&lt;=b;i++)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> rep(i,b,a) for(int i=b;i&gt;=a;i--)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> si(x) scanf(<span class="string">&quot;%d&quot;</span>, &amp;x);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> sl(x) scanf(<span class="string">&quot;%lld&quot;</span>, &amp;x);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ss(x) scanf(<span class="string">&quot;%s&quot;</span>, x);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> YES &#123;puts(<span class="string">&quot;YES&quot;</span>);return;&#125;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NO &#123;puts(<span class="string">&quot;NO&quot;</span>); return;&#125;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> all(x) x.begin(),x.end()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> ull;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; PII;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, PII&gt; PIII;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">char</span>, <span class="type">int</span>&gt; PCI;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, <span class="type">char</span>&gt; PIC;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">double</span>, <span class="type">double</span>&gt; PDD;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;ll, ll&gt; PLL;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">200010</span>, M = <span class="number">2</span> * N, B = N, MOD = <span class="number">1000000007</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> INF = <span class="number">0x3f3f3f3f</span>;</span><br><span class="line"><span class="type">const</span> ll LLINF = <span class="number">0x3f3f3f3f3f3f3f3f</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> dx[<span class="number">4</span>] = &#123; <span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span> &#125;, dy[<span class="number">4</span>] = &#123; <span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span> &#125;;</span><br><span class="line"><span class="type">int</span> n, m, k;</span><br><span class="line"><span class="type">int</span> h[N], ne[M], e[M], idx;</span><br><span class="line">ll w[M], p[N], verval[N];</span><br><span class="line">ll ans;</span><br><span class="line"></span><br><span class="line"><span class="function">ll <span class="title">gcd</span><span class="params">(ll a, ll b)</span> </span>&#123; <span class="keyword">return</span> b ? <span class="built_in">gcd</span>(b, a % b) : a; &#125;</span><br><span class="line"><span class="function">ll <span class="title">lowbit</span><span class="params">(ll x)</span> </span>&#123; <span class="keyword">return</span> x &amp; -x; &#125;</span><br><span class="line"><span class="function">ll <span class="title">qmi</span><span class="params">(ll a, ll b, ll mod)</span> </span>&#123;</span><br><span class="line">    ll res = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (b) &#123;</span><br><span class="line">        <span class="keyword">if</span> (b &amp; <span class="number">1</span>) res = res * a % mod;</span><br><span class="line">        a = a * a % mod;</span><br><span class="line">        b &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b, <span class="type">int</span> c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    e[idx] = b, w[idx] = c, ne[idx] = h[a], h[a] = idx++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">void</span> <span class="title">insert</span><span class="params">(ll x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">rep</span>(i, <span class="number">63</span>, <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (!(x &gt;&gt; i &amp; <span class="number">1</span>))<span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span> (!p[i]) &#123;</span><br><span class="line">            p[i] = x;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        x ^= p[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> u, ll val)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    verval[u] = val;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = h[u]; ~i; i = ne[i])</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> j = e[i];</span><br><span class="line">        <span class="keyword">if</span> (verval[j] != <span class="number">-1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">insert</span>(val ^ verval[j] ^ w[i]);</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">dfs</span>(j, val ^ w[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">slove</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">memset</span>(verval, <span class="number">-1</span>, <span class="keyword">sizeof</span> verval);</span><br><span class="line">    <span class="built_in">memset</span>(h, <span class="number">-1</span>, <span class="keyword">sizeof</span> h);</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m; </span><br><span class="line">    <span class="type">int</span> a, b, c;</span><br><span class="line">    <span class="built_in">pre</span>(i, <span class="number">1</span>, m)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; a &gt;&gt; b &gt;&gt; c;</span><br><span class="line">        <span class="built_in">add</span>(a, b, c); <span class="built_in">add</span>(b, a, c);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">dfs</span>(<span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    ll t = ans= verval[n];</span><br><span class="line">    <span class="built_in">rep</span>(i, <span class="number">63</span>, <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        ans = <span class="built_in">max</span>(ans, ans ^ p[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">signed</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> _;</span><br><span class="line">    <span class="comment">//si(_);</span></span><br><span class="line">    _ = <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">init</span>();</span><br><span class="line">    <span class="keyword">while</span> (_--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">slove</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="线性无关的特性线性基的大小">线性无关的特性，线性基的大小</h4><p>https://vjudge.csgrandeur.cn/problem/CodeForces-1101G</p><p>该题要求将一个数组分成若干个段，保证段本身，段与段之间的异或都不为 0的最大段数量</p><p>由于段与段之间的异或和不为0，因此，可判断，各段的异或值是线性无关的，且根据题目要求各段的异或值是不为0 的，因此，数组中线性相关的值必须放在同一段里另外加任意值。</p><p>应该敏锐的察觉到，上述特性与线性基的特性高度重合，因此，题目所求的值即位线性基的大小</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;set&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bitset&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;array&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;atomic&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;sstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iomanip&gt;</span></span></span><br><span class="line"><span class="comment">//#include&lt;bits/stdc++.h&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//#define int ll</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pb push_back</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> endl <span class="string">&#x27;\n&#x27;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> x first</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> y second</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> Endl endl</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pre(i,a,b) for(int i=a;i&lt;=b;i++)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> rep(i,b,a) for(int i=b;i&gt;=a;i--)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> si(x) scanf(<span class="string">&quot;%d&quot;</span>, &amp;x);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> sl(x) scanf(<span class="string">&quot;%lld&quot;</span>, &amp;x);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ss(x) scanf(<span class="string">&quot;%s&quot;</span>, x);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> YES &#123;puts(<span class="string">&quot;YES&quot;</span>);return;&#125;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NO &#123;puts(<span class="string">&quot;NO&quot;</span>); return;&#125;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> all(x) x.begin(),x.end()</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> ll;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> ull;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; PII;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, PII&gt; PIII;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">char</span>, <span class="type">int</span>&gt; PCI;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, <span class="type">char</span>&gt; PIC;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">double</span>, <span class="type">double</span>&gt; PDD;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;ll, ll&gt; PLL;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">200010</span>, M = <span class="number">2</span> * N, B = N, MOD = <span class="number">998244353</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> INF = <span class="number">0x3f3f3f3f</span>;</span><br><span class="line"><span class="type">const</span> ll LLINF = <span class="number">0x3f3f3f3f3f3f3f3f</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> dx[<span class="number">4</span>] = &#123; <span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span> &#125;, dy[<span class="number">4</span>] = &#123; <span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span> &#125;;</span><br><span class="line"><span class="type">int</span> n, m, k;</span><br><span class="line"><span class="type">int</span> a[N], p[<span class="number">35</span>];</span><br><span class="line"></span><br><span class="line"><span class="function">ll <span class="title">gcd</span><span class="params">(ll a, ll b)</span> </span>&#123; <span class="keyword">return</span> b ? <span class="built_in">gcd</span>(b, a % b) : a; &#125;</span><br><span class="line"><span class="function">ll <span class="title">lowbit</span><span class="params">(ll x)</span> </span>&#123; <span class="keyword">return</span> x &amp; -x; &#125;</span><br><span class="line"><span class="function">ll <span class="title">qmi</span><span class="params">(ll a, ll b, ll mod)</span> </span>&#123;</span><br><span class="line">    ll res = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (b) &#123;</span><br><span class="line">        <span class="keyword">if</span> (b &amp; <span class="number">1</span>) res = res * a % mod;</span><br><span class="line">        a = a * a % mod;</span><br><span class="line">        b &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">insert</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="type">bool</span> res = <span class="literal">false</span>;</span><br><span class="line">    <span class="built_in">rep</span>(i, <span class="number">30</span>, <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (((x &gt;&gt; i) &amp; <span class="number">1</span>)==<span class="number">0</span>)<span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span> (p[i]==<span class="number">0</span>) &#123;</span><br><span class="line">            p[i] = x; res = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        x ^= p[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">slove</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="built_in">pre</span>(i, <span class="number">1</span>, n)cin &gt;&gt; a[i];</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> t=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">pre</span>(i, <span class="number">1</span>, n) &#123;</span><br><span class="line">        <span class="built_in">insert</span>(a[i]);</span><br><span class="line">        t ^= a[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!t) &#123; cout &lt;&lt; <span class="number">-1</span> &lt;&lt; endl; <span class="keyword">return</span>; &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">pre</span>(i, <span class="number">0</span>, <span class="number">30</span>)<span class="keyword">if</span> (p[i])cnt++;</span><br><span class="line">        cout &lt;&lt; cnt &lt;&lt; Endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">signed</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> _;</span><br><span class="line">    <span class="comment">//si(_);</span></span><br><span class="line">    _ = <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">init</span>();</span><br><span class="line">    <span class="keyword">while</span> (_--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">slove</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="常见数学模型的特殊性质">常见数学模型的特殊性质</h1><h2 id="斐波那契数列">斐波那契数列</h2><p><strong><spanclass="math inline">\(\sum{_i^k}F[i]+1=F[k+2]\)</span></strong></p><p>如果 k 为奇数</p><p><spanclass="math inline">\(F[k]=F[1]+\sum{_{i=1}^{k/2}}F[2*i]\)</span></p><p>如果 k 为偶数</p><p><spanclass="math inline">\(F[k]=\sum{_{i=1}^{k/2}}F[2*i-1]\)</span></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;前言需要用到的偏僻语法知识&quot;&gt;前言：需要用到的偏僻语法知识&lt;/h1&gt;
&lt;h2 id=&quot;c随机数函数&quot;&gt;c++随机数函数&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80NDIwMDg1ODk=&quot;&gt;如何优雅的用
C++生成随机数 - 知乎 (zhihu.com)&lt;i class=&quot;fa fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;头文件：c: cstdlib c++: random&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cstdlib&lt;/code&gt; 中的 rand（）和 srand（）函数是 C
语言使用的随机数生成方法，通过 &lt;strong&gt;&lt;em&gt;线性同余法&lt;/em&gt;&lt;/strong&gt;
计算。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://deepcity.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    <category term="数学" scheme="https://deepcity.github.io/categories/%E7%AE%97%E6%B3%95/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="数论" scheme="https://deepcity.github.io/categories/%E7%AE%97%E6%B3%95/%E6%95%B0%E5%AD%A6/%E6%95%B0%E8%AE%BA/"/>
    
    
    <category term="算法" scheme="https://deepcity.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="数论" scheme="https://deepcity.github.io/tags/%E6%95%B0%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>Nodejs安装简要建议</title>
    <link href="https://deepcity.github.io/2024/Nodejs%E5%AE%89%E8%A3%85%E7%AE%80%E8%A6%81%E5%BB%BA%E8%AE%AE/article.html"/>
    <id>https://deepcity.github.io/2024/Nodejs%E5%AE%89%E8%A3%85%E7%AE%80%E8%A6%81%E5%BB%BA%E8%AE%AE/article.html</id>
    <published>2024-08-15T02:28:48.000Z</published>
    <updated>2024-08-15T02:41:25.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NodeJS"><a href="#NodeJS" class="headerlink" title="NodeJS"></a>NodeJS</h2><h3 id="什么是Nodejs"><a href="#什么是Nodejs" class="headerlink" title="什么是Nodejs"></a>什么是Nodejs</h3><p><span class="exturl" data-url="aHR0cHM6Ly9ub2RlanMub3JnLw==">Node.js — Run JavaScript Everywhere<i class="fa fa-external-link-alt"></i></span>这是NodeJS的官网标题，很简单的概括，他就是在任何地方都可以运行javascript。</p><p>具体的讲，如下。。</p><blockquote><p> Node.js 就是运行在服务端的 JavaScript。 Node.js 是一个基于 Chrome JavaScript 运行时建立的一个平台。 Node.js 是一个事件驱动 I/O 服务端 JavaScript 环境，基于 Google 的 V8 引擎，V8 引擎执行 Javascript 的速度非常快，性能非常好。</p></blockquote><span id="more"></span><h3 id="下载时一步步涉及到的包管理"><a href="#下载时一步步涉及到的包管理" class="headerlink" title="下载时一步步涉及到的包管理"></a>下载时一步步涉及到的包管理</h3><p>nvm——Node Version Manager</p><p>fnm——Fast Node Manager</p><p>nvm 有一个致命的缺点，就是它的自动切换版本极其麻烦，而 fnm 就没这个问题。并且 Windows 上的 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL252bS1zaC9udm0=">nvm<i class="fa fa-external-link-alt"></i></span> 与 macOS 上的 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NvcmV5YnV0bGVyL252bS13aW5kb3dz">nvm<i class="fa fa-external-link-alt"></i></span> 实际上并不是同一个，Windows 的只是借了 nvm 的名称，API略有不同，构建两者的语言更是完全不相同。而 fnm 在三大系统上都是同一个，这保证了 API 的一致性。</p><p>下面以fnm为例</p><h3 id="fnm-下载"><a href="#fnm-下载" class="headerlink" title="fnm 下载"></a>fnm 下载</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://fnm.vercel.app/install | bash</span><br></pre></td></tr></table></figure><p>如果安装不成功，考虑是不是服务器连不上外网，通过其他渠道下载fnm软件包并解压到对应位置，在.bashrc中添加环境变量即可。下载链接如下：</p><p><span class="exturl" data-url="aHR0cHM6Ly9vYmplY3RzLmdpdGh1YnVzZXJjb250ZW50LmNvbS9naXRodWItcHJvZHVjdGlvbi1yZWxlYXNlLWFzc2V0LTJlNjViZS8xNjYwNDU0MjQvNGMwYTllMmEtOWIyMi00ZWJlLWIwMjYtZGU1ZTc4ZGU5MzUxP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9cmVsZWFzZWFzc2V0cHJvZHVjdGlvbiUyRjIwMjQwNTI5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDUyOVQwODMzMjRaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1kNWUxMzc2M2I5NzUwNzI3YmQ0NWIwMTRkMTJhYTdlM2E4OTNmZmUyNzU4MTg5NzEwZDhjMTFkODc0ZWI5ZTY2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD05MTI3MzE2MSZrZXlfaWQ9MCZyZXBvX2lkPTE2NjA0NTQyNCZyZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRGZubS1saW51eC56aXAmcmVzcG9uc2UtY29udGVudC10eXBlPWFwcGxpY2F0aW9uJTJGb2N0ZXQtc3RyZWFtb2JqZWN0cy5naXRodWJ1c2VyY29udGVudC5jb20vZ2l0aHViLXByb2R1Y3Rpb24tcmVsZWFzZS1hc3NldC0yZTY1YmUvMTY2MDQ1NDI0LzRjMGE5ZTJhLTliMjItNGViZS1iMDI2LWRlNWU3OGRlOTM1MT9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPXJlbGVhc2Vhc3NldHByb2R1Y3Rpb24lMkYyMDI0MDUyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MjlUMDgzMzI0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDVlMTM3NjNiOTc1MDcyN2JkNDViMDE0ZDEyYWE3ZTNhODkzZmZlMjc1ODE4OTcxMGQ4YzExZDg3NGViOWU2NiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9OTEyNzMxNjEma2V5X2lkPTAmcmVwb19pZD0xNjYwNDU0MjQmcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj1hdHRhY2htZW50JTNCJTIwZmlsZW5hbWUlM0Rmbm0tbGludXguemlwJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT1hcHBsaWNhdGlvbiUyRm9jdGV0LXN0cmVhbQ==">fnm.zip<i class="fa fa-external-link-alt"></i></span></p><h3 id="NodeJS18版本以上的gcc-amp-make版本要求"><a href="#NodeJS18版本以上的gcc-amp-make版本要求" class="headerlink" title="NodeJS18版本以上的gcc&amp;make版本要求"></a>NodeJS18版本以上的gcc&amp;make版本要求</h3><p>要求gcc必须版本8以上（较流行的版本为11），make版本4以上，并且在一些环境下需要更新GLIBC（只能通过源码安装，因为涉及Linux底层动态链接库），通过下载源代码包configure（注意参数）以及make，make install即可安装，注意，此步骤为高级操作，操作前请备份快照重要文件，可能导致库文件缺失引起的ssh无法连接，编译时长30min以上，make参数采用-j𝑛(通常为处理器数目两倍)可以加速。</p><p>以下是另一个博主的详细介绍，可以参考一下，笔者在更新GLIBC时也是参考的这篇blog</p><p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81NTk3OTE0NTA/c3BtPWEyYzZoLjEyODczNjM5LmFydGljbGUtZGV0YWlsLjcuNzEzNzY1ZmFadXZTd2g=">OSError: /lib64/libm.so.6: version `GLIBC_2.27’ not found (required by xxx.so) ——升级GLIBC并解决系统错误 - 知乎 (zhihu.com)<i class="fa fa-external-link-alt"></i></span></p><p>在make过程中通常涉及</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ld-linux-x86-64.so.2\libc.so.6</span><br></pre></td></tr></table></figure><p>两个软连接的更改，在更改时会中断make程序并导致系统异常，需要重新手动连接软连接（如果你make时出错并且系统无法ls的情况下）<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81NTk3OTE0NTA/c3BtPWEyYzZoLjEyODczNjM5LmFydGljbGUtZGV0YWlsLjcuNzEzNzY1ZmFadXZTd2g=">https://zhuanlan.zhihu.com/p/559791450?spm=a2c6h.12873639.article-detail.7.713765faZuvSwh<i class="fa fa-external-link-alt"></i></span>)</p><p>最后记得设置环境变量，切勿随意删除系统gcc文件。</p><blockquote><p> 许多blog在configure中设置—profix=/usr，需要自定义的用户请注意，这样会导致文件混乱。个人常用（/usr/local/soft-name）</p></blockquote><p>安装NodeJS详细步骤建议参考官方：<span class="exturl" data-url="aHR0cHM6Ly9ub2RlanMub3JnL2VuL2Rvd25sb2FkL3BhY2thZ2UtbWFuYWdlcg==">Node.js — Download Node.js® (nodejs.org)<i class="fa fa-external-link-alt"></i></span></p><p>注意前面提到的依赖项，以及官网的curlpost请求不到可能是网络因素</p><h3 id="NodeJS验证"><a href="#NodeJS验证" class="headerlink" title="NodeJS验证"></a>NodeJS验证</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br><span class="line">npm -v</span><br></pre></td></tr></table></figure><p>两条指令均正常即说明程序成功安装</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;NodeJS&quot;&gt;&lt;a href=&quot;#NodeJS&quot; class=&quot;headerlink&quot; title=&quot;NodeJS&quot;&gt;&lt;/a&gt;NodeJS&lt;/h2&gt;&lt;h3 id=&quot;什么是Nodejs&quot;&gt;&lt;a href=&quot;#什么是Nodejs&quot; class=&quot;headerlink&quot; title=&quot;什么是Nodejs&quot;&gt;&lt;/a&gt;什么是Nodejs&lt;/h3&gt;&lt;p&gt;&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly9ub2RlanMub3JnLw==&quot;&gt;Node.js — Run JavaScript Everywhere&lt;i class=&quot;fa fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/span&gt;这是NodeJS的官网标题，很简单的概括，他就是在任何地方都可以运行javascript。&lt;/p&gt;
&lt;p&gt;具体的讲，如下。。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; Node.js 就是运行在服务端的 JavaScript。 Node.js 是一个基于 Chrome JavaScript 运行时建立的一个平台。 Node.js 是一个事件驱动 I/O 服务端 JavaScript 环境，基于 Google 的 V8 引擎，V8 引擎执行 Javascript 的速度非常快，性能非常好。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="软件" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/"/>
    
    <category term="经典库" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/%E7%BB%8F%E5%85%B8%E5%BA%93/"/>
    
    <category term="前端" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/%E7%BB%8F%E5%85%B8%E5%BA%93/%E5%89%8D%E7%AB%AF/"/>
    
    <category term="node" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/%E7%BB%8F%E5%85%B8%E5%BA%93/%E5%89%8D%E7%AB%AF/node/"/>
    
    
    <category term="nodejs" scheme="https://deepcity.github.io/tags/nodejs/"/>
    
    <category term="gcc" scheme="https://deepcity.github.io/tags/gcc/"/>
    
    <category term="linux" scheme="https://deepcity.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Miracl的配置ForCentos7</title>
    <link href="https://deepcity.github.io/2024/Miracl%E7%9A%84%E9%85%8D%E7%BD%AEForCentos7/article.html"/>
    <id>https://deepcity.github.io/2024/Miracl%E7%9A%84%E9%85%8D%E7%BD%AEForCentos7/article.html</id>
    <published>2024-08-14T11:55:03.000Z</published>
    <updated>2024-08-26T03:16:10.240Z</updated>
    
    <content type="html"><![CDATA[<p>Miracl is Multiprecision Integer and Rational Arithmetic Cryptographic Library – the MIRACL Crypto SDK – is a C software library that is widely regarded by developers as the gold standard open source SDK for elliptic curve cryptography (ECC).</p><p>Miracl 是多精度整数和有理数算术加密库（MIRACL Crypto SDK），是一个 C 软件库，被开发人员广泛视为椭圆曲线加密 (ECC) 的黄金标准开源 SDK。也可在c++环境下通过对c库的</p><span id="more"></span><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span></span><br><span class="line">&#123;</span><br><span class="line">     <span class="meta">#<span class="keyword">include</span> <span class="string">&quot;miracl.h&quot;</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现兼容。</p><p>下面是对miracl在c++环境下部署的简单步骤</p><p>第一步、联网状态下通过该命令获取压缩包，也可离线通过ftp传输</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/miracl/MIRACL/archive/master.zip</span><br></pre></td></tr></table></figure><p>第二步、创建一个文件夹用来存放解压文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir miracl</span><br></pre></td></tr></table></figure><p>第三步、复制并解压</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp master.zip ./miracl</span><br><span class="line">cd miracl/</span><br><span class="line">unzip -j -aa -L master.zip</span><br></pre></td></tr></table></figure><p>第四步、验证解压并运行linux64（若32位运行linux）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ls</span><br><span class="line">bash linux64</span><br><span class="line">ll | grep miracl.a</span><br></pre></td></tr></table></figure><p>第五步、运行官方程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./pk-demo</span><br></pre></td></tr></table></figure><p>第六步、一般情况下、复制一下文件到你的源代码目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp ../miracl/miracl.a miracl.a</span><br><span class="line">cp ../miracl/miracl.h miracl.h</span><br><span class="line">cp ../miracl/mirdef.h mirdef.h</span><br></pre></td></tr></table></figure><p>完成，注意在linux部署十分简单，但在windows环境下按照网络上的教程会爆出各种各样奇怪的错误，读者若要尝试，建议多参考官方文档，准备好比linux环境下部署多耗费许多心神的准备（ps:做好了发个blog）笔者虽然也已经配好了，静态库如下，但在一些程序内还是会报错，个人觉得是静态库制作过程中少了一些源文件并未制作，如下：</p><p><span class="exturl" data-url="aHR0cHM6Ly8xZHJ2Lm1zL2YvcyFBcC1lblk3Y2tMQU5nb05NSVF5c1hSVlM4TGRHeVE=">静态库文件<i class="fa fa-external-link-alt"></i></span></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Miracl is Multiprecision Integer and Rational Arithmetic Cryptographic Library – the MIRACL Crypto SDK – is a C software library that is widely regarded by developers as the gold standard open source SDK for elliptic curve cryptography (ECC).&lt;/p&gt;
&lt;p&gt;Miracl 是多精度整数和有理数算术加密库（MIRACL Crypto SDK），是一个 C 软件库，被开发人员广泛视为椭圆曲线加密 (ECC) 的黄金标准开源 SDK。也可在c++环境下通过对c库的&lt;/p&gt;</summary>
    
    
    
    <category term="软件" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/"/>
    
    <category term="经典库" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/%E7%BB%8F%E5%85%B8%E5%BA%93/"/>
    
    <category term="信息安全" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/%E7%BB%8F%E5%85%B8%E5%BA%93/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/"/>
    
    <category term="Miracl" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/%E7%BB%8F%E5%85%B8%E5%BA%93/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/Miracl/"/>
    
    
    <category term="信息安全" scheme="https://deepcity.github.io/tags/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/"/>
    
    <category term="软件" scheme="https://deepcity.github.io/tags/%E8%BD%AF%E4%BB%B6/"/>
    
    <category term="Miracl" scheme="https://deepcity.github.io/tags/Miracl/"/>
    
  </entry>
  
  <entry>
    <title>RSA涉及算法与数论知识</title>
    <link href="https://deepcity.github.io/2024/RSA%E6%B6%89%E5%8F%8A%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E8%AE%BA%E7%9F%A5%E8%AF%86/article.html"/>
    <id>https://deepcity.github.io/2024/RSA%E6%B6%89%E5%8F%8A%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E8%AE%BA%E7%9F%A5%E8%AF%86/article.html</id>
    <published>2024-08-14T11:51:58.000Z</published>
    <updated>2024-08-27T08:23:26.277Z</updated>
    
    <content type="html"><![CDATA[<p><strong>RSA</strong> (<strong>Rivest–Shamir–Adleman</strong>) is a <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvUHVibGljLWtleV9jcnlwdG9ncmFwaHk=">public-keycryptosystem<i class="fa fa-external-link-alt"></i></span>, one of the oldest widely used for secure datatransmission.</p><p>RSA（Rivest–Shamir–Adleman）是一种公钥密码系统，是最古老且广泛用于安全数据传输的系统之一。它是一种非对称公钥-私钥密码系统。</p><span id="more"></span><h2 id="基础数论知识纲要">基础数论知识纲要</h2><p>传送门：<span class="exturl" data-url="aHR0cHM6Ly93d3cuc3R1ZG9jdS5jb20vc2cvY291cnNlL25hbnlhbmctdGVjaG5vbG9naWNhbC11bml2ZXJzaXR5L251bWJlci10aGVvcnkvMzAzMTkzNA==">MH3210- NTU - Number Theory - Studocu<i class="fa fa-external-link-alt"></i></span></p><p>知乎blog：<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82MzUzMzI2NTg=">基础数论学习笔记（1）-Divisibility 整除 - 知乎 (zhihu.com)<i class="fa fa-external-link-alt"></i></span></p><p>讲义：<span class="exturl" data-url="aHR0cHM6Ly8xZHJ2Lm1zL2YvcyFBcC1lblk3Y2tMQU5nb05Ib3R6cTNTUlRhZnA0cGc=">NanYangTechnological University MH1300ANDMH3210<i class="fa fa-external-link-alt"></i></span></p><h2 id="概念定义">概念定义</h2><p><strong>费马小定理</strong>：选一个<strong>素数p</strong>，再选一个和p不成倍数关系的整数β，必然满足<strong>β的p次幂</strong>和<strong>β</strong>对p同余。公式如下。<span class="math display">\[\beta^p \equiv \beta (modp)\]</span> <strong>二次剩余</strong>： 取定 <spanclass="math display">\[𝑎\perp 𝑝\]</span>, 假若存在着 <spanclass="math inline">\(x\)</span>使得 <span class="math display">\[x^2 \equiv a (modp)\]</span></p><p>则称<span class="math display">\[a\]</span>是<spanclass="math display">\[modp\]</span>的特殊剩余，否则则是<spanclass="math display">\[modp\]</span>的二次非剩余</p><p><strong>欧拉准则</strong>：元素β是<strong>模奇素数p</strong>的平方剩余的充要条件是，β的(p-1)/2次幂和1对p同余。即以下公式<span class="math display">\[\beta ^ {(p-1)/2} \equiv 1 (mod p) | p \equiv 1 (mod2)\]</span></p><blockquote><p>欧拉准则证明： 设<span class="math display">\[r^2 =\beta\]</span>则有 <span class="math display">\[r^{p-1} \equiv 1(modp)\]</span>,利用费马小定理即可得证。</p></blockquote><p><strong>勒让德符号</strong>：<spanclass="math inline">\((\frac{a}{p})\)</span> 若<spanclass="math inline">\(a\)</span> 是<spanclass="math inline">\(modp\)</span>的平方剩余 则 <spanclass="math inline">\((\frac{a}{p}) = 1\)</span> ，若不是，则<spanclass="math inline">\((\frac{a}{p})= -1\)</span>，<spanclass="math inline">\(a\)</span><spanclass="math inline">\(和\)</span><spanclass="math inline">\(p\)</span>是整除关系则<spanclass="math inline">\((\frac{a}{p})=0\)</span> ,如下列公式 <spanclass="math display">\[(\frac{a}{p}) =\begin{cases}1,\quad x^2 \equiv a(modp)\\0, \quad a \equiv 0 (modp) \\-1, \quad x^2 \not\equiv a(modp)\end{cases}\tag{1}\]</span> Solovay-Strassen算法：若n是一个素数，那么勒让德符号<spanclass="math inline">\((\frac{β}{n})\equiv β^{(n-1)/2}modn\)</span>。</p><blockquote><p>Strassen算法证明：根据欧拉准则<strong>β的(n-1)/2次方和1对n不同余</strong>，根据费马小定理，<strong>β的n-1次方</strong>和1对n同余，根据平方差公式，<spanclass="math inline">\(β^{n-1}-1=(β^{(n-1)/2}-1)(β^{(n-1)/2}+1)\)</span>。由于<span class="math inline">\(（β^{(n-1)/2}-1）\)</span>无法被n整除，所以<span class="math inline">\(（β^{(n-1)/2}+1）\)</span>必然能被n整除，进而得到<spanclass="math inline">\(β^{(n-1)/2}\equiv-1  (mod n)\)</span>。</p></blockquote><p><strong>该命题的逆命题是不成立的</strong></p><p><strong>二次互反律</strong>：<span class="math inline">\(\mathbbZ_p^\times\to\mathbb Z_2\)</span></p><p><strong>Miller-Rabin算法</strong>：</p><p>输入待测试大数<span class="math display">\[n\]</span>，对<spanclass="math display">\[n-1\]</span>不断地进行除<spanclass="math display">\[2\]</span>操作，直到得到一个<strong>奇数t</strong>。</p><p>于是这可以写成 <span class="math inline">\(n-1 = 2^s \cdot t\)</span>。显然的，待测试大数<spanclass="math inline">\(n\)</span>肯定是个奇数（废话，是偶数还测个锤子了），那么<spanclass="math display">\[n-1\]</span>肯定是个偶数，故<spanclass="math inline">\(s\not=0\)</span>。</p><p>选择<strong>随机种子<spanclass="math inline">\(a\)</span></strong>，<spanclass="math inline">\(a＜n\)</span>且与<spanclass="math inline">\(n\)</span>互素。（这个也很容易做到，如果随便一下子就测出来不互素，那就不用测了）</p><p>先设<span class="math inline">\(m=t\)</span>。计算 <spanclass="math inline">\(b \equiv a^t(modn)\)</span> ：</p><ul><li><p>情况①：当 <span class="math inline">\(m=2^s \cdot t = n-1\)</span>时，停机，输出“n是一个合数”。</p></li><li><p>情况②：当 <span class="math inline">\(b \equiv -1 (modn)\)</span>时，停机，换一个随机种子<spanclass="math inline">\(a\)</span>再次进行测试。</p></li><li><p>情况③：当<span class="math inline">\(b \equiv-1(modn)\)</span>不成立时，重新设<spanclass="math inline">\(b\)</span>为<span class="math inline">\(b²(modn)\)</span>，<span class="math inline">\(m\)</span>为<spanclass="math inline">\(2m\)</span>​；继续循环，直到得到情况①或情况②为止。</p></li></ul><h2 id="计算方法">计算方法</h2><p><strong>快速幂</strong>：通过二进制的快速幂优化</p><p><strong>计算勒让德符号<spanclass="math inline">\((\frac{\beta}{n})\)</span>​</strong> ：二次互反</p><p><strong>大整数除法中对小除数的优化</strong>：移位计算小除数的商</p><p><strong>大整数除法中对大除数的优化</strong>：二分查找对每次计算试商的优化</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;RSA&lt;/strong&gt; (&lt;strong&gt;Rivest–Shamir–Adleman&lt;/strong&gt;) is a &lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvUHVibGljLWtleV9jcnlwdG9ncmFwaHk=&quot;&gt;public-key
cryptosystem&lt;i class=&quot;fa fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/span&gt;, one of the oldest widely used for secure data
transmission.&lt;/p&gt;
&lt;p&gt;RSA（Rivest–Shamir–Adleman）是一种公钥密码系统，是最古老且广泛用于安全数据传输的系统之一。它是一种非对称公钥-私钥密码系统。&lt;/p&gt;</summary>
    
    
    
    <category term="信息安全" scheme="https://deepcity.github.io/categories/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/"/>
    
    <category term="加密算法" scheme="https://deepcity.github.io/categories/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/"/>
    
    <category term="数学" scheme="https://deepcity.github.io/categories/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="信息安全" scheme="https://deepcity.github.io/tags/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/"/>
    
    <category term="数学" scheme="https://deepcity.github.io/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Latex宏包pgfplots做矩形函数图像</title>
    <link href="https://deepcity.github.io/2024/Latex%E5%AE%8F%E5%8C%85pgfplots%E5%81%9A%E7%9F%A9%E5%BD%A2%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F/article.html"/>
    <id>https://deepcity.github.io/2024/Latex%E5%AE%8F%E5%8C%85pgfplots%E5%81%9A%E7%9F%A9%E5%BD%A2%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F/article.html</id>
    <published>2024-08-14T11:47:58.000Z</published>
    <updated>2024-08-26T03:36:42.995Z</updated>
    
    <content type="html"><![CDATA[<p>持续更新遇到的问题，完结或不再使用该包将删除这句话</p><h2 id="pgfplots"><a href="#pgfplots" class="headerlink" title="pgfplots"></a>pgfplots</h2><p>Pgfplots是一种可视化工具，可简化在文档中包含绘图的过程。基本思想是，用户提供输入数据/公式，然后pgfplots 宏包会帮助用户绘制响应的图像。</p><span id="more"></span><h3 id="实例一、通过函数方程绘制函数图像"><a href="#实例一、通过函数方程绘制函数图像" class="headerlink" title="实例一、通过函数方程绘制函数图像"></a>实例一、通过函数方程绘制函数图像</h3><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%Ex1:f(x)=exp(x)</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;tikzpicture&#125;</span><br><span class="line"><span class="keyword">\begin</span>&#123;axis&#125;</span><br><span class="line"><span class="keyword">\addplot</span>[color=red]&#123;exp(x)&#125;;</span><br><span class="line"><span class="keyword">\end</span>&#123;axis&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;tikzpicture&#125;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2024/05/07/iPmhCOxRgzZGYVy.png" alt="test-1-crop-1"></p><p>其中绘制图像的语法如下：</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\addplot</span>[option] &#123;expression of 2D function&#125;;</span><br></pre></td></tr></table></figure><p>在方括号内可以传递一些选项，比如我们可以像Ex1一样，将函数图像的颜色设置为红色。我方括号是必需的，如果没有传递任何选项，则方括号之间留有空白即可。在花括号内，我们要填写的2D 函数的表达式，比如在Ex1中，我们所写的表达式是 exp⁡(𝑥) 。最后最重要的是该命令必须以分号 <strong>;</strong> 结尾。</p><h3 id="实例二、坐标系的绘制以及3d函数图像"><a href="#实例二、坐标系的绘制以及3d函数图像" class="headerlink" title="实例二、坐标系的绘制以及3d函数图像"></a>实例二、坐标系的绘制以及3d函数图像</h3><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\documentclass</span>&#123;ctexart&#125;</span><br><span class="line"><span class="keyword">\pagestyle</span>&#123;empty&#125;</span><br><span class="line"><span class="keyword">\usepackage</span>&#123;pgfplots&#125;</span><br><span class="line"><span class="keyword">\begin</span>&#123;document&#125;</span><br><span class="line"><span class="comment">%Ex2: put the 2D plot and the 3D plot together</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;tikzpicture&#125;</span><br><span class="line"><span class="keyword">\begin</span>&#123;axis&#125;</span><br><span class="line"><span class="keyword">\addplot</span>[color=red]&#123;exp(x)&#125;;</span><br><span class="line"><span class="keyword">\end</span>&#123;axis&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;tikzpicture&#125;</span><br><span class="line"><span class="comment">%Here ends the furst 2D plot</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">\hskip</span> 10pt</span><br><span class="line"></span><br><span class="line"><span class="comment">%Here begins the 3d plot</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;tikzpicture&#125;</span><br><span class="line"><span class="keyword">\begin</span>&#123;axis&#125;</span><br><span class="line"><span class="keyword">\addplot</span>3[</span><br><span class="line">surf,</span><br><span class="line">]</span><br><span class="line">&#123;exp(-x<span class="built_in">^</span>2-y<span class="built_in">^</span>2)*x&#125;;</span><br><span class="line"><span class="keyword">\end</span>&#123;axis&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;tikzpicture&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2024/05/07/d7OUNVRI3LWthxK.png" alt="test-1-crop-1"></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\addplot</span>3[</span><br><span class="line">    surf,</span><br><span class="line">]</span><br><span class="line">&#123;exp(-x<span class="built_in">^</span>2-y<span class="built_in">^</span>2)*x&#125;;</span><br></pre></td></tr></table></figure><p>注意到这里只是将2d图像的命令后面加了一个3，读者可以多修改一下option中的参数，这里的surf指surface，声明这是一个曲面图，并表面着色。</p><p>可以看到对多个图像只需要设定多个tikzpicture作用域即可</p><h4 id="坐标系图像中的常用命令、参数与特性"><a href="#坐标系图像中的常用命令、参数与特性" class="headerlink" title="坐标系图像中的常用命令、参数与特性"></a>坐标系图像中的常用命令、参数与特性</h4><ul><li>xlabel、ylabel：设定x,y坐标轴上的标志。</li><li>多个addplot可在同一个图中多次作图</li><li><script type="math/tex">\tt{domian = a:b}</script>设置 𝑥 的范围为 [𝑎,𝑏] 。即只绘制函数在 𝑥∈[𝑎,𝑏] 之间的图像；</li><li><script type="math/tex">\tt{axis\,\, lines = left}</script>这个命令会仅在图的左侧和底部设置轴，即我们的平面直角坐标系，而不是像我们在图片3中看到的那种默认框；</li><li>\addlegendentry{$function（x)$​} 添加函数标签 </li><li>\legend 按顺序批量谭家函数标签</li><li>title在axis中为图像添加标题</li></ul><h3 id="实例三、数据图"><a href="#实例三、数据图" class="headerlink" title="实例三、数据图"></a>实例三、数据图</h3><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%Ex6:plot from data</span></span><br><span class="line"><span class="keyword">\begin</span>&#123;tikzpicture&#125;</span><br><span class="line"><span class="keyword">\begin</span>&#123;axis&#125;[</span><br><span class="line">title=&#123;Temperature dependence of CuSO<span class="built_in">$</span><span class="built_in">_</span>4<span class="keyword">\cdot</span><span class="built_in">$</span>5H<span class="built_in">$</span><span class="built_in">_</span>2<span class="built_in">$</span>O solubility&#125;,</span><br><span class="line">xlabel=&#123;Temperature [<span class="keyword">\textcelsius</span>]&#125;,</span><br><span class="line">ylabel=&#123;Solubility [g per 100 g water]&#125;,</span><br><span class="line">xmin=0, xmax=100,</span><br><span class="line">ymin=0, ymax=120,</span><br><span class="line">xtick=&#123;0,20,40,60,80,100&#125;,</span><br><span class="line">ytick=&#123;0,20,40,60,80,100,120&#125;,</span><br><span class="line">legend pos=north west,</span><br><span class="line">ymajorgrids=true,</span><br><span class="line">grid style=dashed,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">\addplot</span>[</span><br><span class="line">color=blue,</span><br><span class="line">mark=square,</span><br><span class="line">]</span><br><span class="line">coordinates &#123;</span><br><span class="line">(0,23.1)</span><br><span class="line">(10,27.5)</span><br><span class="line">(20,32)</span><br><span class="line">(30,37.8)</span><br><span class="line">(40,44.6)</span><br><span class="line">(60,61.8)</span><br><span class="line">(80,83.8)</span><br><span class="line">(100,114)</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">\legend</span>&#123;CuSO<span class="built_in">$</span><span class="built_in">_</span>4<span class="keyword">\cdot</span><span class="built_in">$</span>5H<span class="built_in">$</span><span class="built_in">_</span>2<span class="built_in">$</span>O&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">\end</span>&#123;axis&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;tikzpicture&#125;</span><br></pre></td></tr></table></figure><h4 id="数据图像中的常用命令、参数与特性"><a href="#数据图像中的常用命令、参数与特性" class="headerlink" title="数据图像中的常用命令、参数与特性"></a>数据图像中的常用命令、参数与特性<img src="https://s2.loli.net/2024/05/08/NzwV7TsPHLWjbJB.png" alt="test-1-crop-1"></h4><ul><li>mark：设定图像点的形状；如square、triangle、x等</li><li><script type="math/tex">\tt{ymajorgrids=true/false\,,xmajorgrids=true/false}</script>​启用/禁用 𝑦,𝑥 轴上<strong>刻度线位置上</strong>的网格线；</li><li><script type="math/tex">\tt{xmin=a, xmax=b, ymin=c, ymax=d}</script>​设置 𝑥 的最小值为 𝑎 ，最大值为 𝑏 ；设置 𝑦 的最小值为 𝑐 ，最大值为 𝑑 ；</li><li><script type="math/tex">\tt{coordinates \{\}}</script>​设定坐标点画折线图</li><li>\addplot[option] table {file_with_the_data.dat},使用该命令可直接通过dat数据画图</li></ul><h3 id="其他图像"><a href="#其他图像" class="headerlink" title="其他图像"></a>其他图像</h3><p>pgfplots还支持散点图，直方图，3d图等图像。由于笔者暂时没用到，对这些图不甚了解，就不做阐述了。值得注意的是，在这些图像中使用dat数据格式的数据较多。</p><h3 id="导言区的一些设定"><a href="#导言区的一些设定" class="headerlink" title="导言区的一些设定"></a>导言区的一些设定</h3><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\pgfplotsset</span>&#123;width=10cm&#125;</span><br></pre></td></tr></table></figure><p>这里指定了每一张图的宽度为10cm</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\usepgfplotslibrary</span>&#123;external&#125;</span><br><span class="line"><span class="keyword">\tikzexternalize</span> </span><br></pre></td></tr></table></figure><p>由于<script type="math/tex">LATEX</script> 诞生初期并未考虑使其具备绘图功能，因此当文档中有多个pgfplot图形或它们非常复杂时，渲染它们将花费大量时间。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;持续更新遇到的问题，完结或不再使用该包将删除这句话&lt;/p&gt;
&lt;h2 id=&quot;pgfplots&quot;&gt;&lt;a href=&quot;#pgfplots&quot; class=&quot;headerlink&quot; title=&quot;pgfplots&quot;&gt;&lt;/a&gt;pgfplots&lt;/h2&gt;&lt;p&gt;Pgfplots是一种可视化工具，可简化在文档中包含绘图的过程。基本思想是，用户提供输入数据/公式，然后pgfplots 宏包会帮助用户绘制响应的图像。&lt;/p&gt;</summary>
    
    
    
    <category term="软件" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/"/>
    
    <category term="latex" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/latex/"/>
    
    <category term="package" scheme="https://deepcity.github.io/categories/%E8%BD%AF%E4%BB%B6/latex/package/"/>
    
    
    <category term="Latex" scheme="https://deepcity.github.io/tags/Latex/"/>
    
    <category term="图像处理" scheme="https://deepcity.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>MindSpore专题</title>
    <link href="https://deepcity.github.io/2024/special_subject/MindSpore/MindSpore%E5%AD%A6%E4%B9%A0%E7%9B%AE%E5%BD%95/article.html"/>
    <id>https://deepcity.github.io/2024/special_subject/MindSpore/MindSpore%E5%AD%A6%E4%B9%A0%E7%9B%AE%E5%BD%95/article.html</id>
    <published>2024-08-14T11:47:58.000Z</published>
    <updated>2024-08-17T07:58:14.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MindSpore-Studying-By-Windows-And-Ubuntu"><a href="#MindSpore-Studying-By-Windows-And-Ubuntu" class="headerlink" title="MindSpore Studying By Windows And Ubuntu"></a>MindSpore Studying By Windows And Ubuntu</h1><p>文档编写采用Typora，须获得更好观看体验请自行clone本仓库</p><h2 id="初级教程"><a href="#初级教程" class="headerlink" title="初级教程"></a>初级教程</h2><p><a href="../Chapters/Concept/article.html">前置数学</a></p><span id="more"></span><p><a href="../Chapters/First_Install/article.html">第一章 Ubuntu以及Windows安装MindSpore</a></p><p><a href="../Chapters/Second_TryMindSpore/article.html">第二章 尝试使用MindSpore</a></p><p><a href="../Chapters/Third_Tensor/article.html">第三章 张量</a></p><p><a href="../Chapters/Fourth_DataSet/article.html">第四章 数据集</a></p><p><a href="../Chapters/Fivth_ConstructNetwork/article.html">第五章 网格构建</a></p><p><a href="../Chapters/Sixth_FunctionAutoDifferentalCalc/article.html">第六章 函数式自动微分</a></p><p><a href="../Chapters/Seven_ModelTrain/article.html">第七章 模型训练</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;MindSpore-Studying-By-Windows-And-Ubuntu&quot;&gt;&lt;a href=&quot;#MindSpore-Studying-By-Windows-And-Ubuntu&quot; class=&quot;headerlink&quot; title=&quot;MindSpore Studying By Windows And Ubuntu&quot;&gt;&lt;/a&gt;MindSpore Studying By Windows And Ubuntu&lt;/h1&gt;&lt;p&gt;文档编写采用Typora，须获得更好观看体验请自行clone本仓库&lt;/p&gt;
&lt;h2 id=&quot;初级教程&quot;&gt;&lt;a href=&quot;#初级教程&quot; class=&quot;headerlink&quot; title=&quot;初级教程&quot;&gt;&lt;/a&gt;初级教程&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;../Chapters/Concept/article.html&quot;&gt;前置数学&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="专题" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/"/>
    
    <category term="机器学习" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Mindspore" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Mindspore/"/>
    
    <category term="基本概念" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Mindspore/%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    
    
    <category term="机器学习" scheme="https://deepcity.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="MindSpore" scheme="https://deepcity.github.io/tags/MindSpore/"/>
    
    <category term="目录" scheme="https://deepcity.github.io/tags/%E7%9B%AE%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>MindSpore专题——第〇章、概念</title>
    <link href="https://deepcity.github.io/2024/special_subject/MindSpore/Chapters/Concept/article.html"/>
    <id>https://deepcity.github.io/2024/special_subject/MindSpore/Chapters/Concept/article.html</id>
    <published>2024-08-14T11:47:58.000Z</published>
    <updated>2024-08-29T11:27:04.971Z</updated>
    
    <content type="html"><![CDATA[<h1 id="初等概念名词解释">初等概念名词解释</h1><p>在接近学完初级的机器学习后，我意识到单纯的看MindSpore文档以及看Youtube视频是远远不够的，我必须系统的学习一些概念知识以避免在后面的学习中云里雾里，故就有了这一章，在这一章中几乎全是对概念的解释，他们的来源广泛，传送门我都会贴在此处，供查阅：</p><blockquote><p><span class="exturl" data-url="aHR0cHM6Ly93d3cucnVhbnlpZmVuZy5jb20vYmxvZy8yMDE3LzA3L25ldXJhbC1uZXR3b3JrLmh0bWw=">神经网络入门- 阮一峰的网络日志 (ruanyifeng.com)<i class="fa fa-external-link-alt"></i></span></p><p>https://github.com/exacity/deeplearningbook-chinese</p></blockquote><span id="more"></span><h2 id="感知机perceptron">感知机（Perceptron）</h2><figure><img src="https://s2.loli.net/2024/06/10/bkVr3Lymea1OAnw.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>​上图的圆圈就代表一个感知器。它接受多个输入（x1，x2，x3...），产生一个输出（output），好比神经末梢感受各种外部环境的变化，最后产生电信号。</p><p>​ 感知机是输出传入参数的一个函数变换，大多数情况下他是(最初涉及) <spanclass="math display">\[output = \sigma(w_1*a_1 +\cdots +w_n*a_n + b)\]</span> 其中<span class="math display">\[\sigma\]</span>​函数表达式如下 <span class="math display">\[σ(z) = 1 / (1 + e^{-z})\]</span></p><h2 id="表示学习representation-learning">表示学习（RepresentationLearning）</h2><p>使用机器学习来发掘表示本身，而不仅仅把表示映射到输出,学习到的表示往往比手动设计的表示表现得更好。并且不需要人工干预就能迅速适应新的任务。</p><h2 id="变差因素">变差因素</h2><p>在此背景下，‘‘因素’’这个词仅指代影响的不同来源；因素通常不是乘性组合。这些因素通常是不能被直接观察到的量。相反，它们可能是现实世界中观察不到的物体或者不可观测的力，但会影响可观测的量。为了对观察到的数据提供有用的简化解释或推断其原因，它们还可能以概念的形式存在于人类的思维中。</p><p>它们可以被看作数据的概念或者抽象，帮助我们了解这些数据的丰富多样性。</p><h2 id="可见层">可见层</h2><p>也就是神经网路的输入层，这样命名的原因是因为它包含我们能观察到的变量。</p><h2 id="隐藏层">隐藏层</h2><p>也就是神经网路的中间层。因为它们的值不在数据中给出，所以将这些层称为‘‘隐藏”;模型必须确定哪些概念有利于解释观察数据中的关系。这里的图像是每个隐藏单元表示的特征的可视化。这里也是分形的思想运用的层次。</p><h2 id="输出层">输出层</h2><p>输出神经网路的判断也称Object Identify</p><h2 id="前馈深度网络">前馈深度网络</h2><h3 id="多层感知机multilayer-perceptron">多层感知机（MultilayerPerceptron）</h3><p>多层感知机仅仅是一个将一组输入值映射到输出值的数学函数。该函数由许多较简单的函数复合而成。我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。</p><h2 id="为什么需要使用非线性函数">为什么需要使用非线性函数</h2><p>这是由于线性模型的局限性，一个很经典的例子是，线性模型是无法学习异或函数的。</p><h2 id="分布式表示">分布式表示</h2><p>其思想是：系统的每一个输入都应该由多个特征表示，并且每一个特征都应该参与到多个可能输入的表示。这显然也与神经网络每一层layer与layer之间的dense有关联</p><h2 id="激活函数">激活函数</h2><p>激活函数是神经网络中的一个重要组件，用于对输入信号进行非线性变换，他决定了一个神经元是否被激活，或者说神经元的输出是什么。</p><p>激活函数在数学上表示为一个非线性的函数。下面是一些常见的激活函数</p><h3 id="sigmoid函数">Sigmoid函数</h3><p><span class="math display">\[σ(x) = 1 / (1 + e^{-x})\]</span>输出介于0到1之间，常用于二分类任务，但容易在深层网络中造成梯度消失的问题</p><figure><img src="https://s2.loli.net/2024/08/26/DxICv7oeaSA5hrz.png"alt="Sigmoid" /><figcaption aria-hidden="true">Sigmoid</figcaption></figure><h3 id="tanh函数">Tanh函数</h3><p>输出值在-1到1之间，相对于Sigmoid函数，其输出均值为0，但同样存在梯度消失的问题。</p><p><span class="math display">\[tanh(x) =  (e^x - e^{-x}) / (e^x +e^{-x})\]</span></p><figure><img src="https://s2.loli.net/2024/08/26/fs2bXCkaleBDYQS.png"alt="Tanh" /><figcaption aria-hidden="true">Tanh</figcaption></figure><h3 id="relu-rectified-linear-unit函数">ReLU (Rectified LinearUnit函数)</h3><p>最常见的激活函数之一</p><p>输出非负值，计算简单，能够缓解梯度消失问题，但可能存在“死亡ReLU”现象（即某些神经元在训练过程中永远不被激活）</p><p><span class="math display">\[ReLU(x) = max(0 ,x)\]</span></p><h3 id="leaky-relu函数">Leaky ReLU函数</h3><p>ReLU的改进版本，允许负值以一个小的斜率通过，减少死亡ReLU中的死亡现象。</p><p><span class="math display">\[Leaky\ ReLU(x) = max(\alpha x,x)\]</span></p><p>其中<span class="math inline">\(\alpha\)</span> 是一个较小的常数</p><h3 id="softmax函数">Softmax函数</h3><p>通常用于多分类问题的输出层，将神经元的输出转换为概率分布，输出值的总和为1。</p><p><span class="math display">\[Softmax(x_i) = e^{x_i} / \sume^{x_j}\]</span></p><h2 id="损失函数">损失函数</h2><p>用于量化真实值与预测值之间的差距的函数，它量化了模型预测的错误程度，数值越小表示模型的预测越接近真实值，反之则说明预测误差较大。</p><h3id="回归问题中的损失函数">1<strong>回归问题中的损失函数</strong></h3><ul><li><p>均方误差（Mean Squared Error,MSE）：计算预测值与真实值之间差值的平方和的平均值。MSE是回归问题中最常用的损失函数。</p><ul><li>公式：<span class="math inline">\(MSE = (1/n) * Σ (y_{pred} -y_{true})^2\)</span></li></ul></li><li><p>平均绝对误差（Mean Absolute Error, MAE）</p><p>：计算预测值与真实值之间绝对差值的平均值。</p><ul><li>公式：<span class="math inline">\(MAE = (1/n) * Σ |y_{pred} -y_{true}|\)</span></li></ul></li></ul><h3 id="分类问题中的损失函数">2.<strong>分类问题中的损失函数</strong></h3><ul><li><p>交叉熵损失（Cross-Entropy Loss）</p><p>：用于分类任务，特别是在多分类问题中常用。它衡量模型输出的概率分布与真实分布之间的差异。</p><ul><li><p>对于二分类问题，二元交叉熵损失的公式为：</p><ul><li><p>公式：</p></li><li><p><span class="math display">\[Binary\ Cross-Entropy\ Loss = - (y_{true} * log(y_{pred}) + (1 -y_{true}) * log(1 - y_{pred}))\]</span></p></li></ul></li><li><p>对于多分类问题，使用Softmax和交叉熵损失的组合：</p><ul><li><p>公式：</p></li><li><p><span class="math display">\[Categorical\ Cross-Entropy\ Loss = - \sum y_{true} * log(y_{pred})\]</span></p></li></ul></li></ul></li></ul><h3 id="对比损失hinge-loss">3. <strong>对比损失（HingeLoss）</strong></h3><ul><li>主要用于支持向量机（SVM）等模型，目的是最大化分类边界。HingeLoss通过惩罚错误分类点来优化分类模型。</li><li>公式：<span class="math display">\[Hinge Loss = max(0, 1 - y_{true}* y_{pred})\]</span></li></ul><h3 id="自定义损失函数">4. <strong>自定义损失函数</strong></h3><ul><li>在某些复杂场景中，标准的损失函数可能无法满足需求。此时，研究人员可以根据具体需求定义自适应的损失函数。</li></ul><p>通过最小化损失函数的值，模型可以逐渐提高其在数据上的表现。损失函数的选择应与任务类型及目标密切相关，从而能够准确反映模型性能。</p><h2 id="梯度下降">梯度下降</h2><h2 id="正则化">正则化</h2><h2 id="过拟合">过拟合</h2><h2 id="线性代数">线性代数</h2><h3 id="主对角线">主对角线</h3><p>即满足<span class="math display">\[a =\{a_{i,j}|i=j\}\]</span>的元素构成的线，特殊的<spanclass="math display">\[n\not =m\]</span></p><figure><img src="https://s2.loli.net/2024/06/10/7WA9zDuB2UYrlKF.png"alt="image-20240609143044969" /><figcaption aria-hidden="true">image-20240609143044969</figcaption></figure><h3 id="转置">转置</h3><p>即矩阵对主对角线的镜像，特俗的有<span class="math display">\[n\not =m\]</span></p><figure><img src="https://s2.loli.net/2024/06/10/vtux96qNrkAH8jZ.png"alt="image-20240609143213322" /><figcaption aria-hidden="true">image-20240609143213322</figcaption></figure><h3 id="元素对应乘积hadamard-乘积">元素对应乘积（Hadamard 乘积）</h3><p>记为<span class="math display">\[A\odot B\]</span></p><h3 id="点积">点积</h3><p>两个相同维数的向量x和y的点积（dot product）可看作是矩阵乘积<spanclass="math display">\[A^\topB\]</span>​。我们可以把矩阵乘积C=AB中计算Cij的步骤看作是A的第i行和B的第j列之间的点积。</p><p>矩阵的乘法是不满足交换律的，但是矩阵的点积是满足的。 <spanclass="math display">\[x^\top y = y^\top x\]</span></p><p>矩阵乘积转置的简单形式 <span class="math display">\[(AB)^\top = B^\top A^\top\]</span> 注意顺序是不能更改的因为矩阵乘法不满足交换律</p><h3 id="单位矩阵">单位矩阵</h3><p>单位矩阵指主对角线上的值都为1，其他地方都为零的矩阵。</p><p>我们将保持n维向量不变的单位矩阵记作<spanclass="math display">\[I_n\]</span>。</p><h3 id="逆矩阵">逆矩阵</h3><p>很朴素的定义，需要注意的是由于矩阵乘法不满足交换律，因此我们再说一个矩阵的逆的时候通常是说矩阵的左逆。</p><p>对于方阵而言，它的左逆和右逆是相等的</p><h3 id="线性相关与生成子空间">线性相关与生成子空间</h3><p>如果逆矩阵<spanclass="math display">\[A^{-1}\]</span>​存在。那么式(2.11)肯定对于每一个向量b恰好存在一个解。但是，对于方程组而言，对于向量b的某些值，有可能不存在解，或者存在无限多个解。存在多于一个解但是少于无限多个解的情况是不可能发生的；因为如果x和y都是某方程组的解，则x,y则构成了一个张成空间。</p><p>为了分析方程有多少个解，我们可以将A的列向量看作从原点（origin）（元素都是零的向量）出发的不同方向，确定有多少种方法可以到达向量b。在这个观点下，向量x中的每个元素表示我们应该沿着这些方向走多远，即xi表示我们需要沿着第i个向量的方向走多远：<span class="math display">\[Ax = \sum _i x_i A_{:,i}\]</span>这样的操作我们称为线性组合。形式上，一组向量的线性组合是指每个向量乘以对应标量系数之后的和，即：<span class="math display">\[\sum _i c_i v^{(i)}\]</span>一组向量的生成子空间（span）是原始向量线性组合后所能抵达的点的集合。</p><p>确定Ax=b是否有解相当于确定向量b是否在A列向量的生成子空间中。这个特殊的生成子空间被称为A的列空间（columnspace）或者A的值域（range）。</p><p>为了使方程Ax=b对于任意向量<span class="math display">\[b\in\mathbb{R}^m\]</span>都存在解，我们要求A的列空间构成整个<spanclass="math display">\[\mathbb{R}^m\]</span>。如果<spanclass="math display">\[\mathbb{R}^m\]</span>中的某个点不在A的列空间中，那么该点对应的b会使得该方程没有解。矩阵A的列空间是整个<spanclass="math display">\[\mathbb{R}^m\]</span>的要求，意味着A至少有m列，即n&gt;=m。否则，A列空间的维数会小于m。例如，假设A是一个<spanclass="math display">\[3\times2\]</span>的矩阵。目标b是3维的，但是x只有2维。所以无论如何修改x的值，也只能描绘出<spanclass="math display">\[\mathbb{R}^3\]</span>空间中的二维平面。当且仅当向量b在该二维平面中时，该方程有解。</p><p>不等式n&gt;=m仅是方程对每一点都有解的必要条件。这不是一个充分条件，因为有些列向量可能是冗余的。假设有一个<spanclass="math display">\[\mathbb{R}^{2\times2}\]</span>中的矩阵，它的两个列向量是相同的。那么它的列空间和它的一个列向量作为矩阵的列空间是一样的。换言之，虽然该矩阵有2列，但是它的列空间仍然只是一条线，不能涵盖整个<spanclass="math display">\[\mathbb{R}^2\]</span>空间。</p><p>正式地说，这种冗余被称为线性相关（lineardependence）。如果一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么这组向量称为线性无关（linearlyindependent）。如果某个向量是一组向量中某些向量的线性组合，那么我们将这个向量加入这组向量后不会增加这组向量的生成子空间。这意味着，如果一个矩阵的列空间涵盖整个<spanclass="math display">\[\mathbb{R}^m\]</span>，那么该矩阵必须包含至少一组m个线性无关的向量。这是对于每一个向量b的取值都有解的充分必要条件。值得注意的是，这个条件是说该向量集恰好有m个线性无关的列向量，而不是至少m个。不存在一个m维向量的集合具有多于m个彼此线性不相关的列向量，但是一个有多于m个列向量的矩阵有可能拥有不止一个大小为m的线性无关向量集。</p><p>要想使矩阵可逆，我们还需要保证对于每一个b值至多有一个解。为此，我们需要确保该矩阵至多有m个列向量。否则，该方程会有不止一个解。</p><h3 id="奇异矩阵singular-square">奇异矩阵（singular square）</h3><p>该矩阵必须是一个方阵（square），即m=n，并且所有列向量都是线性无关的。一个列向量线性相关的方阵被称为奇异的（singular）。</p><h3 id="矩阵右乘">矩阵右乘</h3><p><span class="math display">\[AA^{-1}=I\]</span></p><h3 id="范数norm">范数（norm）</h3><p>用来衡量一个向量的大小。机器学习中常用范数衡量向量大小。形式上<spanclass="math display">\[L^p\]</span>定义如下 <spanclass="math display">\[||x||_p = \left( \sum _i |x_i|^p \right) ^{\frac{1}{p}}\]</span> 范数（包括Lp范数）是将向量映射到非负值的函数</p><p>严格的讲，范数是满足以下性质的函数</p><ul><li>f(x) = 0 =&gt; x=0</li><li>f(x+y) &lt;=f(x) + f(y) （三角不等式）</li><li>对<span class="math display">\[\forall \alpha \in\mathbb{R},f(\alpha x) = |\alpha|f(x)\]</span></li></ul><p>当p= 2时，<spanclass="math display">\[L^2\]</span>范数被称为欧几里得范数（Euclideannorm）。它表示从原点出发到向量x确定的点的欧几里得距离。<spanclass="math display">\[L^2\]</span>范数在机器学习中出现地十分频繁，经常简化表示为∥x∥，略去了下标2。平方<spanclass="math display">\[L^2\]</span>范数也经常用来衡量向量的大小，可以简单地通过点积<spanclass="math display">\[x ^\top x\]</span>计算。</p><p>但是在很多情况下，平方<spanclass="math display">\[L^2\]</span>范数也可能不受欢迎，因为它在原点附近增长得十分缓慢。在某些机器学习应用中，区分恰好是零的元素和非零但值很小的元素是很重要的。在这些情况下，我们转而使用在各个位置斜率相同，同时保持简单的数学形式的函数：<spanclass="math display">\[L^1\]</span>范数。<spanclass="math display">\[L^1\]</span>范数可以简化如下： <spanclass="math display">\[||x||_1 = \sum _i |x_i|\]</span>有时候我们会统计向量中非零元素的个数来衡量向量的大小。有些作者将这种函数称为“<spanclass="math display">\[L^0\]</span>范数’’，但是这个术语在数学意义上是不对的</p><p>另外一个经常在机器学习中出现的范数是<spanclass="math display">\[L^\inf\]</span>范数，也被称为最大范数（maxnorm）。这个范数表示向量中具有最大幅值的元素的绝对值：<span class="math display">\[||x||_1 = \max _i |x_i|\]</span>有时候我们可能也希望衡量矩阵的大小。在深度学习中，最常见的做法是使用Frobenius范数（Frobeniusnorm）， <span class="math display">\[||A||_F = \sqrt{\sum _{i,j} A^2_{i,j}}\]</span> 两个向量的点集可以用范数来表示，具体的 <spanclass="math display">\[x^\top y = ||x||_2 ||y||_2 cos\theta\]</span> <spanclass="math display">\[\theta\]</span>表示x,y之间的夹角</p><h3 id="对角矩阵">对角矩阵</h3><p>只在主对角线上含有非零元素，，其他位置都是零。用diag(<spanclass="math display">\[v\]</span>)表示一个对角矩阵。</p><p>计算乘法diag(v)x，我们只需要将x中的每个元素xi放大vi倍。换言之，diag(v)x=v⊙x</p><p>对角方阵的逆矩阵存在，当且仅当对角元素都是非零值，在这种情况下，diag(v)1=diag([1/v1;....;1/vn]⊤)。</p><p>非方阵的对角矩阵没有逆矩阵</p><h3 id="对称矩阵">对称矩阵</h3><p>对称矩阵是矩阵的转置和自己相等的矩阵 <span class="math display">\[A = A ^\top\]</span></p><h3 id="单位矩阵-1">单位矩阵</h3><p>具有单位范数的矩阵</p><p>如果<span class="math display">\[x^\topy=0\]</span>，那么向量x和向量y互相正交（orthogonal）。如果两个向量都有非零范数，那么这两个向量之间的夹角是90度。在Rn中，至多有n个范数非零向量互相正交。如果这些向量不仅互相正交，并且范数都为1，那么我们称它们是标准正交（orthonormal）。</p><p>正交矩阵（orthogonalmatrix）是指行向量和列向量是分别标准正交的方阵，更具体的，他们是满足以下条件的矩阵<span class="math display">\[A^\top A = A ^\top A = I.\]</span> 这样意味着 <span class="math display">\[A^{-1} = A^\top\]</span></p><h3 id="特征分解">特征分解</h3><p>特征分解（eigendecomposition）是使用最广的矩阵分解之一，即我们将矩阵分解成一组特征向量和特征值。</p><p>方阵A的特征向量（eigenvector）是指与A相乘后相当于对该向量进行缩放的非零向量v：<span class="math display">\[A v = \lambda v\]</span> 标量<spanclass="math inline">\(\lambda\)</span>被称为这个特征向量对应的特征值（eigenvalue）。</p><p>（类似地，我们也可以定义左特征向量（left eigenvector）<spanclass="math display">\[v^⊤A=\lambdav^⊤\]</span>，但是通常我们更关注右特征向量（right eigenvector））。</p><p>所有特征值都是正数的矩阵被称为正定（positivedefinite）；所有特征值都是非负数的矩阵被称为半正定（positivesemidefinite）。同样地，所有特征值都是负数的矩阵被称为负定（negativedefinite）；所有特征值都是非正数的矩阵被称为半负定（negativesemidefinite）。</p><p>然而，我们也常常希望将矩阵分解（decompose）成特征值和特征向量。这样可以帮助我们分析矩阵的特定性质，就像质因数分解有助于我们理解整数。</p><p>矩阵A的特征分解可以记作 <span class="math display">\[A = V diag(\lambda) V^{-1}\]</span>不是每一个矩阵都可以分解成特征值和特征向量。在某些情况下，特征分解存在，但是会涉及复数而非实数。</p><p>其中Q是A的特征向量组成的正交矩阵，是对角矩阵。特征值<spanclass="math display">\[\lambda_{i;i}\]</span>对应的特征向量是矩阵Q的第i列，记作Q:;i。因为Q是正交矩阵，我们可以将A看作沿方向v(i)延展i倍的空间</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;初等概念名词解释&quot;&gt;初等概念名词解释&lt;/h1&gt;
&lt;p&gt;在接近学完初级的机器学习后，我意识到单纯的看MindSpore文档以及看Youtube视频是远远不够的，我必须系统的学习一些概念知识以避免在后面的学习中云里雾里，故就有了这一章，在这一章中几乎全是对概念的解释，他们的来源广泛，传送门我都会贴在此处，供查阅：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly93d3cucnVhbnlpZmVuZy5jb20vYmxvZy8yMDE3LzA3L25ldXJhbC1uZXR3b3JrLmh0bWw=&quot;&gt;神经网络入门
- 阮一峰的网络日志 (ruanyifeng.com)&lt;i class=&quot;fa fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;https://github.com/exacity/deeplearningbook-chinese&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="专题" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/"/>
    
    <category term="机器学习" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Mindspore" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Mindspore/"/>
    
    <category term="基本概念" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Mindspore/%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    
    
    <category term="机器学习" scheme="https://deepcity.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="MindSpore" scheme="https://deepcity.github.io/tags/MindSpore/"/>
    
    <category term="目录" scheme="https://deepcity.github.io/tags/%E7%9B%AE%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>MindSpore专题——第一章、安装</title>
    <link href="https://deepcity.github.io/2024/special_subject/MindSpore/Chapters/First_Install/article.html"/>
    <id>https://deepcity.github.io/2024/special_subject/MindSpore/Chapters/First_Install/article.html</id>
    <published>2024-08-14T11:47:58.000Z</published>
    <updated>2024-08-16T08:58:49.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Ubuntu系统安装"><a href="#Ubuntu系统安装" class="headerlink" title="Ubuntu系统安装"></a>Ubuntu系统安装</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p> VMware Workstation 17Pro</p><ol><li>原有的系统为Ubuntu22.04因此下载Ubuntu18.04 镜像光盘文件<ul><li>通过cat /proc/version文件查看当前的系统版本</li></ul></li></ol><span id="more"></span><p><img src="https://s2.loli.net/2024/06/04/9tNsqxUkrJFHQOc.png" alt="image-20240604174857507"></p><ol><li><p>下载Ubuntu18.04系统光盘<span class="exturl" data-url="aHR0cHM6Ly9yZWxlYXNlcy51YnVudHUuY29tLzE4LjA0L3VidW50dS0xOC4wNC42LWRlc2t0b3AtYW1kNjQuaXNv">https://releases.ubuntu.com/18.04/ubuntu-18.04.6-desktop-amd64.iso<i class="fa fa-external-link-alt"></i></span></p></li><li><p>VMware简易安装后对18旧版本遇到的问题的解决方案</p><ol><li>旧版本屏幕无法适应屏幕，想到vmwaretools的问题，检查该部分发现并未自动安装</li></ol></li></ol><p>通过重新启动挂载\VMware\VMware Workstation目录下linux.iso文件出现</p><p><img src="https://s2.loli.net/2024/06/06/IWioHYeLj9JgMtb.png" alt="image-20240604182654391"></p><p>安装该软件</p><p><img src="https://s2.loli.net/2024/06/04/vIXUHlWVx2sb1co.png" alt="image-20240604182901358"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo perl vmware-install.pl</span><br></pre></td></tr></table></figure><p>通过在解压的目录下运行该脚本文件安装vmware-tools</p><p><img src="https://s2.loli.net/2024/06/04/pJI4WAgKdYmvPES.png" alt="image-20240604183315457"></p><p>在经过一系列configure后安装完成</p><p>由此问题解决</p><pre><code> 2. update apt-get</code></pre><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo update apt-get</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2024/06/04/9cXhEq3V2DMiWNx.png" alt="image-20240604184437693"></p><ol><li>修复vmware剪贴板不共享的问题</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install open-vm-tools-desktop</span><br></pre></td></tr></table></figure><p>​    安装该软件一路回车即可</p><h2 id="尝试自动脚本安装"><a href="#尝试自动脚本安装" class="headerlink" title="尝试自动脚本安装"></a>尝试自动脚本安装</h2><p>由于vmware对nvdia的支持补全，此处采用版本如下</p><p><img src="https://s2.loli.net/2024/06/04/5e3IG9tuAPdfjKr.png" alt="image-20240604183528425"></p><p>进入默认源码目录进行操作</p><p><img src="https://s2.loli.net/2024/06/04/K8tqnFmeQuOMiZc.png" alt="image-20240604183616489"></p><p><img src="https://s2.loli.net/2024/06/04/d1YgnqUjbWJfXLH.png" alt="image-20240604185144770"></p><p>由于系统全新，设定一下系统密码并切换到root用户</p><p><img src="https://s2.loli.net/2024/06/04/KQiWzJtOUsS2mnT.png" alt="image-20240604185341858"></p><p>根据官网下载</p><p><img src="https://s2.loli.net/2024/06/04/g3BCMVGQHDzjAUL.png" alt="image-20240604185417980"></p><p>根据官网命令执行脚本</p><p><img src="https://s2.loli.net/2024/06/05/xBlJgjPwU3krsqa.png" alt="MindSpore安装成功"></p><p>可见安装成功</p><h2 id="Windows中安装MindSpore以及杂项"><a href="#Windows中安装MindSpore以及杂项" class="headerlink" title="Windows中安装MindSpore以及杂项"></a>Windows中安装MindSpore以及杂项</h2><ol><li>相似的安装程序，记得更改选定的官方安装脚本</li><li>Windows一般位于C:\Users\%USERNAME%\pip 目录下存在配置文件，可通过更改该配置文件修改镜像，在使用镜像中启用代理可能会导致满屏的飘红报错</li><li>对于MindSpore的前置软件安装而言，最重要易错的为Python的版本，建议多通过</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python --version</span><br></pre></td></tr></table></figure><ol><li>查看当前版本，及时正确配置环境变量</li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Ubuntu系统安装&quot;&gt;&lt;a href=&quot;#Ubuntu系统安装&quot; class=&quot;headerlink&quot; title=&quot;Ubuntu系统安装&quot;&gt;&lt;/a&gt;Ubuntu系统安装&lt;/h2&gt;&lt;h3 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h3&gt;&lt;p&gt; VMware Workstation 17Pro&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;原有的系统为Ubuntu22.04因此下载Ubuntu18.04 镜像光盘文件&lt;ul&gt;
&lt;li&gt;通过cat /proc/version文件查看当前的系统版本&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="专题" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/"/>
    
    <category term="机器学习" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Mindspore" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Mindspore/"/>
    
    <category term="基本概念" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Mindspore/%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    
    
    <category term="机器学习" scheme="https://deepcity.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="MindSpore" scheme="https://deepcity.github.io/tags/MindSpore/"/>
    
    <category term="目录" scheme="https://deepcity.github.io/tags/%E7%9B%AE%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>MindSpore专题——第五章、网络构建</title>
    <link href="https://deepcity.github.io/2024/special_subject/MindSpore/Chapters/Fivth_ConstructNetwork/article.html"/>
    <id>https://deepcity.github.io/2024/special_subject/MindSpore/Chapters/Fivth_ConstructNetwork/article.html</id>
    <published>2024-08-14T11:47:58.000Z</published>
    <updated>2024-08-16T08:58:53.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网格构建"><a href="#网格构建" class="headerlink" title="网格构建"></a>网格构建</h2><p>神经网络模型是由神经网络层和Tensor操作构成的，<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL21pbmRzcG9yZS5ubi5odG1s">mindspore.nn<i class="fa fa-external-link-alt"></i></span>提供了常见神经网络层的实现，在MindSpore中，<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL25uL21pbmRzcG9yZS5ubi5DZWxsLmh0bWw=">Cell<i class="fa fa-external-link-alt"></i></span>类是构建所有网络的基类，也是网络的基本单元。一个神经网络模型表示为一个<code>Cell</code>，它由不同的子<code>Cell</code>构成。使用这样的嵌套结构，可以简单地使用面向对象编程的思维，对神经网络结构进行构建和管理。</p><p>下面我们将构建一个用于Mnist数据集分类的神经网络模型。</p><span id="more"></span><h3 id="定义模型类"><a href="#定义模型类" class="headerlink" title="定义模型类"></a>定义模型类</h3><p>当我们定义神经网络时，可以继承<code>nn.Cell</code>类，在<code>__init__</code>方法中进行子Cell的实例化和状态管理，在<code>construct</code>方法中实现Tensor操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(nn.Cell):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">        <span class="variable language_">self</span>.dense_relu_sequential = nn.SequentialCell(</span><br><span class="line">            nn.Dense(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>, weight_init=<span class="string">&quot;normal&quot;</span>, bias_init=<span class="string">&quot;zeros&quot;</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dense(<span class="number">512</span>, <span class="number">512</span>, weight_init=<span class="string">&quot;normal&quot;</span>, bias_init=<span class="string">&quot;zeros&quot;</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dense(<span class="number">512</span>, <span class="number">10</span>, weight_init=<span class="string">&quot;normal&quot;</span>, bias_init=<span class="string">&quot;zeros&quot;</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">construct</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.dense_relu_sequential(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Network()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Network&lt;</span><br><span class="line">  (flatten): Flatten&lt;&gt;</span><br><span class="line">  (dense_relu_sequential): SequentialCell&lt;</span><br><span class="line">    (0): Dense&lt;input_channels=784, output_channels=512, has_bias=True&gt;</span><br><span class="line">    (1): ReLU&lt;&gt;</span><br><span class="line">    (2): Dense&lt;input_channels=512, output_channels=512, has_bias=True&gt;</span><br><span class="line">    (3): ReLU&lt;&gt;</span><br><span class="line">    (4): Dense&lt;input_channels=512, output_channels=10, has_bias=True&gt;</span><br><span class="line">    &gt;</span><br><span class="line">  &gt;</span><br></pre></td></tr></table></figure></blockquote><p>我们构造一个输入数据，直接调用模型，可以获得一个十维的Tensor输出，其包含每个类别的原始预测值。</p><p><strong><code>model.construct()</code>方法不可直接调用。</strong></p><p>这里出现了很多很抽象的概念好像一下子让机器学习变成了一个黑盒子，实际上，我们观察上面的代码，很容易可以从中看出一些结构来，下面记录一下我这个初学者的理解。</p><p>首先搬出一幅经典的图像</p><p><img src="https://s2.loli.net/2024/06/07/n358NwDPeSg1LsZ.jpg" alt="神经网络入门 - 阮一峰的网络日志"></p><p>这幅图形象的描述了上面的过程。其中</p><p><img src="https://s2.loli.net/2024/06/07/KgVFjZ6LTY54r1s.png" alt="image-20240607140358272"></p><p>这里开始的inputLayer就是我们的输入数据，中间连的密密麻麻的线就是上面结构输出中的Dense，可以观察到我们分别有<script type="math/tex">784\rightarrow 512\rightarrow512\rightarrow10</script>​四层这样的全连接。最初的784也和28*28对应了起来。即我们每一个像素点都是一个参数，为最初输入的一个点，最后输出的则是代表数字概率的10个参数，最终我们会选择一个最接近的参数。这里的变量名用logits（不确定是否标准，但这里是这么用的）</p><p>下面是一些展示调用的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = ops.ones((<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), mindspore.float32)</span><br><span class="line">logits = model(X)</span><br><span class="line"><span class="comment"># print logits</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(Tensor(logits))</span><br><span class="line">pred_probab = nn.Softmax(axis=<span class="number">1</span>)(logits)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>读者可以试着运行几次，由于给出数据全是1，代表全黑或全白。因此输出是杂乱的。但可以观察到，我们最终的预测值为最大的值的标签。</p><p>下面官网上给出了一些模型层次的解释，来看看。</p><h3 id="模型层"><a href="#模型层" class="headerlink" title="模型层"></a>模型层</h3><blockquote><p> 本节中我们分解上节构造的神经网络模型中的每一层。首先我们构造一个shape为(3, 28, 28)的随机数据（3个28x28的图像），依次通过每一个神经网络层来观察其效果。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_image = ops.ones((3, 28, 28), mindspore.float32)</span><br><span class="line">print(input_image.shape)</span><br></pre></td></tr></table></figure><blockquote><p>(3, 28, 28)</p></blockquote><h3 id="nn-Flatten"><a href="#nn-Flatten" class="headerlink" title="nn.Flatten"></a>nn.Flatten</h3><blockquote><p> 实例化<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL25uL21pbmRzcG9yZS5ubi5GbGF0dGVuLmh0bWw=">nn.Flatten<i class="fa fa-external-link-alt"></i></span>层，将28x28的2D张量转换为784大小的连续数组。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flatten = nn.Flatten()</span><br><span class="line">flat_image = flatten(input_image)</span><br><span class="line"><span class="built_in">print</span>(flat_image.shape)</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(3, 784)</span><br></pre></td></tr></table></figure></blockquote><p>这里可见flatten打平只是打平第一维以外即输入数据样例外的维度，如果说初始数据是一个结构体数组，那Flatten的作用就是将其变为一个一维数组的数组。</p><h3 id="nn-Dense"><a href="#nn-Dense" class="headerlink" title="nn.Dense"></a>nn.Dense</h3><blockquote><p><span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL25uL21pbmRzcG9yZS5ubi5EZW5zZS5odG1s">nn.Dense<i class="fa fa-external-link-alt"></i></span>为全连接层，其使用权重和偏差对输入进行线性变换。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">layer1 = nn.Dense(in_channels=28*28, out_channels=20)</span><br><span class="line">hidden1 = layer1(flat_image)</span><br><span class="line">print(hidden1.shape)</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(3, 20)</span><br></pre></td></tr></table></figure></blockquote><p>这里注意到有两个额外的参数即权重和偏差。也就是说下面每一层的节点就是上一层节点的带权和加上一个常数。</p><h3 id="nn-ReLU"><a href="#nn-ReLU" class="headerlink" title="nn.ReLU"></a>nn.ReLU</h3><blockquote><p> <span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL25uL21pbmRzcG9yZS5ubi5SZUxVLmh0bWw=">nn.ReLU<i class="fa fa-external-link-alt"></i></span>层给网络中加入非线性的激活函数，帮助神经网络学习各种复杂的特征。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Before ReLU: <span class="subst">&#123;hidden1&#125;</span>\n\n&quot;</span>)</span><br><span class="line">hidden1 = nn.ReLU()(hidden1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;After ReLU: <span class="subst">&#123;hidden1&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Before ReLU: [[-0.04736331  0.2939465  -0.02713677 -0.30988005 -0.11504349 -0.11661264</span><br><span class="line">   0.18007928  0.43213072  0.12091967 -0.17465964  0.53133243  0.12605792</span><br><span class="line">   0.01825903  0.01287796  0.17238477 -0.1621131  -0.0080034  -0.24523425</span><br><span class="line">  -0.10083733  0.05171938]</span><br><span class="line"> [-0.04736331  0.2939465  -0.02713677 -0.30988005 -0.11504349 -0.11661264</span><br><span class="line">   0.18007928  0.43213072  0.12091967 -0.17465964  0.53133243  0.12605792</span><br><span class="line">   0.01825903  0.01287796  0.17238477 -0.1621131  -0.0080034  -0.24523425</span><br><span class="line">  -0.10083733  0.05171938]</span><br><span class="line"> [-0.04736331  0.2939465  -0.02713677 -0.30988005 -0.11504349 -0.11661264</span><br><span class="line">   0.18007928  0.43213072  0.12091967 -0.17465964  0.53133243  0.12605792</span><br><span class="line">   0.01825903  0.01287796  0.17238477 -0.1621131  -0.0080034  -0.24523425</span><br><span class="line">  -0.10083733  0.05171938]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">After ReLU: [[0.         0.2939465  0.         0.         0.         0.</span><br><span class="line">  0.18007928 0.43213072 0.12091967 0.         0.53133243 0.12605792</span><br><span class="line">  0.01825903 0.01287796 0.17238477 0.         0.         0.</span><br><span class="line">  0.         0.05171938]</span><br><span class="line"> [0.         0.2939465  0.         0.         0.         0.</span><br><span class="line">  0.18007928 0.43213072 0.12091967 0.         0.53133243 0.12605792</span><br><span class="line">  0.01825903 0.01287796 0.17238477 0.         0.         0.</span><br><span class="line">  0.         0.05171938]</span><br><span class="line"> [0.         0.2939465  0.         0.         0.         0.</span><br><span class="line">  0.18007928 0.43213072 0.12091967 0.         0.53133243 0.12605792</span><br><span class="line">  0.01825903 0.01287796 0.17238477 0.         0.         0.</span><br><span class="line">  0.         0.05171938]]</span><br></pre></td></tr></table></figure></blockquote><p>注意到，这里首次提出非线性运算的概念，也就是说，在这之前，运算都是线性的，这一步显然比较复杂，也不谈论。</p><h3 id="nn-SequentialCell"><a href="#nn-SequentialCell" class="headerlink" title="nn.SequentialCell"></a>nn.SequentialCell</h3><blockquote><p><span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL25uL21pbmRzcG9yZS5ubi5TZXF1ZW50aWFsQ2VsbC5odG1s">nn.SequentialCell<i class="fa fa-external-link-alt"></i></span>是一个有序的Cell容器。输入Tensor将按照定义的顺序通过所有Cell。我们可以使用<code>SequentialCell</code>来快速组合构造一个神经网络模型。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">seq_modules = nn.SequentialCell(</span><br><span class="line">    flatten,</span><br><span class="line">    layer1,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Dense(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">logits = seq_modules(input_image)</span><br><span class="line"><span class="built_in">print</span>(logits.shape)</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(3, 10)</span><br></pre></td></tr></table></figure></blockquote><p>注意看代码，对于这里的代码最好都不要跳过，也不要深究实现原理（新手），可见这里依次调用了上面的所有步骤，是对这一流程的封装。</p><h3 id="nn-Softmax"><a href="#nn-Softmax" class="headerlink" title="nn.Softmax"></a>nn.Softmax</h3><blockquote><p> 最后使用<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL25uL21pbmRzcG9yZS5ubi5Tb2Z0bWF4Lmh0bWw=">nn.Softmax<i class="fa fa-external-link-alt"></i></span>将神经网络最后一个全连接层返回的logits的值缩放为[0, 1]，表示每个类别的预测概率。<code>axis</code>指定的维度数值和为1。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">softmax = nn.Softmax(axis=1)</span><br><span class="line">pred_probab = softmax(logits)</span><br></pre></td></tr></table></figure><p>官网文档已经非常清楚了。还有不懂可以跑跑代码</p><h2 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h2><blockquote><p>网络内部神经网络层具有权重参数和偏置参数（如<code>nn.Dense</code>），这些参数会在训练过程中不断进行优化，可通过 <code>model.parameters_and_names()</code> 来获取参数名及对应的参数详情。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model structure: <span class="subst">&#123;model&#125;</span>\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.parameters_and_names():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Layer: <span class="subst">&#123;name&#125;</span>\nSize: <span class="subst">&#123;param.shape&#125;</span>\nValues : <span class="subst">&#123;param[:<span class="number">2</span>]&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>通过这一代码可以查看权重参数和偏置参数，这一代码应该是十分常见并且常用的。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;网格构建&quot;&gt;&lt;a href=&quot;#网格构建&quot; class=&quot;headerlink&quot; title=&quot;网格构建&quot;&gt;&lt;/a&gt;网格构建&lt;/h2&gt;&lt;p&gt;神经网络模型是由神经网络层和Tensor操作构成的，&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL21pbmRzcG9yZS5ubi5odG1s&quot;&gt;mindspore.nn&lt;i class=&quot;fa fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/span&gt;提供了常见神经网络层的实现，在MindSpore中，&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL25uL21pbmRzcG9yZS5ubi5DZWxsLmh0bWw=&quot;&gt;Cell&lt;i class=&quot;fa fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/span&gt;类是构建所有网络的基类，也是网络的基本单元。一个神经网络模型表示为一个&lt;code&gt;Cell&lt;/code&gt;，它由不同的子&lt;code&gt;Cell&lt;/code&gt;构成。使用这样的嵌套结构，可以简单地使用面向对象编程的思维，对神经网络结构进行构建和管理。&lt;/p&gt;
&lt;p&gt;下面我们将构建一个用于Mnist数据集分类的神经网络模型。&lt;/p&gt;</summary>
    
    
    
    <category term="专题" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/"/>
    
    <category term="机器学习" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Mindspore" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Mindspore/"/>
    
    <category term="基本概念" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Mindspore/%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    
    
    <category term="机器学习" scheme="https://deepcity.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="MindSpore" scheme="https://deepcity.github.io/tags/MindSpore/"/>
    
    <category term="目录" scheme="https://deepcity.github.io/tags/%E7%9B%AE%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>MindSpore专题——第四章、数据集</title>
    <link href="https://deepcity.github.io/2024/special_subject/MindSpore/Chapters/Fourth_DataSet/article.html"/>
    <id>https://deepcity.github.io/2024/special_subject/MindSpore/Chapters/Fourth_DataSet/article.html</id>
    <published>2024-08-14T11:47:58.000Z</published>
    <updated>2024-08-16T08:58:57.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>继续<a href="./Second_TryMindSpore.md">第二章</a>中的Mnist数据库为例，介绍使用mindspore.dataset进行加载的方法。</p><p>详情请见MNIST数据官方网站：<span class="exturl" data-url="aHR0cDovL3lhbm4ubGVjdW4uY29tL2V4ZGIvbW5pc3Qv">MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges<i class="fa fa-external-link-alt"></i></span></p><span id="more"></span><p>下面是简略介绍</p><div class="table-container"><table><thead><tr><th style="text-align:center">数据集</th><th style="text-align:center">MNIST中的文件名</th><th style="text-align:center">下载地址</th><th style="text-align:center">文件大小</th></tr></thead><tbody><tr><td style="text-align:center">训练集图像</td><td style="text-align:center">train-images-idx3-ubyte.gz</td><td style="text-align:center"><span class="exturl" data-url="aHR0cDovL3lhbm4ubGVjdW4uY29tL2V4ZGIvbW5pc3QvdHJhaW4taW1hZ2VzLWlkeDMtdWJ5dGUuZ3o=">http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz<i class="fa fa-external-link-alt"></i></span></td><td style="text-align:center">9912422字节</td></tr><tr><td style="text-align:center">训练集标签</td><td style="text-align:center">train-labels-idx1-ubyte.gz</td><td style="text-align:center"><span class="exturl" data-url="aHR0cDovL3lhbm4ubGVjdW4uY29tL2V4ZGIvbW5pc3QvdHJhaW4tbGFiZWxzLWlkeDEtdWJ5dGUuZ3o=">http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz<i class="fa fa-external-link-alt"></i></span></td><td style="text-align:center">28881字节</td></tr><tr><td style="text-align:center">测试集图像</td><td style="text-align:center">t10k-images-idx3-ubyte.gz</td><td style="text-align:center"><span class="exturl" data-url="aHR0cDovL3lhbm4ubGVjdW4uY29tL2V4ZGIvbW5pc3QvdDEway1pbWFnZXMtaWR4My11Ynl0ZS5neg==">http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz<i class="fa fa-external-link-alt"></i></span></td><td style="text-align:center">1648877字节</td></tr><tr><td style="text-align:center">测试集标签</td><td style="text-align:center">t10k-labels-idx1-ubyte.gz</td><td style="text-align:center"><span class="exturl" data-url="aHR0cDovL3lhbm4ubGVjdW4uY29tL2V4ZGIvbW5pc3QvdDEway1sYWJlbHMtaWR4MS11Ynl0ZS5neg==">http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz<i class="fa fa-external-link-alt"></i></span></td><td style="text-align:center">4542字节</td></tr></tbody></table></div><h3 id="数据库加载"><a href="#数据库加载" class="headerlink" title="数据库加载"></a>数据库加载</h3><p><em>请注意：mindspore.dataset的接口仅支持解压后的数据文件</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = MnistDataset(<span class="string">&quot;MNIST_Data/train&quot;</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(train_dataset))</span><br></pre></td></tr></table></figure><blockquote><class 'mindspore.dataset.engine.datasets_vision.MnistDataset'></blockquote><h3 id="数据库迭代"><a href="#数据库迭代" class="headerlink" title="数据库迭代"></a>数据库迭代</h3><p>数据集加载后，一般以迭代方式获取数据，然后送入神经网络中进行训练。我们可以用<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL2RhdGFzZXQvZGF0YXNldF9tZXRob2QvaXRlcmF0b3IvbWluZHNwb3JlLmRhdGFzZXQuRGF0YXNldC5jcmVhdGVfdHVwbGVfaXRlcmF0b3IuaHRtbA==">create_tuple_iterator<i class="fa fa-external-link-alt"></i></span>或<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL2RhdGFzZXQvZGF0YXNldF9tZXRob2QvaXRlcmF0b3IvbWluZHNwb3JlLmRhdGFzZXQuRGF0YXNldC5jcmVhdGVfZGljdF9pdGVyYXRvci5odG1s">create_dict_iterator<i class="fa fa-external-link-alt"></i></span>接口创建数据迭代器，迭代访问数据。访问的数据类型默认为<code>Tensor</code>；若设置<code>output_numpy=True</code>，访问的数据类型为<code>Numpy</code>。</p><p>下面定义一个可视化函数，迭代9张图片进行展示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">visualize</span>(<span class="params">dataset</span>):</span><br><span class="line">    figure = plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">    cols, rows = <span class="number">3</span>, <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    plt.subplots_adjust(wspace=<span class="number">0.5</span>, hspace=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, (image, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataset.create_tuple_iterator()):</span><br><span class="line">        figure.add_subplot(rows, cols, idx + <span class="number">1</span>)</span><br><span class="line">        plt.title(<span class="built_in">int</span>(label))</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">        plt.imshow(image.asnumpy().squeeze(), cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> idx == cols * rows - <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>在<code>for idx, (image, label) in enumerate(dataset.create_tuple_iterator()):</code>此处的循环中枚举了训练集的前9个图像<code>enumerate()</code>函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。</p><p>在循环体中使用了plt类画图。</p><p><img src="https://s2.loli.net/2024/06/06/36CU8spH9eSEkxl.png" alt="image-20240606173353887"></p><h3 id="数据集常用操作"><a href="#数据集常用操作" class="headerlink" title="数据集常用操作"></a>数据集常用操作</h3><p>Pipeline的设计理念使得数据集的常用操作采用<code>dataset = dataset.operation()</code>的异步执行方式，执行操作返回新的Dataset，此时不执行具体操作，而是在Pipeline中加入节点，最终进行迭代时，并行执行整个Pipeline。</p><p>下面分别介绍几种常见的数据集操作。</p><h3 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h3><p>数据集随机<code>shuffle</code>可以消除数据排列造成的分布不均问题</p><p><img src="https://s2.loli.net/2024/06/06/M54ICySt9dzenva.png" alt="op-shuffle"></p><p><code>mindspore.dataset</code>提供的数据集在加载时可配置<code>shuffle=True</code>，或使用如下操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = train_dataset.shuffle(buffer_size=<span class="number">64</span>)</span><br><span class="line">visualize(train_dataset)</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2024/06/06/yB76TmklvtYXIgE.png" alt="image-20240606173414784"></p><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p><code>map</code>操作是数据预处理的关键操作，可以针对数据集指定列（column）添加数据变换（Transforms），将数据变换应用于该列数据的每个元素，并返回包含变换后元素的新数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">image, label = <span class="built_in">next</span>(train_dataset.create_tuple_iterator())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;map前：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(image.shape, image.dtype)</span><br><span class="line">train_dataset = train_dataset.<span class="built_in">map</span>(vision.Rescale(<span class="number">1.0</span> / <span class="number">255.0</span>, <span class="number">0</span>), input_columns=<span class="string">&#x27;image&#x27;</span>)</span><br><span class="line">image, label = <span class="built_in">next</span>(train_dataset.create_tuple_iterator())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;map后：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(image.shape, image.dtype)</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">map前：</span><br><span class="line">(28, 28, 1) UInt8</span><br><span class="line">map后：</span><br><span class="line">(28, 28, 1) Float32</span><br></pre></td></tr></table></figure></blockquote><h3 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h3><p>将数据集打包为固定大小的<code>batch</code>是在有限硬件资源下使用梯度下降进行模型优化的折中方法，可以保证梯度下降的随机性和优化计算量。分块思想</p><p>一般我们会设置一个固定的batch size，将连续的数据分为若干批（batch）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一般设定固定batchSize</span></span><br><span class="line">train_dataset = train_dataset.batch(batch_size=<span class="number">32</span>)</span><br><span class="line"><span class="comment"># batch后的数据增加一维，大小为batch_size</span></span><br><span class="line">image, label = <span class="built_in">next</span>(train_dataset.create_tuple_iterator())</span><br><span class="line"><span class="built_in">print</span>(image.shape, image.dtype)</span><br></pre></td></tr></table></figure><blockquote><p>(32, 28, 28, 1) Float32</p></blockquote><h3 id="自定义数据集"><a href="#自定义数据集" class="headerlink" title="自定义数据集"></a>自定义数据集</h3><p><code>mindspore.dataset</code>模块提供了一些常用的公开数据集和标准格式数据集的加载API。</p><p>对于MindSpore暂不支持直接加载的数据集，可以构造自定义数据加载类或自定义数据集生成函数的方式来生成数据集，然后通过<code>GeneratorDataset</code>接口实现自定义方式的数据集加载。</p><p><code>GeneratorDataset</code>支持通过可随机访问数据集对象、可迭代数据集对象和生成器(generator)构造自定义数据集，下面分别对其进行介绍。</p><h4 id="可随机访问数据集"><a href="#可随机访问数据集" class="headerlink" title="可随机访问数据集"></a>可随机访问数据集</h4><p>可随机访问数据集是实现了<code>__getitem__</code>和<code>__len__</code>方法的数据集，表示可以通过索引/键直接访问对应位置的数据样本。</p><ol><li>实现了<code>__init__</code>，<code>__getitem__</code> 和<code>__len__</code></li><li>当使用<code>dataset[idx]</code>访问这样的数据集时，可以读取dataset内容中第idx个样本或标签</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Random-accessible object as input source</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomAccessDataset</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>._data = np.ones((<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="variable language_">self</span>._label = np.zeros((<span class="number">5</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._data[index], <span class="variable language_">self</span>._label[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>._data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loader = RandomAccessDataset()</span><br><span class="line">dataset = GeneratorDataset(source=loader, column_names=[<span class="string">&quot;data&quot;</span>, <span class="string">&quot;label&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    </span><br><span class="line">loader = [np.array(<span class="number">0</span>), np.array(<span class="number">1</span>), np.array(<span class="number">2</span>)]</span><br><span class="line">dataset = GeneratorDataset(source=loader, column_names=[<span class="string">&quot;data&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>[Tensor(shape=[2], dtype=Float64, value= [ 1.00000000e+00,  1.00000000e+00]), Tensor(shape=[1], dtype=Float64, value= [ 0.00000000e+00])]<br>[Tensor(shape=[2], dtype=Float64, value= [ 1.00000000e+00,  1.00000000e+00]), Tensor(shape=[1], dtype=Float64, value= [ 0.00000000e+00])]<br>[Tensor(shape=[2], dtype=Float64, value= [ 1.00000000e+00,  1.00000000e+00]), Tensor(shape=[1], dtype=Float64, value= [ 0.00000000e+00])]<br>[Tensor(shape=[2], dtype=Float64, value= [ 1.00000000e+00,  1.00000000e+00]), Tensor(shape=[1], dtype=Float64, value= [ 0.00000000e+00])]<br>[Tensor(shape=[2], dtype=Float64, value= [ 1.00000000e+00,  1.00000000e+00]), Tensor(shape=[1], dtype=Float64, value= [ 0.00000000e+00])]</p></blockquote><p>创建的过程非常简单，通过numpy的数据结构为底层，实现三个方法就好了，更简单的直接使用list，tuple也是可行的。</p><h4 id="可迭代数据集"><a href="#可迭代数据集" class="headerlink" title="可迭代数据集"></a>可迭代数据集</h4><p>可迭代的数据集是实现了<code>__iter__</code>和<code>__next__</code>方法的数据集，表示可以通过迭代的方式逐步获取数据样本。这种类型的数据集特别适用于随机访问成本太高或者不可行的情况。</p><p>例如，当使用<code>iter(dataset)</code>的形式访问数据集时，可以读取从数据库、远程服务器返回的数据流。</p><p>下面构造一个简单迭代器，并将其加载至<code>GeneratorDataset</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Iterator as input source</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">IterableDataset</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, start, end</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;init the class object to hold the data&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="variable language_">self</span>.start = start</span><br><span class="line">        <span class="variable language_">self</span>.end = end</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__next__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;iter one data and return&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">next</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;reset the iter&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="variable language_">self</span>.data = <span class="built_in">iter</span>(<span class="built_in">range</span>(<span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end))</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loader = IterableDataset(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">dataset = GeneratorDataset(source=loader, column_names=[<span class="string">&quot;data&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="built_in">print</span>(d)</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>[Tensor(shape=[], dtype=Int32, value= 1)]<br>[Tensor(shape=[], dtype=Int32, value= 2)]<br>[Tensor(shape=[], dtype=Int32, value= 3)]<br>[Tensor(shape=[], dtype=Int32, value= 4)]</p></blockquote><p>同样的，实现方法即可</p><h4 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h4><p>生成器也属于可迭代的数据集类型，其直接依赖Python的生成器类型<code>generator</code>返回数据，直至生成器抛出<code>StopIteration</code>异常。</p><p>下面构造一个生成器，并将其加载至<code>GeneratorDataset</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generator</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_generator</span>(<span class="params">start, end</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, end):</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># since a generator instance can be only iterated once, we need to wrap it by lambda to generate multiple instances</span></span><br><span class="line">dataset = GeneratorDataset(source=<span class="keyword">lambda</span>: my_generator(<span class="number">3</span>, <span class="number">6</span>), column_names=[<span class="string">&quot;data&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="built_in">print</span>(d)</span><br></pre></td></tr></table></figure><blockquote><p>[Tensor(shape=[], dtype=Int32, value= 3)]<br>[Tensor(shape=[], dtype=Int32, value= 4)]<br>[Tensor(shape=[], dtype=Int32, value= 5)]</p></blockquote><p>这个更绝，仅用一个函数即可生成。（此处匿不匿名无关紧要）</p><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><ol><li>找不到模块 matplotlib</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install matplotlib</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;数据集&quot;&gt;&lt;a href=&quot;#数据集&quot; class=&quot;headerlink&quot; title=&quot;数据集&quot;&gt;&lt;/a&gt;数据集&lt;/h2&gt;&lt;p&gt;继续&lt;a href=&quot;./Second_TryMindSpore.md&quot;&gt;第二章&lt;/a&gt;中的Mnist数据库为例，介绍使用mindspore.dataset进行加载的方法。&lt;/p&gt;
&lt;p&gt;详情请见MNIST数据官方网站：&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cDovL3lhbm4ubGVjdW4uY29tL2V4ZGIvbW5pc3Qv&quot;&gt;MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges&lt;i class=&quot;fa fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="专题" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/"/>
    
    <category term="机器学习" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Mindspore" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Mindspore/"/>
    
    <category term="基本概念" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Mindspore/%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    
    
    <category term="机器学习" scheme="https://deepcity.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="MindSpore" scheme="https://deepcity.github.io/tags/MindSpore/"/>
    
    <category term="目录" scheme="https://deepcity.github.io/tags/%E7%9B%AE%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>MindSpore专题</title>
    <link href="https://deepcity.github.io/2024/special_subject/MindSpore/Chapters/Second_TryMindSpore/article.html"/>
    <id>https://deepcity.github.io/2024/special_subject/MindSpore/Chapters/Second_TryMindSpore/article.html</id>
    <published>2024-08-14T11:47:58.000Z</published>
    <updated>2024-08-16T08:59:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL3R1dG9yaWFscy96aC1DTi9yMi4zLjByYzIvYmVnaW5uZXIvcXVpY2tfc3RhcnQuaHRtbA==">MindSpore<i class="fa fa-external-link-alt"></i></span></p><span id="more"></span><h2 id="MindSpore-数据处理"><a href="#MindSpore-数据处理" class="headerlink" title="MindSpore 数据处理"></a>MindSpore 数据处理</h2><h3 id="download"><a href="#download" class="headerlink" title="download"></a>download</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install download</span><br></pre></td></tr></table></figure><p>下载download python模块</p><h3 id="引包并下载所需数据集"><a href="#引包并下载所需数据集" class="headerlink" title="引包并下载所需数据集"></a>引包并下载所需数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mindspore</span><br><span class="line"><span class="keyword">from</span> mindspore <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> mindspore.dataset <span class="keyword">import</span> vision, transforms</span><br><span class="line"><span class="keyword">from</span> mindspore.dataset <span class="keyword">import</span> MnistDataset</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Download data from open datasets</span></span><br><span class="line"><span class="keyword">from</span> download <span class="keyword">import</span> download</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/&quot;</span> \</span><br><span class="line">      <span class="string">&quot;notebook/datasets/MNIST_Data.zip&quot;</span></span><br><span class="line">path = download(url, <span class="string">&quot;./&quot;</span>, kind=<span class="string">&quot;zip&quot;</span>, replace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2024/06/05/EuK87I6YUjdAs2v.png" alt="image-20240605204942126"></p><p>下载下来MNIST_Data数据，根据官网这是一份Mnist数据集，结构如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MNIST_Data</span><br><span class="line">└── train</span><br><span class="line">    ├── train-images-idx3-ubyte (60000个训练图片)</span><br><span class="line">    ├── train-labels-idx1-ubyte (60000个训练标签)</span><br><span class="line">└── test</span><br><span class="line">    ├── t10k-images-idx3-ubyte (10000个测试图片)</span><br><span class="line">    ├── t10k-labels-idx1-ubyte (10000个测试标签)</span><br></pre></td></tr></table></figure><h3 id="MindSpore数据处理"><a href="#MindSpore数据处理" class="headerlink" title="MindSpore数据处理"></a>MindSpore数据处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_dataset.get_col_names())</span><br></pre></td></tr></table></figure><p>打印数据集中所包含的数据列名</p><blockquote><p>MindSpore的dataset使用数据处理流水线（Data Processing Pipeline），需指定map、batch、shuffle等操作。这里我们使用map对图像数据及标签进行变换处理，将输入的图像缩放为1/255，根据均值0.1307和标准差值0.3081进行归一化处理，然后将处理好的数据集打包为大小为64的batch。</p></blockquote><p>上面是官网对以下代码的处理。</p><p>其中出现了一个新名词——<a href="#归一化处理">归一化处理</a>，这里使用的是Z-score normalization。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">datapipe</span>(<span class="params">dataset, batch_size</span>):</span><br><span class="line">    image_transforms = [</span><br><span class="line">        vision.Rescale(<span class="number">1.0</span> / <span class="number">255.0</span>, <span class="number">0</span>),</span><br><span class="line">        vision.Normalize(mean=(<span class="number">0.1307</span>,), std=(<span class="number">0.3081</span>,)),</span><br><span class="line">        vision.HWC2CHW()</span><br><span class="line">    ]</span><br><span class="line">    label_transform = transforms.TypeCast(mindspore.int32)</span><br><span class="line"></span><br><span class="line">    dataset = dataset.<span class="built_in">map</span>(image_transforms, <span class="string">&#x27;image&#x27;</span>)</span><br><span class="line">    dataset = dataset.<span class="built_in">map</span>(label_transform, <span class="string">&#x27;label&#x27;</span>)</span><br><span class="line">    dataset = dataset.batch(batch_size)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Map vision transforms and batch dataset</span></span><br><span class="line">train_dataset = datapipe(train_dataset, <span class="number">64</span>)</span><br><span class="line">test_dataset = datapipe(test_dataset, <span class="number">64</span>)</span><br></pre></td></tr></table></figure><hr><p>可使用<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL2RhdGFzZXQvZGF0YXNldF9tZXRob2QvaXRlcmF0b3IvbWluZHNwb3JlLmRhdGFzZXQuRGF0YXNldC5jcmVhdGVfdHVwbGVfaXRlcmF0b3IuaHRtbA==">create_tuple_iterator<i class="fa fa-external-link-alt"></i></span> 或<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL2RhdGFzZXQvZGF0YXNldF9tZXRob2QvaXRlcmF0b3IvbWluZHNwb3JlLmRhdGFzZXQuRGF0YXNldC5jcmVhdGVfZGljdF9pdGVyYXRvci5odG1s">create_dict_iterator<i class="fa fa-external-link-alt"></i></span>对数据集进行迭代访问，查看数据和标签的shape和datatype。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> image, label <span class="keyword">in</span> test_dataset.create_tuple_iterator():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of image [N, C, H, W]: <span class="subst">&#123;image.shape&#125;</span> <span class="subst">&#123;image.dtype&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of label: <span class="subst">&#123;label.shape&#125;</span> <span class="subst">&#123;label.dtype&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><blockquote><p>Shape of image [N, C, H, W]: (64, 1, 28, 28) Float32<br>Shape of label: (64,) Int32</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_dataset.create_dict_iterator():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of image [N, C, H, W]: <span class="subst">&#123;data[<span class="string">&#x27;image&#x27;</span>].shape&#125;</span> <span class="subst">&#123;data[<span class="string">&#x27;image&#x27;</span>].dtype&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of label: <span class="subst">&#123;data[<span class="string">&#x27;label&#x27;</span>].shape&#125;</span> <span class="subst">&#123;data[<span class="string">&#x27;label&#x27;</span>].dtype&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><blockquote><p>Shape of image [N, C, H, W]: (64, 1, 28, 28) Float32<br>Shape of label: (64,) Int32</p></blockquote><p>更多细节详见<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL3R1dG9yaWFscy96aC1DTi9yMi4yL2JlZ2lubmVyL2RhdGFzZXQuaHRtbA==">数据集 Dataset<i class="fa fa-external-link-alt"></i></span>与<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL3R1dG9yaWFscy96aC1DTi9yMi4yL2JlZ2lubmVyL3RyYW5zZm9ybXMuaHRtbA==">数据变换 Transforms<i class="fa fa-external-link-alt"></i></span>。</p><h3 id="网络构建"><a href="#网络构建" class="headerlink" title="网络构建"></a>网络构建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(nn.Cell):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">        <span class="variable language_">self</span>.dense_relu_sequential = nn.SequentialCell(</span><br><span class="line">            nn.Dense(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dense(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dense(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">construct</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.flatten(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.dense_relu_sequential(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">model = Network()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><blockquote><p>Network&lt;<br>  (flatten): Flatten&lt;&gt;<br>  (dense_relu_sequential): SequentialCell&lt;<br>    (0): Dense<input_channels=784, output_channels=512, has_bias=True><br>    (1): ReLU&lt;&gt;<br>    (2): Dense<input_channels=512, output_channels=512, has_bias=True><br>    (3): ReLU&lt;&gt;<br>    (4): Dense<input_channels=512, output_channels=10, has_bias=True><br>    &gt;</p></blockquote><p>以上为网格的构建以及其输出，有以下几点需要注意</p><ol><li>上面重命名过MindSpore中的nn类是构建所有网格的基类，也是网格的基本单元</li><li>当需要自定义网络时可以重写<code>`nn.Cell</code>类重写<code>__init__</code>方法和<code>construct</code>方法</li><li><code>__init__</code>包含所有网络层的定义<code>construct</code>中包含数据（<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL3R1dG9yaWFscy96aC1DTi9yMi4yL2JlZ2lubmVyL3RlbnNvci5odG1s">Tensor<i class="fa fa-external-link-alt"></i></span>）的变换过程</li></ol><blockquote><p><code>mindspore.nn</code>类是构建所有网络的基类，也是网络的基本单元。当用户需要自定义网络时，可以继承<code>nn.Cell</code>类，并重写<code>__init__</code>方法和<code>construct</code>方法。<code>__init__</code>包含所有网络层的定义，<code>construct</code>中包含数据（<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL3R1dG9yaWFscy96aC1DTi9yMi4yL2JlZ2lubmVyL3RlbnNvci5odG1s">Tensor<i class="fa fa-external-link-alt"></i></span>）的变换过程。</p></blockquote><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>个人菜狗形象的想，实际上这里的网格神经网络对图像的识别类似一种带指向的可变哈希函数（只不过这个函数的实现比较复杂和通用）。他是从大量信息到小量准确抽象的描述过程。下面是<span class="exturl" data-url="aHR0cHM6Ly9vcGVubWxzeXMuZ2l0aHViLmlvL2NoYXB0ZXJfcHJvZ3JhbW1pbmdfaW50ZXJmYWNlL21sX3dvcmtmbG93Lmh0bWw=">3.2. 机器学习工作流 — 机器学习系统：设计和实现 1.0.0 documentation (openmlsys.github.io)<i class="fa fa-external-link-alt"></i></span>手册上的机器学习流程图，这里以求形象的理解</p><p><img src="./../img/img_workflow.svg" alt="img_workflow"></p><blockquote><p>在模型训练中，一个完整的训练过程（step）需要实现以下三步：</p><ol><li><strong>正向计算</strong>：模型预测结果（logits），并与正确标签（label）求预测损失（loss）。</li><li><strong>反向传播</strong>：利用自动微分机制，自动求模型参数（parameters）对于loss的梯度（gradients）。</li><li><strong>参数优化</strong>：将梯度更新到参数上。</li></ol><p>MindSpore使用函数式自动微分机制，因此针对上述步骤需要实现：</p><ol><li>定义正向计算函数。</li><li>使用<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL21pbmRzcG9yZS9taW5kc3BvcmUudmFsdWVfYW5kX2dyYWQuaHRtbA==">value_and_grad<i class="fa fa-external-link-alt"></i></span>通过函数变换获得梯度计算函数。</li><li>定义训练函数，使用<span class="exturl" data-url="aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL2RvY3MvemgtQ04vcjIuMi9hcGlfcHl0aG9uL25uL21pbmRzcG9yZS5ubi5DZWxsLmh0bWwjbWluZHNwb3JlLm5uLkNlbGwuc2V0X3RyYWlu">set_train<i class="fa fa-external-link-alt"></i></span>设置为训练模式，执行正向计算、反向传播和参数优化。</li></ol></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instantiate loss function and optimizer</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = nn.SGD(model.trainable_params(), <span class="number">1e-2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Define forward function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward_fn</span>(<span class="params">data, label</span>):</span><br><span class="line">    logits = model(data)</span><br><span class="line">    loss = loss_fn(logits, label)</span><br><span class="line">    <span class="keyword">return</span> loss, logits</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Get gradient function</span></span><br><span class="line">grad_fn = mindspore.value_and_grad(forward_fn, <span class="literal">None</span>, optimizer.parameters, has_aux=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Define function of one-step training</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">data, label</span>):</span><br><span class="line">    (loss, _), grads = grad_fn(data, label)</span><br><span class="line">    optimizer(grads)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, dataset</span>):</span><br><span class="line">    size = dataset.get_dataset_size()</span><br><span class="line">    model.set_train()</span><br><span class="line">    <span class="keyword">for</span> batch, (data, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataset.create_tuple_iterator()):</span><br><span class="line">        loss = train_step(data, label)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            loss, current = loss.asnumpy(), batch</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;loss: <span class="subst">&#123;loss:&gt;7f&#125;</span>  [<span class="subst">&#123;current:&gt;3d&#125;</span>/<span class="subst">&#123;size:&gt;3d&#125;</span>]&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>除训练外，我们定义测试函数，用来评估模型的性能。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">model, dataset, loss_fn</span>):</span><br><span class="line">    num_batches = dataset.get_dataset_size()</span><br><span class="line">    model.set_train(<span class="literal">False</span>)</span><br><span class="line">    total, test_loss, correct = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> dataset.create_tuple_iterator():</span><br><span class="line">        pred = model(data)</span><br><span class="line">        total += <span class="built_in">len</span>(data)</span><br><span class="line">        test_loss += loss_fn(pred, label).asnumpy()</span><br><span class="line">        correct += (pred.argmax(<span class="number">1</span>) == label).asnumpy().<span class="built_in">sum</span>()</span><br><span class="line">    test_loss /= num_batches</span><br><span class="line">    correct /= total</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Test: \n Accuracy: <span class="subst">&#123;(<span class="number">100</span>*correct):&gt;<span class="number">0.1</span>f&#125;</span>%, Avg loss: <span class="subst">&#123;test_loss:&gt;8f&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>训练过程需多次迭代数据集，一次完整的迭代称为一轮（epoch）。在每一轮，遍历训练集进行训练，结束后使用测试集进行预测。打印每一轮的loss值和预测准确率（Accuracy），可以看到loss在不断下降，Accuracy在不断提高。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;t+<span class="number">1</span>&#125;</span>\n-------------------------------&quot;</span>)</span><br><span class="line">    train(model, train_dataset)</span><br><span class="line">    test(model, test_dataset, loss_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Done!&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">&gt;Epoch 1</span><br><span class="line">&gt;-------------------------------</span><br><span class="line">&gt;loss: 2.302088  [  0/938]</span><br><span class="line">&gt;loss: 2.290692  [100/938]</span><br><span class="line">&gt;loss: 2.266338  [200/938]</span><br><span class="line">&gt;loss: 2.205240  [300/938]</span><br><span class="line">&gt;loss: 1.907198  [400/938]</span><br><span class="line">&gt;loss: 1.455603  [500/938]</span><br><span class="line">&gt;loss: 0.861103  [600/938]</span><br><span class="line">&gt;loss: 0.767219  [700/938]</span><br><span class="line">&gt;loss: 0.422253  [800/938]</span><br><span class="line">&gt;loss: 0.513922  [900/938]</span><br><span class="line">&gt;Test:</span><br><span class="line">Accuracy: 83.8%, Avg loss: 0.529534</span><br><span class="line"></span><br><span class="line">&gt;Epoch 2</span><br><span class="line">&gt;-------------------------------</span><br><span class="line">&gt;loss: 0.580867  [  0/938]</span><br><span class="line">&gt;loss: 0.479347  [100/938]</span><br><span class="line">&gt;loss: 0.677991  [200/938]</span><br><span class="line">&gt;loss: 0.550141  [300/938]</span><br><span class="line">&gt;loss: 0.226565  [400/938]</span><br><span class="line">&gt;loss: 0.314738  [500/938]</span><br><span class="line">&gt;loss: 0.298739  [600/938]</span><br><span class="line">&gt;loss: 0.459540  [700/938]</span><br><span class="line">&gt;loss: 0.332978  [800/938]</span><br><span class="line">&gt;loss: 0.406709  [900/938]</span><br><span class="line">&gt;Test:</span><br><span class="line">Accuracy: 90.2%, Avg loss: 0.334828</span><br><span class="line"></span><br><span class="line">&gt;Epoch 3</span><br><span class="line">&gt;-------------------------------</span><br><span class="line">&gt;loss: 0.461890  [  0/938]</span><br><span class="line">&gt;loss: 0.242303  [100/938]</span><br><span class="line">&gt;loss: 0.281414  [200/938]</span><br><span class="line">&gt;loss: 0.207835  [300/938]</span><br><span class="line">&gt;loss: 0.206000  [400/938]</span><br><span class="line">&gt;loss: 0.409646  [500/938]</span><br><span class="line">&gt;loss: 0.193608  [600/938]</span><br><span class="line">&gt;loss: 0.217575  [700/938]</span><br><span class="line">&gt;loss: 0.212817  [800/938]</span><br><span class="line">&gt;loss: 0.202862  [900/938]</span><br><span class="line">&gt;Test:</span><br><span class="line">Accuracy: 91.9%, Avg loss: 0.280962</span><br><span class="line"></span><br><span class="line">&gt;Done!</span><br></pre></td></tr></table></figure><p>训练过程需多次迭代数据集，一次完整的迭代称为一轮（epoch）。在每一轮，遍历训练集进行训练，结束后使用测试集进行预测。打印每一轮的loss值和预测准确率（Accuracy），可以看到loss在不断下降，Accuracy在不断提高。</p></blockquote><h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><p>模型训练完成后，需要将其参数进行保存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save checkpoint</span></span><br><span class="line">mindspore.save_checkpoint(model, <span class="string">&quot;model.ckpt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Saved Model to model.ckpt&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>Saved Model to model.ckpt</p></blockquote><h3 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h3><blockquote><p>加载保存的权重分为两步：</p><ol><li>重新实例化模型对象，构造模型。</li><li>加载模型参数，并将其加载至模型上。</li></ol></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instantiate a random initialized model</span></span><br><span class="line">model = Network()</span><br><span class="line"><span class="comment"># Load checkpoint and load parameter to model</span></span><br><span class="line">param_dict = mindspore.load_checkpoint(<span class="string">&quot;model.ckpt&quot;</span>)</span><br><span class="line">param_not_load, _ = mindspore.load_param_into_net(model, param_dict)</span><br><span class="line"><span class="built_in">print</span>(param_not_load)</span><br></pre></td></tr></table></figure><blockquote><p><code>param_not_load未被加载的参数列表，为空时代表所有参数均加载成功。</code></p><p>加载后的模型可以直接用于预测推理。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.set_train(<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> test_dataset:</span><br><span class="line">    pred = model(data)</span><br><span class="line">    predicted = pred.argmax(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Predicted: &quot;<span class="subst">&#123;predicted[:<span class="number">10</span>]&#125;</span>&quot;, Actual: &quot;<span class="subst">&#123;label[:<span class="number">10</span>]&#125;</span>&quot;&#x27;</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>以上就是一个简单的图像识别机械学习</p><p><strong>注意在加载模型的过程中必须定义模型构建与datapipe</strong></p><h2 id="归一化处理"><a href="#归一化处理" class="headerlink" title="归一化处理"></a>归一化处理</h2><blockquote><p>一些传送门：<br><span class="exturl" data-url="aHR0cHM6Ly9wYWRkbGVwZWRpYS5yZWFkdGhlZG9jcy5pby9lbi9sYXRlc3QvdHV0b3JpYWxzL2RlZXBfbGVhcm5pbmcvbm9ybWFsaXphdGlvbi9iYXNpY19ub3JtYWxpemF0aW9uLmh0bWw=">归一化基础知识点 — PaddleEdu documentation (paddlepedia.readthedocs.io)<i class="fa fa-external-link-alt"></i></span></p><p><span class="exturl" data-url="aHR0cHM6Ly9nZWVrLWRvY3MuY29tL251bXB5L251bXB5LWFzay1hbnN3ZXIvbm9ybWFsaXplLW51bXB5LWFycmF5X3oxLmh0bWw=">Numpy数组归一化|极客教程 (geek-docs.com)<i class="fa fa-external-link-alt"></i></span></p></blockquote><p>归一化是一种数据处理方式，能将数据经过处理后限制在某个固定范围内。</p><h3 id="归一化处理的两种形式"><a href="#归一化处理的两种形式" class="headerlink" title="归一化处理的两种形式"></a>归一化处理的两种形式</h3><p>归一化存在两种形式</p><ol><li>将数处理为 [0, 1] 之间的小数，其目的是为了在随后的数据处理过程中更便捷，其他情况下，也可将数据处理到 [-1, 1] 之间，或其他的固定范围内。</li></ol><blockquote><p>例如，在图像处理中，就会将图像从 [0, 255] 归一化到 [0, 1]之间，这样既不会改变图像本身的信息储存，又可加速后续的网络处理。</p></blockquote><ol><li>通过归一化将有<a href="#有/无量纲表达式">量纲表达式</a>变成<a href="#有/无量纲表达式">无量纲表达式</a>。</li></ol><h3 id="为什么要进行归一化"><a href="#为什么要进行归一化" class="headerlink" title="为什么要进行归一化"></a>为什么要进行归一化</h3><ol><li>解决数据间的可比性问题</li><li>数据归一化后，寻求最优解的过程会变得平缓，可以更快速的收敛到最优解。<a href="为什么归一化能提高求解最优解的速度">为什么能提高收敛速度</a>.</li></ol><h3 id="归一化类型"><a href="#归一化类型" class="headerlink" title="归一化类型"></a>归一化类型</h3><ol><li>Min-max normalization (Rescaling) 范围为[0,1]:</li></ol><script type="math/tex; mode=display">x^{'} = \frac{x - min(x)}{max(x) - min(x)}</script><ol><li>Mean normalization范围为[-1,1]：</li></ol><script type="math/tex; mode=display">x^{'} = \frac{x - mean(x)}{max(x) - min(x)}</script><blockquote><p>mean(x)：x数据的平均值</p></blockquote><p>​    Min-max归一化和mean归一化适合在最大最小值明确不变的情况下使用，比如图像处理时，灰度值限定在 [0, 255] 的范围内，就可以用min-max归一化将其处理到[0, 1]之间。在最大最小值不明确时，每当有新数据加入，都可能会改变最大或最小值，导致归一化结果不稳定，后续使用效果也不稳定。同时，数据需要相对稳定，如果有过大或过小的异常值存在，min-max归一化和mean归一化的效果也不会很好。如果对处理后的数据范围有严格要求，也应使用min-max归一化或mean归一化。</p><ol><li>Z-score normalization (Standardization)范围为实数集：</li></ol><script type="math/tex; mode=display">x^{'} = \frac{x - \mu}{\sigma}</script><blockquote><p>𝜇、𝜎 分别为样本数据的均值和标准差。</p></blockquote><p>​    Z-score归一化也可称为标准化，经过处理的数据呈均值为0，标准差为1的分布。在数据存在异常值、最大最小值不固定的情况下，可以使用标准化。标准化会改变数据的状态分布，但不会改变分布的种类。特别地，神经网络中经常会使用到z-score归一化，针对这一点，我们将在后续的文章中进行详细的介绍。</p><ol><li>对数归一化：</li></ol><script type="math/tex; mode=display">x^{'} = \frac{\lg x}{\lg max(x)}</script><ol><li>反正切函数归一化：</li></ol><script type="math/tex; mode=display">x^{'} = \arctan(x) * \frac{2}{\pi}</script><ol><li>小数定标标准化（Demical Point Normalization）:</li></ol><script type="math/tex; mode=display">x^{'} = \frac{x}{10^j}</script><p>​    非线性归一化通常被用在数据分化程度较大的场景，有时需要通过一些数学函数对原始值进行映射，如对数、反正切等。</p><h3 id="归一化和标准化的联系与区别"><a href="#归一化和标准化的联系与区别" class="headerlink" title="归一化和标准化的联系与区别"></a>归一化和标准化的联系与区别</h3><p>谈到归一化和标准化可能会存在一些概念的混淆，我们都知道归一化是指normalization，标准化是指standardization，但根据wiki上对feature scaling方法的定义，standardization其实就是z-score normalization，也就是说标准化其实是归一化的一种，而一般情况下，我们会把z-score归一化称为标准化，把min-max归一化简称为归一化。在下文中，我们也是用标准化指代z-score归一化，并使用归一化指代min-max归一化。</p><p>其实，归一化和标准化在本质上都是一种线性变换。在<a href="#归一化类型">归一化类型</a>中，我们提到了归一化和标准化的公式，对于归一化的公式，在数据给定的情况下，可以令𝑎=𝑚𝑎𝑥(𝑥)−𝑚𝑖𝑛(𝑥)、𝑏=𝑚𝑖𝑛(𝑥)，则归一化的公式可变形为：</p><script type="math/tex; mode=display">x^{'} = \frac{x - b}{a} = \frac{x}{a} - \frac{b}{a} = \frac{x}{a} - c</script><p>标准化的公式与变形后的归一化类似，其中的$\mu $和$\sigma$在数据给定的情况下，可以看作常数。因此，标准化的变形与归一化的类似，都可看作对𝑥按比例𝑎进行缩放，再进行𝑐个单位的平移。由此可见，归一化和标准化的本质都是一种线性变换，他们都不会因为对数据的处理而改变数据的原始数值排序。</p><p>那么归一化和标准化又有什么区别呢？</p><ol><li>归一化不会改变数据的状态分布，但标准化会改变数据的状态分布；</li><li>归一化会将数据限定在一个具体的范围内，如 [0, 1]，但标准化不会，标准化只会将数据处理为均值为0，标准差为1。</li></ol><h3 id="为什么归一化能提高求解最优解的速度"><a href="#为什么归一化能提高求解最优解的速度" class="headerlink" title="为什么归一化能提高求解最优解的速度"></a>为什么归一化能提高求解最优解的速度</h3><script type="math/tex; mode=display">\begin{split}\begin{align}y &= \theta_1x_1 + \theta_2x_2 \\J &= (\theta_{1}x_{1} + \theta_{2}x_{2} - y_{label})^2\end{align}\end{split}</script><p>假设自变量只有房子到地铁站的距离<script type="math/tex">𝑥_1</script>和房子内房间的个数<script type="math/tex">𝑥_2</script>，因变量为房价，预测公式和损失函数分别为：</p><script type="math/tex; mode=display">J = (1000\theta_{1}+3\theta_{2} - y_{label})^2</script><p><img src="https://s2.loli.net/2024/06/05/QAonmGKX4FqxUMt.png" alt="normalization"></p><div align="center">图1: 损失函数的等高线，图1（左）为未归一化时，图1（右）为归一化</div><p>​    在图1中，左图的红色椭圆代表归一化前的损失函数等高线，蓝色线段代表梯度的更新，箭头的方向代表梯度更新的方向。寻求最优解的过程就是梯度更新的过程，其更新方向与登高线垂直。由于𝑥1 和 𝑥2 的量级相差过大，损失函数的等高线呈现为一个瘦窄的椭圆。因此如图1（左）所示，瘦窄的椭圆形会使得梯度下降过程呈之字形呈现，导致梯度下降速度缓慢。</p><p>​    当数据经过归一化后，$x<em>{1}^{‘} = \frac{1000-0}{5000-0}=0.2$，$x</em>{2}^{‘} = \frac{3-0}{10-0}=0.3$，那么损失函数的公式可以写为：</p><script type="math/tex; mode=display">J(x) = (0.2\theta_{1} + 0.3\theta_{2} - y_{label})^2</script><p>​    我们可以看到，经过归一化后的数据属于同一量级，损失函数的等高线呈现为一个矮胖的椭圆形（如图1（右）所示），求解最优解过程变得更加迅速且平缓，因此可以在通过梯度下降进行求解时获得更快的收敛。</p><h2 id="有-无量纲表达式"><a href="#有-无量纲表达式" class="headerlink" title="有/无量纲表达式"></a>有/无量纲表达式</h2><p>我们假定数据都是一个个变量（不过提前收集好了）量纲指有些未知数他们是变量存在的一种或几种依赖关系，该变量的值由这些依赖关系的未知数（量纲）决定。</p><blockquote><p> 就像一些函数的y值，理想状态下的房价，他们由x，面积/位置决定。</p></blockquote><p>显然位置差几百米，面积差几百平方米对变量影响差距极大，这种情况下我们称对这些依赖关系式是有量纲的。他们对变量影响的系数存在数量级的不同。</p><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><ol><li>vscode 出现python解释器的选择错误，这在windows经常有非常多版本的相同软件上经常出现，选定安装MindSpore库的python版本</li></ol><p><img src="https://s2.loli.net/2024/06/05/QSIFMsjBCUAo34y.png" alt="image-20240605210110827"></p><p>​    点击此处python版本即可</p><ol><li><p>注意python的多数语法检查集成已分离为插件，一些在2023.10发布，发布后一些网络上的blog修改linting的将禁用，详情请看<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21pY3Jvc29mdC92c2NvZGUtcHl0aG9uL3dpa2kvTWlncmF0aW9uLXRvLVB5dGhvbi1Ub29scy1FeHRlbnNpb25z">迁移到 Python 工具扩展 ·microsoft/vscode-python 维基 (github.com)<i class="fa fa-external-link-alt"></i></span></p></li><li><p>解决一些格式上的报错也可以不理会，但根据PEP 8 python规范，一行不能超过80个字符，同时换行需要缩进,如果看不管可以在扩展语法检查中增加args</p></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--max-line-length=120</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly93d3cubWluZHNwb3JlLmNuL3R1dG9yaWFscy96aC1DTi9yMi4zLjByYzIvYmVnaW5uZXIvcXVpY2tfc3RhcnQuaHRtbA==&quot;&gt;MindSpore&lt;i class=&quot;fa fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/span&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="专题" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/"/>
    
    <category term="机器学习" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Mindspore" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Mindspore/"/>
    
    <category term="基本概念" scheme="https://deepcity.github.io/categories/%E4%B8%93%E9%A2%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Mindspore/%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    
    
    <category term="机器学习" scheme="https://deepcity.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="MindSpore" scheme="https://deepcity.github.io/tags/MindSpore/"/>
    
    <category term="目录" scheme="https://deepcity.github.io/tags/%E7%9B%AE%E5%BD%95/"/>
    
  </entry>
  
</feed>
